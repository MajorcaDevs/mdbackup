{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Backups utility \u00b6 Small but customizable utility to create backups and store them in cloud storage providers. How to install \u00b6 Download from releases the latest wheel package and install it. It is recommended to use a virtual environment to do that. We will show you this way. What do yo need? : An OS different from Windows (Windows is unsupported) :( Python 3.7 or higher First select a folder where all the needed files will be stored. It is important not to move (or rename) this folder after installation. Run one of those commands. If both fail, try to install python3 - virtualenv (debian based) or pip3 install virtualenv (on macOS). python3 -m venv .venv python3 -m virtualenv .venv When you have the virtual environment created, you have to activate it. With this, you can run python commands and everything you do, will alter the virtual env, not the real one (and so, you don\u2019t need sudo to do things). . .venv/bin/activate # Download the .whl package pip install --upgrade setuptools wheel pip install mdbackup*.whl Now you can run the utility (only if you have enabled the virtual env) with mdbackup . In this folder it is recommended to store the config folder. Note: to be able to use some of the cloud storage and secrets backends, you will be requested to install some packages. Go to the documentation to see what is needed. Documentation \u00b6 Can be found at mdbackup.majorcadevs.com or at the docs folder. To make the documentation, install the requirements in docs / requirements . txt and run mkdocs build --config-file=mkdocs.yaml .","title":"Home"},{"location":"#backups-utility","text":"Small but customizable utility to create backups and store them in cloud storage providers.","title":"Backups utility"},{"location":"#how-to-install","text":"Download from releases the latest wheel package and install it. It is recommended to use a virtual environment to do that. We will show you this way. What do yo need? : An OS different from Windows (Windows is unsupported) :( Python 3.7 or higher First select a folder where all the needed files will be stored. It is important not to move (or rename) this folder after installation. Run one of those commands. If both fail, try to install python3 - virtualenv (debian based) or pip3 install virtualenv (on macOS). python3 -m venv .venv python3 -m virtualenv .venv When you have the virtual environment created, you have to activate it. With this, you can run python commands and everything you do, will alter the virtual env, not the real one (and so, you don\u2019t need sudo to do things). . .venv/bin/activate # Download the .whl package pip install --upgrade setuptools wheel pip install mdbackup*.whl Now you can run the utility (only if you have enabled the virtual env) with mdbackup . In this folder it is recommended to store the config folder. Note: to be able to use some of the cloud storage and secrets backends, you will be requested to install some packages. Go to the documentation to see what is needed.","title":"How to install"},{"location":"#documentation","text":"Can be found at mdbackup.majorcadevs.com or at the docs folder. To make the documentation, install the requirements in docs / requirements . txt and run mkdocs build --config-file=mkdocs.yaml .","title":"Documentation"},{"location":"arguments/","text":"Arguments \u00b6 The tool has some subcommands that can be used manually. By default the tool will run in complete mode which does everything (check configuration, backup, upload and clean up). usage : mdbackup [ - h ] [ - c CONFIG ] mode ... Small but customizable utility to create backups and store them in cloud storage providers optional arguments : - h , -- help show this help message and exit - c CONFIG , -- config CONFIG Path to configuration folder ( default : config ) subcommands : Selects the run mode ( defaults to complete ) mode complete Checks config , does a backup , uploads the backup and does cleanup backup Does a backup upload Upload a pending backups cleanup Does cleanup of backups check - config Checks configuration to catch issues complete \u00b6 This subcommand has no extra arguments. backup \u00b6 This subcommand has no extra arguments. upload \u00b6 usage : mdbackup upload [ - h ] [ -- backup BACKUP ] [ - f ] optional arguments : - h , -- help show this help message and exit -- backup BACKUP Selects which backup to upload by the name of the folder ( which is the date of the backup ) - f , -- force Force upload the backup even if the backup was already uploaded cleanup \u00b6 This subcommand has no extra arguments. check - config \u00b6 This subcommand has no extra arguments.","title":"Arguments"},{"location":"arguments/#arguments","text":"The tool has some subcommands that can be used manually. By default the tool will run in complete mode which does everything (check configuration, backup, upload and clean up). usage : mdbackup [ - h ] [ - c CONFIG ] mode ... Small but customizable utility to create backups and store them in cloud storage providers optional arguments : - h , -- help show this help message and exit - c CONFIG , -- config CONFIG Path to configuration folder ( default : config ) subcommands : Selects the run mode ( defaults to complete ) mode complete Checks config , does a backup , uploads the backup and does cleanup backup Does a backup upload Upload a pending backups cleanup Does cleanup of backups check - config Checks configuration to catch issues","title":"Arguments"},{"location":"arguments/#complete","text":"This subcommand has no extra arguments.","title":"complete"},{"location":"arguments/#backup","text":"This subcommand has no extra arguments.","title":"backup"},{"location":"arguments/#upload","text":"usage : mdbackup upload [ - h ] [ -- backup BACKUP ] [ - f ] optional arguments : - h , -- help show this help message and exit -- backup BACKUP Selects which backup to upload by the name of the folder ( which is the date of the backup ) - f , -- force Force upload the backup even if the backup was already uploaded","title":"upload"},{"location":"arguments/#cleanup","text":"This subcommand has no extra arguments.","title":"cleanup"},{"location":"arguments/#check-config","text":"This subcommand has no extra arguments.","title":"check-config"},{"location":"configuration/","text":"Configuration \u00b6 You have available under config / config . schema . json the JSON schema of the configuration file. You can use it like this on an app like Visual Studio Code or PyCharm: { \"$schema\" : \"./config.schema.json\" } If you are going to use the $ schema , you should download it or reference the URL of the file from the repository directly. This allows you to auto-complete with the elements available in the configuration. But in case you cannot use an app with schema support, here\u2019s it is the (maybe not updated) list of options: JSON syntax { \"backupsPath\" : \"Path where the backups will be stored\" , \"logLevel\" : \"Log level for the app, valid values are: CRITICAL,ERROR,WARNING,INFO,DEBUG\" , \"actionsModules\" : [ \"my.module#actions_register_function\" ], \"maxBackupsKept\" : 7 , \"env\" : { \"something\" : \"true\" }, \"secrets\" : { \"secret-provider\" : { \"envDefs\" : { \"pgpassword\" : \"/path/to/pg-password\" , \"mysqlpassword\" : \"mysql-password\" }, \"config\" : { \"setting-1\" : \"value\" , \"setting-2\" : true }, \"storageProviders\" : [ \"storage/digital-ocean\" , { \"key\" : \"storage/gdrive\" , \"backupsPath\" : \"/Backups/mbp\" }, \"storage/amazon\" ] } }, \"cloud\" : { \"compression\" : { \"method\" : \"gz|xz|bz2|br|zst\" , \"level\" : 8 }, \"cypher\" : { \"strategy\" : \"gpg-keys|gpg-passphrase\" , \"passphrase\" : \"If using gpg-passphrase, this will be used as passphrase for the cypher\" , \"keys\" : \"If using gpg-keys, this will be used as recipients option for the gpg cypher (emails)\" , \"algorithm\" : \"Defines the algorithm to use in the cypher process, depends in the strategy (currently one of `gpg --version` cyphers)\" }, \"providers\" : [ { \"type\" : \"provider-type-1\" , \"backupsPath\" : \"Path in the storage provider where to store the backups\" , \"maxBackupsKept\" : 30 , \"provider-specific-param-1\" : \"config/client_secrets.json\" , \"provider-specific-param-2\" : false }, { \"type\" : \"provider-type-2\" , \"backupsPath\" : \"Path in the storage provider where to store the backups\" , \"maxBackupsKept\" : 7 , \"provider-specific-param-1\" : \"THIS_IS-NOT-AN-API-KEY\" , \"provider-specific-param-2\" : \"THIS_IS_NOT_AN-API-S3Cr3t\" , \"provider-specific-param-3\" : 10 } ] }, \"hooks\" : { \"backup:before\" : \"echo $@\" , \"backup:after\" : \"path/to/script\" , \"backup:error\" : \"wombo combo $1 $2\" , \"upload:before\" : \"echo $@\" , \"upload:after\" : \"echo $@\" , \"upload:error\" : \"echo $@\" , \"oldBackup:deleting\" : \"echo $@\" , \"oldBackup:deleted\" : \"echo $@\" , \"oldBackup:error\" : \"echo $@\" } } YAML syntax backupsPath : Path where the backups will be stored logLevel : \"Log level for the app, valid values are: CRITICAL,ERROR,WARNING,INFO,DEBUG\" actionsModules : - \"my.module#actions_register_function\" maxBackupsKept : 7 env : something : \"true\" secrets : secret-provider : envDefs : pgpassword : /path/to/pg-password mysqlpassword : mysql-password config : \"setting-1\" : \"value\" \"setting-2\" : true storageProviders : - storage/digital-ocean - key : storage/gdrive backupsPath : /Backups/mbp - storage/aws-s3 cloud : compression : method : gz|xz|bz2|br|zst level : 8 cypher : strategy : gpg-keys|gpg-passphrase passphrase : If using gpg-passphrase, this will be used as passphrase for the cypher keys : If using gpg-keys, this will be used as recipients option for the gpg cypher (emails) algorithm : Defines the algorithm to use in the cypher process, depends in the strategy (currently one of `gpg --version` cyphers) providers : - type : \"provider-type-1\" backupsPath : \"Path in the storage provider where to store the backups\" maxBackupsKept : 30 provider-specific-param-1 : \"config/client_secrets.json\" provider-specific-param-2 : false - type : \"provider-type-2\" backupsPath : \"Path in the storage provider where to store the backups\" maxBackupsKept : 7 provider-specific-param-1 : \"THIS_IS-NOT-AN-API-KEY\" provider-specific-param-2 : \"THIS_IS_NOT_AN-API-S3Cr3t\" provider-specific-param-3 : 10 hooks : backup:before : \"echo $@\" backup:after : \"path/to/script\" backup:error : \"wombo combo $1 $2\" upload:before : \"echo $@\" upload:after : \"echo $@\" upload:error : \"echo $@\" oldBackup:deleting : \"echo $@\" oldBackup:deleted : \"echo $@\" oldBackup:error : \"echo $@\" The configuration file must be located in the configuration folder and can be a json or yaml file. By default, the configuration folder is placed at config (based on the current working directory) but can be changed by using the argument - c . backupsPath \u00b6 The path where all the backups will be stored locally. It will contain all the past backups plus the in-process (if any). When a backup is being done, it will create a .partial folder inside backupsPath and inside the folder, all the copied files and directories will be stored. After a backup, the folder will be renamed to YYYY - MM - DDThh : mm , matching the time when the backup was completed. logLevel \u00b6 Configures the log level. Every log issued to the logger that is below the configured log level will be ignored. By default is set to INFO . The available levels, ordered by importance, are: CRITICAL ERROR WARNING INFO DEBUG actionsModules \u00b6 If defined, will load all python modules and run the function that will register new actions into the system that can be used in tasks . maxBackupsKept \u00b6 Defines how many backups will be kept in the local folder. By default is set to 7. To disable the cleanup, use 0 or null as value of this setting. env \u00b6 This section defines environment variables that will be available when running actions . Can be anything that can be accepted by an action. These variables are passed to the actions as parameters, only if the type is a dictionary (i.e.: the action from - file accepts a dictionary or a string as parameter, only when using a dictionary these values will be filled). secrets \u00b6 Defines all secrets providers available to run the tool. Can obtain values for environment and storage providers from the secret providers in runtime, improving security by having in different places all the secrets. It is optional, but it is recommended to use any secret provider. Every type of secret provider is defined inside the secrets section, where the key of the object is the type, and the object contains the configuration of the provider and what to inject from it. config \u00b6 The configuration section contains provider-specific configuration which allows the provider to work. See secret providers documentation to see the available providers and their configuration. envDefs \u00b6 Declares environment variable definitions that can be used to reference secrets in the tasks env sections and actions parameters. storageProviders \u00b6 The storage section defines storage provider configurations that will be grabbed from the secret provider. Each value of the list is a (secret) provider-specific url/path/identifier that tells the provider where to look for the configuration. The value must have the same structure of the configuration of the storage provider. cloud \u00b6 This section defines settings for the storage servers and cloud storage providers. compression \u00b6 If defined, when backups are uploaded to a storage provider, folders will be compressed using this configuration. Can be used with or without cyphering . strategy \u00b6 The strategy defines which compression algorithm is going to be used. Currently, the algorithms supported are gzip (which requires gzip to be installed) and xz (which requires xz to be installed). In general, a lot of Linux distributions includes these commands, as well as in macOS. But it\u2019s worth to check their existence before using them. level \u00b6 The compression level. Higher values indicates better but slower compressions. Values accepted for gzip are from 1 to 9. Values accepted for xz are from 0 to 9 (by default is 6, 7-9 are not recommended). cypher \u00b6 If defined, when backups are uploaded to a storage provider, folders will be encrypted using this configuration. Can be used with or without compression . strategy \u00b6 Defines which strategy to use to encrypt the data. Currently the supported cypher strategies are: gpg - passphrase - Uses a passphrase to encrypt and decrypt the data (requires gpg2 ). The passphrase setting will be used as passphrase. gpg - keys - Uses the keys associated to the list of emails to encrypt the data (requires gpg2 ). The keys list will be used as recipients/emails list that will be used to protect the data. The people in the list will be able to decrypt the data and no one else. Recommended over passphrase . algorithm \u00b6 If defined, will use this algorithm to encrypt the data. The supported algorithms and the default algorithm can be found in gpg --version gpg2 --version . providers \u00b6 If defined, the last backup will be uploaded to the configured storage providers . Each provider must define the type , the backupsPath and maxBackupsKept , as well as the provider specific configuration. Unknown types will be ignored. type \u00b6 Defines the type of the storage provider for the entry. The list of storage providers can be found in the \u2018Storage providers\u2019 section. backupsPath \u00b6 Path in the storage provider where to save the backups. Is the same concept as the backupsPath from above. Some providers need this folder to exist, while others no. If possible, try to ensure that the folder is created before uploading any backup. maxBackupsKept \u00b6 Defines how many backups will be kept in the storage provider. If set to 0 or null will not clean anything. There\u2019s no default value for this, so a value must be always provided. hooks \u00b6 Hooks run a script or program when something is going to happen or just happened. Can be useful to trigger some post-action things, to send messages through Slack or to manipulate the output of a backup. Each key defines the hook type, and their values is the script, program or one-line script that will be run in when the hook is triggered. The hooks run on a sh shell, so hooks like echo $ @ will work out of the box. See hooks section.","title":"Configuration"},{"location":"configuration/#configuration","text":"You have available under config / config . schema . json the JSON schema of the configuration file. You can use it like this on an app like Visual Studio Code or PyCharm: { \"$schema\" : \"./config.schema.json\" } If you are going to use the $ schema , you should download it or reference the URL of the file from the repository directly. This allows you to auto-complete with the elements available in the configuration. But in case you cannot use an app with schema support, here\u2019s it is the (maybe not updated) list of options: JSON syntax { \"backupsPath\" : \"Path where the backups will be stored\" , \"logLevel\" : \"Log level for the app, valid values are: CRITICAL,ERROR,WARNING,INFO,DEBUG\" , \"actionsModules\" : [ \"my.module#actions_register_function\" ], \"maxBackupsKept\" : 7 , \"env\" : { \"something\" : \"true\" }, \"secrets\" : { \"secret-provider\" : { \"envDefs\" : { \"pgpassword\" : \"/path/to/pg-password\" , \"mysqlpassword\" : \"mysql-password\" }, \"config\" : { \"setting-1\" : \"value\" , \"setting-2\" : true }, \"storageProviders\" : [ \"storage/digital-ocean\" , { \"key\" : \"storage/gdrive\" , \"backupsPath\" : \"/Backups/mbp\" }, \"storage/amazon\" ] } }, \"cloud\" : { \"compression\" : { \"method\" : \"gz|xz|bz2|br|zst\" , \"level\" : 8 }, \"cypher\" : { \"strategy\" : \"gpg-keys|gpg-passphrase\" , \"passphrase\" : \"If using gpg-passphrase, this will be used as passphrase for the cypher\" , \"keys\" : \"If using gpg-keys, this will be used as recipients option for the gpg cypher (emails)\" , \"algorithm\" : \"Defines the algorithm to use in the cypher process, depends in the strategy (currently one of `gpg --version` cyphers)\" }, \"providers\" : [ { \"type\" : \"provider-type-1\" , \"backupsPath\" : \"Path in the storage provider where to store the backups\" , \"maxBackupsKept\" : 30 , \"provider-specific-param-1\" : \"config/client_secrets.json\" , \"provider-specific-param-2\" : false }, { \"type\" : \"provider-type-2\" , \"backupsPath\" : \"Path in the storage provider where to store the backups\" , \"maxBackupsKept\" : 7 , \"provider-specific-param-1\" : \"THIS_IS-NOT-AN-API-KEY\" , \"provider-specific-param-2\" : \"THIS_IS_NOT_AN-API-S3Cr3t\" , \"provider-specific-param-3\" : 10 } ] }, \"hooks\" : { \"backup:before\" : \"echo $@\" , \"backup:after\" : \"path/to/script\" , \"backup:error\" : \"wombo combo $1 $2\" , \"upload:before\" : \"echo $@\" , \"upload:after\" : \"echo $@\" , \"upload:error\" : \"echo $@\" , \"oldBackup:deleting\" : \"echo $@\" , \"oldBackup:deleted\" : \"echo $@\" , \"oldBackup:error\" : \"echo $@\" } } YAML syntax backupsPath : Path where the backups will be stored logLevel : \"Log level for the app, valid values are: CRITICAL,ERROR,WARNING,INFO,DEBUG\" actionsModules : - \"my.module#actions_register_function\" maxBackupsKept : 7 env : something : \"true\" secrets : secret-provider : envDefs : pgpassword : /path/to/pg-password mysqlpassword : mysql-password config : \"setting-1\" : \"value\" \"setting-2\" : true storageProviders : - storage/digital-ocean - key : storage/gdrive backupsPath : /Backups/mbp - storage/aws-s3 cloud : compression : method : gz|xz|bz2|br|zst level : 8 cypher : strategy : gpg-keys|gpg-passphrase passphrase : If using gpg-passphrase, this will be used as passphrase for the cypher keys : If using gpg-keys, this will be used as recipients option for the gpg cypher (emails) algorithm : Defines the algorithm to use in the cypher process, depends in the strategy (currently one of `gpg --version` cyphers) providers : - type : \"provider-type-1\" backupsPath : \"Path in the storage provider where to store the backups\" maxBackupsKept : 30 provider-specific-param-1 : \"config/client_secrets.json\" provider-specific-param-2 : false - type : \"provider-type-2\" backupsPath : \"Path in the storage provider where to store the backups\" maxBackupsKept : 7 provider-specific-param-1 : \"THIS_IS-NOT-AN-API-KEY\" provider-specific-param-2 : \"THIS_IS_NOT_AN-API-S3Cr3t\" provider-specific-param-3 : 10 hooks : backup:before : \"echo $@\" backup:after : \"path/to/script\" backup:error : \"wombo combo $1 $2\" upload:before : \"echo $@\" upload:after : \"echo $@\" upload:error : \"echo $@\" oldBackup:deleting : \"echo $@\" oldBackup:deleted : \"echo $@\" oldBackup:error : \"echo $@\" The configuration file must be located in the configuration folder and can be a json or yaml file. By default, the configuration folder is placed at config (based on the current working directory) but can be changed by using the argument - c .","title":"Configuration"},{"location":"configuration/#backupspath","text":"The path where all the backups will be stored locally. It will contain all the past backups plus the in-process (if any). When a backup is being done, it will create a .partial folder inside backupsPath and inside the folder, all the copied files and directories will be stored. After a backup, the folder will be renamed to YYYY - MM - DDThh : mm , matching the time when the backup was completed.","title":"backupsPath"},{"location":"configuration/#loglevel","text":"Configures the log level. Every log issued to the logger that is below the configured log level will be ignored. By default is set to INFO . The available levels, ordered by importance, are: CRITICAL ERROR WARNING INFO DEBUG","title":"logLevel"},{"location":"configuration/#actionsmodules","text":"If defined, will load all python modules and run the function that will register new actions into the system that can be used in tasks .","title":"actionsModules"},{"location":"configuration/#maxbackupskept","text":"Defines how many backups will be kept in the local folder. By default is set to 7. To disable the cleanup, use 0 or null as value of this setting.","title":"maxBackupsKept"},{"location":"configuration/#env","text":"This section defines environment variables that will be available when running actions . Can be anything that can be accepted by an action. These variables are passed to the actions as parameters, only if the type is a dictionary (i.e.: the action from - file accepts a dictionary or a string as parameter, only when using a dictionary these values will be filled).","title":"env"},{"location":"configuration/#secrets","text":"Defines all secrets providers available to run the tool. Can obtain values for environment and storage providers from the secret providers in runtime, improving security by having in different places all the secrets. It is optional, but it is recommended to use any secret provider. Every type of secret provider is defined inside the secrets section, where the key of the object is the type, and the object contains the configuration of the provider and what to inject from it.","title":"secrets"},{"location":"configuration/#config","text":"The configuration section contains provider-specific configuration which allows the provider to work. See secret providers documentation to see the available providers and their configuration.","title":"config"},{"location":"configuration/#envdefs","text":"Declares environment variable definitions that can be used to reference secrets in the tasks env sections and actions parameters.","title":"envDefs"},{"location":"configuration/#storageproviders","text":"The storage section defines storage provider configurations that will be grabbed from the secret provider. Each value of the list is a (secret) provider-specific url/path/identifier that tells the provider where to look for the configuration. The value must have the same structure of the configuration of the storage provider.","title":"storageProviders"},{"location":"configuration/#cloud","text":"This section defines settings for the storage servers and cloud storage providers.","title":"cloud"},{"location":"configuration/#compression","text":"If defined, when backups are uploaded to a storage provider, folders will be compressed using this configuration. Can be used with or without cyphering .","title":"compression"},{"location":"configuration/#strategy","text":"The strategy defines which compression algorithm is going to be used. Currently, the algorithms supported are gzip (which requires gzip to be installed) and xz (which requires xz to be installed). In general, a lot of Linux distributions includes these commands, as well as in macOS. But it\u2019s worth to check their existence before using them.","title":"strategy"},{"location":"configuration/#level","text":"The compression level. Higher values indicates better but slower compressions. Values accepted for gzip are from 1 to 9. Values accepted for xz are from 0 to 9 (by default is 6, 7-9 are not recommended).","title":"level"},{"location":"configuration/#cypher","text":"If defined, when backups are uploaded to a storage provider, folders will be encrypted using this configuration. Can be used with or without compression .","title":"cypher"},{"location":"configuration/#strategy_1","text":"Defines which strategy to use to encrypt the data. Currently the supported cypher strategies are: gpg - passphrase - Uses a passphrase to encrypt and decrypt the data (requires gpg2 ). The passphrase setting will be used as passphrase. gpg - keys - Uses the keys associated to the list of emails to encrypt the data (requires gpg2 ). The keys list will be used as recipients/emails list that will be used to protect the data. The people in the list will be able to decrypt the data and no one else. Recommended over passphrase .","title":"strategy"},{"location":"configuration/#algorithm","text":"If defined, will use this algorithm to encrypt the data. The supported algorithms and the default algorithm can be found in gpg --version gpg2 --version .","title":"algorithm"},{"location":"configuration/#providers","text":"If defined, the last backup will be uploaded to the configured storage providers . Each provider must define the type , the backupsPath and maxBackupsKept , as well as the provider specific configuration. Unknown types will be ignored.","title":"providers"},{"location":"configuration/#type","text":"Defines the type of the storage provider for the entry. The list of storage providers can be found in the \u2018Storage providers\u2019 section.","title":"type"},{"location":"configuration/#backupspath_1","text":"Path in the storage provider where to save the backups. Is the same concept as the backupsPath from above. Some providers need this folder to exist, while others no. If possible, try to ensure that the folder is created before uploading any backup.","title":"backupsPath"},{"location":"configuration/#maxbackupskept_1","text":"Defines how many backups will be kept in the storage provider. If set to 0 or null will not clean anything. There\u2019s no default value for this, so a value must be always provided.","title":"maxBackupsKept"},{"location":"configuration/#hooks","text":"Hooks run a script or program when something is going to happen or just happened. Can be useful to trigger some post-action things, to send messages through Slack or to manipulate the output of a backup. Each key defines the hook type, and their values is the script, program or one-line script that will be run in when the hook is triggered. The hooks run on a sh shell, so hooks like echo $ @ will work out of the box. See hooks section.","title":"hooks"},{"location":"docker/","text":"Docker \u00b6 Use the image \u00b6 It is recommended to have a look to the Quick Start if you did not do it yet\u2026 To be done\u2026 Build image \u00b6 There are two flavours for the mdbackup image: one based on Alpine Linux and the other based on Debian (slim). To build the Debian version, run: docker image build -t mdbackup:slim -f docker/Dockerfile-slim . To build the Alpine Linux version, then run: docker image build -t mdbackup:slim -f docker/Dockerfile-alpine .","title":"Docker"},{"location":"docker/#docker","text":"","title":"Docker"},{"location":"docker/#use-the-image","text":"It is recommended to have a look to the Quick Start if you did not do it yet\u2026 To be done\u2026","title":"Use the image"},{"location":"docker/#build-image","text":"There are two flavours for the mdbackup image: one based on Alpine Linux and the other based on Debian (slim). To build the Debian version, run: docker image build -t mdbackup:slim -f docker/Dockerfile-slim . To build the Alpine Linux version, then run: docker image build -t mdbackup:slim -f docker/Dockerfile-alpine .","title":"Build image"},{"location":"hooks/","text":"Hooks \u00b6 Hooks are scripts that run when some event is going to happen or just happened. It is useful to define custom actions with your own scripts, including one-liner scrips. The hook is run with sh so scripts can be defined inlined. If a hook is not defined, won\u2019t run anything. To define the hooks, see hooks in the configuration . The output of the script is redirected to the logger using the DEBUG level. If you have some issues with your hook script, set the log level to DEBUG . backup:before \u00b6 When : Before starting doing backups. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). backup:after \u00b6 When : After all backups are done. Parameters : The folder where the backups are stored. backup:error \u00b6 When : When the backup process failed but before the partial folder is deleted. Parameters : The folder where the backups were being stored. Exception message. (Optional) Step name. backup:tasks:${tasks_name}:before \u00b6 When : Before starting running all tasks in a tasks definition file. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). The name of the tasks definition. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. backup:tasks:${tasks_name}:after \u00b6 When : After running all tasks in a tasks definition file. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). The name of the tasks definition. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. backup:tasks:${tasks_name}:error \u00b6 When : When one of the tasks failed running and stops the tasks run. Parameters : The exception message. The folder where the backups are going to be stored during the process (will end with .partial ). The name of the tasks definition. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. backup:tasks: {tasks_name}:task: {tasks_name}:task: :before \u00b6 When : Before running a task. Parameters : The folder where the backups are going to be stored during the process. The name of the tasks definition. The name of the task. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. The ${ task_name } must be the name of the task, defined in the name section of the task. backup:tasks: {tasks_name}:task: {tasks_name}:task: :after \u00b6 When : After running a task. Parameters : The folder where the backups are going to be stored during the process. The name of the tasks definition. The name of the task. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. The ${ task_name } must be the name of the task, defined in the name section of the task. backup:tasks: {tasks_name}:task: {tasks_name}:task: :error \u00b6 When : When a task run has failed, even with stopOnFail is set to false. Parameters : The error message. The folder where the backups are going to be stored during the process. The name of the tasks definition. The name of the task. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. The ${ task_name } must be the name of the task, defined in the name section of the task. upload:before \u00b6 When : Before uploading the backup to a storage provider. Parameters : The type of the provider. The path to the backups folder. upload:after \u00b6 When : After uploading the backup to a storage provider. Parameters : The type of the provider. The path to the backups folder. The path in the storage provider where the backup is stored. upload:error \u00b6 When : After uploading the backup to a storage provider but it failed. Parameters : The type of the provider. The path to the backups folder. The exception message. oldBackup:deleting \u00b6 When : Just before a backup folder is going to be deleted. Parameters : The path of the backup to be deleted. oldBackup:deleted \u00b6 When : After a backup folder was deleted. Parameters : The path of the backup deleted. oldBackup:error \u00b6 When : After a backup folder was going to be deleted, but it failed to do so. Parameters : The path of the backup to be deleted.","title":"Hooks"},{"location":"hooks/#hooks","text":"Hooks are scripts that run when some event is going to happen or just happened. It is useful to define custom actions with your own scripts, including one-liner scrips. The hook is run with sh so scripts can be defined inlined. If a hook is not defined, won\u2019t run anything. To define the hooks, see hooks in the configuration . The output of the script is redirected to the logger using the DEBUG level. If you have some issues with your hook script, set the log level to DEBUG .","title":"Hooks"},{"location":"hooks/#backupbefore","text":"When : Before starting doing backups. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ).","title":"backup:before"},{"location":"hooks/#backupafter","text":"When : After all backups are done. Parameters : The folder where the backups are stored.","title":"backup:after"},{"location":"hooks/#backuperror","text":"When : When the backup process failed but before the partial folder is deleted. Parameters : The folder where the backups were being stored. Exception message. (Optional) Step name.","title":"backup:error"},{"location":"hooks/#backuptaskstasks_namebefore","text":"When : Before starting running all tasks in a tasks definition file. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). The name of the tasks definition. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file.","title":"backup:tasks:${tasks_name}:before"},{"location":"hooks/#backuptaskstasks_nameafter","text":"When : After running all tasks in a tasks definition file. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). The name of the tasks definition. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file.","title":"backup:tasks:${tasks_name}:after"},{"location":"hooks/#backuptaskstasks_nameerror","text":"When : When one of the tasks failed running and stops the tasks run. Parameters : The exception message. The folder where the backups are going to be stored during the process (will end with .partial ). The name of the tasks definition. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file.","title":"backup:tasks:${tasks_name}:error"},{"location":"hooks/#backuptaskstasks_nametasktasks_nametaskbefore","text":"When : Before running a task. Parameters : The folder where the backups are going to be stored during the process. The name of the tasks definition. The name of the task. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. The ${ task_name } must be the name of the task, defined in the name section of the task.","title":"backup:tasks:{tasks_name}:task:{tasks_name}:task::before"},{"location":"hooks/#backuptaskstasks_nametasktasks_nametaskafter","text":"When : After running a task. Parameters : The folder where the backups are going to be stored during the process. The name of the tasks definition. The name of the task. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. The ${ task_name } must be the name of the task, defined in the name section of the task.","title":"backup:tasks:{tasks_name}:task:{tasks_name}:task::after"},{"location":"hooks/#backuptaskstasks_nametasktasks_nametaskerror","text":"When : When a task run has failed, even with stopOnFail is set to false. Parameters : The error message. The folder where the backups are going to be stored during the process. The name of the tasks definition. The name of the task. Note ${ tasks_name } must be the name of the tasks definition, defined in the name section of the file. The ${ task_name } must be the name of the task, defined in the name section of the task.","title":"backup:tasks:{tasks_name}:task:{tasks_name}:task::error"},{"location":"hooks/#uploadbefore","text":"When : Before uploading the backup to a storage provider. Parameters : The type of the provider. The path to the backups folder.","title":"upload:before"},{"location":"hooks/#uploadafter","text":"When : After uploading the backup to a storage provider. Parameters : The type of the provider. The path to the backups folder. The path in the storage provider where the backup is stored.","title":"upload:after"},{"location":"hooks/#uploaderror","text":"When : After uploading the backup to a storage provider but it failed. Parameters : The type of the provider. The path to the backups folder. The exception message.","title":"upload:error"},{"location":"hooks/#oldbackupdeleting","text":"When : Just before a backup folder is going to be deleted. Parameters : The path of the backup to be deleted.","title":"oldBackup:deleting"},{"location":"hooks/#oldbackupdeleted","text":"When : After a backup folder was deleted. Parameters : The path of the backup deleted.","title":"oldBackup:deleted"},{"location":"hooks/#oldbackuperror","text":"When : After a backup folder was going to be deleted, but it failed to do so. Parameters : The path of the backup to be deleted.","title":"oldBackup:error"},{"location":"quick-start/","text":"Quick start guide \u00b6 mdbackup is tested under Linux and macOS, but it should work on any platform where Python has support and has bash , except for Windows. Before installing the tool, make sure to have installed at least rsync and bash . Most Linux distributions have both installed, some only bash . On macOS, both come installed by default. Also check that Python 3.6 or higher is installed (use [ brew ] on macOS for that). In this guide, a virtual environment will be used to install and use the tool. It is not recommended to install it directly in the system. First prepare the virtual environment. You can use venv or virtualenv , but the first will be used. python -m venv .venv python -m virtualenv .venv Note: python here it is referred to the python 3 executable. In some platforms will be python3 . Once the environment is created, we must \u201cactivate\u201d it: . .venv/bin/activate Now you can use python and pip and everything will work from and install to the virtual environment. Now you can download the tool and install it: #Download using curl... curl -sSL https://github.com/MajorcaDevs/mdbackup/releases/latest/download/mdbackup.whl > mdbackup.whl #...or wget wget https://github.com/MajorcaDevs/mdbackup/releases/latest/download/mdbackup.whl #If they don't work, go to https://github.com/MajorcaDevs/mdbackup/releases and copy the URL from the latest release pip install mdbackup.whl mdbackup --help To check if the tool is installed properly, run the help of the tool and you should get something like this . You can also use the Docker container . But it is recommended to read the guide to get an idea. First configuration \u00b6 Note: this will get through getting an initial configuration for backups. To get in more detail, check out the Configuration page. In order to get your first backup, the tool must be configured properly. To achieve this, you will learn the core concepts used in the tool and how to use them to fit your needs. The tool needs three folders to work: config : a folder where the configuration, and other files related to configuration, tokens or cookies are going to be stored. config / tasks : a folder where the backup logic is stored in form of yaml or json files. *put a full path here*: the folder, placed in some folder, where the backups are going to be stored. The folder in where you are right now should have the following structure: .venv / \u2026 config / config . json tasks / 01 . yaml mdbackup . whl And the third folder, it does not matter where is placed, but it will be used soon to store backups. It can be a network storage, an external drive or a partition in some local drive. It is recommended to store them outside the root partition ( / ), if possible. Did you notice the config . json ? This file holds the configuration of the tool. Write in it the following: { \"backupsPath\" : \"/the/path/to/the/folder/where/the/backups/are/going/to/be/stored\" , \"logLevel\" : \"DEBUG\" , \"env\" : {} } Note You can download the JSON Schema and use it to validate the structure: \"$schema\" : \"./config.schema.json\" , . You can grab it from the latest release. yaml is an option You can also use yaml for configuration and tasks file, if you prefer. This configuration is really basic and tells the tool where to place the backups, which log level to use (will be very verbose, but it is OK for now) and to inject no extra environment variables. Now we need to define the logic to create backups. We use the term tasks to refer a group of actions that will run in order to backup something. Tasks can be grouped in a tasks definition file and stored in a json or yaml file inside the config / tasks folder. An action is just something that accepts an input and some parameters and transforms the input to something else, but it can be also something that writes the input to a file or a folder, or a source of data that does not receive any input and writes data as output. The steps are executed following the natural order (alphanumeric order) of the names of the files. For example, 01 . json will run before 02 . json . It can be as many files as desired, or just one. It does not matter. So now we will give some contents to 01 . yaml (the file shown in the tree). YAML syntax name : 'test' tasks : - name : Backup home actions : - from-directory : /home/user #macOS users, use \"/Users/YourUser\" - to-directory : path : home JSON syntax { \"name\" : \"test\" , \"tasks\" : [ { \"name\" : \"Backup home\" , \"actions\" : [ { \"from-directory\" : \"/home/user\" }, { \"to-directory\" : { \"path\" : \"home\" } } ] } ] } This task will copy your home directory and all its contents into the backup folder. You can use any other folder you want just to try, this is an example. Now try running the tool: mdbackup . If everything is well configured, you will have a new folder in the backups folder with the date and time of now and with your folder copied. Well, try now to make a backup again. If the folder being copied is large enough, you will notice that this time, the backup took less time than the first time. This is because the action to - directory takes into account the previous backup and will try to do an incremental backup: if the file to copy already exists in the previous backup and has not been modified since the last time, then it will do a hard - link instead of a copy. This trick only works for some of the actions, check their documentation to know what are the ones doing this. Note that current folder is always present and is a soft link to the latest backup. So it\u2019s easy to access to the latest backup from the file explorer or from the command line :) Now you have backups of whatever you want! Just configure the tool and write the right scripts to fit your needs. SuperUser rights may be needed It is possible that you will need to run the tool as root to access some system folders. Remember that the virtual environment is not inherited when using sudo . Make your own script, or checkout one of the contrib folder . Injecting environment variables \u00b6 Actions receive parameters to be able to do their job. These parameters can be defined previously in env sections either in the config file, in the tasks definition file or inside a task. The value of a variable can refer to a secret, which is identified by using a # hashtag at the begining of the string. Secrets won\u2019t be covered in the quick start, but is good to know :) Let\u2019s try to make a backup from a postgres database with the predefined action. For this example, pg_dump must be installed on the system. Time to add a new tasks file called 02 . yaml with the following contents: YAML syntax name : PostgreSQL example env : host : localhost user : postgres password : WonderfulPassword123 tasks : - name : Postgres task example actions : - postgres-database : database : test1 - to-file : path : test1.sql - name : Postgres task example 2 actions : - postgres-database : database : test2 - to-file : path : test2.sql JSON syntax { \"name\" : \"PostgreSQL example\" , \"env\" : { \"host\" : \"localhost\" , \"user\" : \"postgres\" , \"password\" : \"WonderfulPassword123\" }, \"tasks\" : [ { \"name\" : \"Postgres task example\" , \"actions\" : [ { \"postgres-database\" : { \"database\" : \"test1\" } }, { \"to-file\" : { \"path\" : \"test1.sql\" } } ] }, { \"name\" : \"Postgres task example 2\" , \"actions\" : [ { \"postgres-database\" : { \"database\" : \"test2\" } }, { \"to-file\" : { \"path\" : \"test2.sql\" } } ] } ] } The tool will read the environment variables, and inject them in the actions parameters. If everything went well, you now will have a test1 . sql and test2 . sql files in the backup folder. YAML is better for tasks Check the YAML and JSON syntax in the examples for the tasks\u2026 We think that YAML is better for writing tasks, lesser to write and easier to understand. Compression \u00b6 What if your postgres backup takes some MB and you think \u201cwhat if the file were compressed, will be a few KBs?\u201d You can add a compress action in the middle of the actions to compress the output. See the example: ... actions : - postgres-database : database : test1 - compress-gz : {} #Compress using `gzip` (the command) - to-file : path : test1.sql.gz ... The compress - gz action receives data as input and compresses it using gzip command. And that\u2019s how you compress some data. In addition, when uploading folders to a storage provider, they automatically are archived into a tar file. If you want to save some bits, you can also compress them by adding the following configuration in the configuration file: { \"...\" : \"...\" , \"compression\" : { \"strategy\" : \"gzip\" , \"level\" : 7 } } Encrypt \u00b6 You need OpenGPG 2 in order to cypher files. And as well as the compression, will be used automatically to upload to storage providers. Most Linux distributions have installed gpg tools, but you must check the version. On macOS, install GPGTools . There\u2019s two ways to encrypt data, in this guide will be using the passphrase. Here is a configuration example: { \"...\" : \"...\" , \"cypher\" : { \"strategy\" : \"gpg-passphrase\" , \"passphrase\" : \"ThisIsAPassphrase321\" , \"algorithm\" : \"AES256\" } } But, if instead what you want is to encrypt a file in a task, there is an action for you: ... actions : - postgres-database : database : test1 - encrypt-gpg : passphrase : AVeryPowerfulPassw0rd:) - to-file : path : test1.sql.asc ... The encrypt - gpg action will use gpg to encrypt the input data. You may even compress and encrypt the data in a task (preferible in this order): ... actions : - postgres-database : database : test1 - compress-gz : {} - encrypt-gpg : passphrase : AVeryPowerfulPassw0rd:) - to-file : path : test1.sql.asc ... Note The algorithm changes between distributions. Check the available in gpg --version . In this guide we will be using AES256. Upload to the cloud \u00b6 Well, you can also use an FTP server for this, but it\u2019s cool to say \u201cTO THE CLOUD\u201d . If desired, the backups can be uploaded to what we call \u201cstorage provider\u201d. It can be a FTP or SFTP server, or a Cloud Storage (like S3 or Google Drive). This is run after the backup is done, and can be uploaded to one or more storage providers. Also to speed up the upload, the folders are written into a tar file, compressed (if configured) and encrypted (if configured). It is recommended to, at least, configure the compression in order to save some storage at the cloud. There are many storage providers to choose. In the guid will be using a FTP server to quickly show how to upload files. Also because this is the only storage provider that does not need to install any extra packages. This is the configuration for the FTP: { \"...\" : \"...\" , \"storage\" : [ { \"type\" : \"ftp\" , \"backupsPath\" : \"/backups\" , \"host\" : \"ftp.local\" , \"user\" : \"anonymous\" } ] } This will upload the files to the FTP server ftp . local , in the folder / backups using the anonymous user and no password. The end \u00b6 There\u2019s more to learn about the tool, but this is a rather good introduction to it. Take a look to the other sections of this documentation to learn and discover new stuff of the tool. Concerned about too much hardcoded credentials in the configuration file? Check out the Secret backends .","title":"Quick start"},{"location":"quick-start/#quick-start-guide","text":"mdbackup is tested under Linux and macOS, but it should work on any platform where Python has support and has bash , except for Windows. Before installing the tool, make sure to have installed at least rsync and bash . Most Linux distributions have both installed, some only bash . On macOS, both come installed by default. Also check that Python 3.6 or higher is installed (use [ brew ] on macOS for that). In this guide, a virtual environment will be used to install and use the tool. It is not recommended to install it directly in the system. First prepare the virtual environment. You can use venv or virtualenv , but the first will be used. python -m venv .venv python -m virtualenv .venv Note: python here it is referred to the python 3 executable. In some platforms will be python3 . Once the environment is created, we must \u201cactivate\u201d it: . .venv/bin/activate Now you can use python and pip and everything will work from and install to the virtual environment. Now you can download the tool and install it: #Download using curl... curl -sSL https://github.com/MajorcaDevs/mdbackup/releases/latest/download/mdbackup.whl > mdbackup.whl #...or wget wget https://github.com/MajorcaDevs/mdbackup/releases/latest/download/mdbackup.whl #If they don't work, go to https://github.com/MajorcaDevs/mdbackup/releases and copy the URL from the latest release pip install mdbackup.whl mdbackup --help To check if the tool is installed properly, run the help of the tool and you should get something like this . You can also use the Docker container . But it is recommended to read the guide to get an idea.","title":"Quick start guide"},{"location":"quick-start/#first-configuration","text":"Note: this will get through getting an initial configuration for backups. To get in more detail, check out the Configuration page. In order to get your first backup, the tool must be configured properly. To achieve this, you will learn the core concepts used in the tool and how to use them to fit your needs. The tool needs three folders to work: config : a folder where the configuration, and other files related to configuration, tokens or cookies are going to be stored. config / tasks : a folder where the backup logic is stored in form of yaml or json files. *put a full path here*: the folder, placed in some folder, where the backups are going to be stored. The folder in where you are right now should have the following structure: .venv / \u2026 config / config . json tasks / 01 . yaml mdbackup . whl And the third folder, it does not matter where is placed, but it will be used soon to store backups. It can be a network storage, an external drive or a partition in some local drive. It is recommended to store them outside the root partition ( / ), if possible. Did you notice the config . json ? This file holds the configuration of the tool. Write in it the following: { \"backupsPath\" : \"/the/path/to/the/folder/where/the/backups/are/going/to/be/stored\" , \"logLevel\" : \"DEBUG\" , \"env\" : {} } Note You can download the JSON Schema and use it to validate the structure: \"$schema\" : \"./config.schema.json\" , . You can grab it from the latest release. yaml is an option You can also use yaml for configuration and tasks file, if you prefer. This configuration is really basic and tells the tool where to place the backups, which log level to use (will be very verbose, but it is OK for now) and to inject no extra environment variables. Now we need to define the logic to create backups. We use the term tasks to refer a group of actions that will run in order to backup something. Tasks can be grouped in a tasks definition file and stored in a json or yaml file inside the config / tasks folder. An action is just something that accepts an input and some parameters and transforms the input to something else, but it can be also something that writes the input to a file or a folder, or a source of data that does not receive any input and writes data as output. The steps are executed following the natural order (alphanumeric order) of the names of the files. For example, 01 . json will run before 02 . json . It can be as many files as desired, or just one. It does not matter. So now we will give some contents to 01 . yaml (the file shown in the tree). YAML syntax name : 'test' tasks : - name : Backup home actions : - from-directory : /home/user #macOS users, use \"/Users/YourUser\" - to-directory : path : home JSON syntax { \"name\" : \"test\" , \"tasks\" : [ { \"name\" : \"Backup home\" , \"actions\" : [ { \"from-directory\" : \"/home/user\" }, { \"to-directory\" : { \"path\" : \"home\" } } ] } ] } This task will copy your home directory and all its contents into the backup folder. You can use any other folder you want just to try, this is an example. Now try running the tool: mdbackup . If everything is well configured, you will have a new folder in the backups folder with the date and time of now and with your folder copied. Well, try now to make a backup again. If the folder being copied is large enough, you will notice that this time, the backup took less time than the first time. This is because the action to - directory takes into account the previous backup and will try to do an incremental backup: if the file to copy already exists in the previous backup and has not been modified since the last time, then it will do a hard - link instead of a copy. This trick only works for some of the actions, check their documentation to know what are the ones doing this. Note that current folder is always present and is a soft link to the latest backup. So it\u2019s easy to access to the latest backup from the file explorer or from the command line :) Now you have backups of whatever you want! Just configure the tool and write the right scripts to fit your needs. SuperUser rights may be needed It is possible that you will need to run the tool as root to access some system folders. Remember that the virtual environment is not inherited when using sudo . Make your own script, or checkout one of the contrib folder .","title":"First configuration"},{"location":"quick-start/#injecting-environment-variables","text":"Actions receive parameters to be able to do their job. These parameters can be defined previously in env sections either in the config file, in the tasks definition file or inside a task. The value of a variable can refer to a secret, which is identified by using a # hashtag at the begining of the string. Secrets won\u2019t be covered in the quick start, but is good to know :) Let\u2019s try to make a backup from a postgres database with the predefined action. For this example, pg_dump must be installed on the system. Time to add a new tasks file called 02 . yaml with the following contents: YAML syntax name : PostgreSQL example env : host : localhost user : postgres password : WonderfulPassword123 tasks : - name : Postgres task example actions : - postgres-database : database : test1 - to-file : path : test1.sql - name : Postgres task example 2 actions : - postgres-database : database : test2 - to-file : path : test2.sql JSON syntax { \"name\" : \"PostgreSQL example\" , \"env\" : { \"host\" : \"localhost\" , \"user\" : \"postgres\" , \"password\" : \"WonderfulPassword123\" }, \"tasks\" : [ { \"name\" : \"Postgres task example\" , \"actions\" : [ { \"postgres-database\" : { \"database\" : \"test1\" } }, { \"to-file\" : { \"path\" : \"test1.sql\" } } ] }, { \"name\" : \"Postgres task example 2\" , \"actions\" : [ { \"postgres-database\" : { \"database\" : \"test2\" } }, { \"to-file\" : { \"path\" : \"test2.sql\" } } ] } ] } The tool will read the environment variables, and inject them in the actions parameters. If everything went well, you now will have a test1 . sql and test2 . sql files in the backup folder. YAML is better for tasks Check the YAML and JSON syntax in the examples for the tasks\u2026 We think that YAML is better for writing tasks, lesser to write and easier to understand.","title":"Injecting environment variables"},{"location":"quick-start/#compression","text":"What if your postgres backup takes some MB and you think \u201cwhat if the file were compressed, will be a few KBs?\u201d You can add a compress action in the middle of the actions to compress the output. See the example: ... actions : - postgres-database : database : test1 - compress-gz : {} #Compress using `gzip` (the command) - to-file : path : test1.sql.gz ... The compress - gz action receives data as input and compresses it using gzip command. And that\u2019s how you compress some data. In addition, when uploading folders to a storage provider, they automatically are archived into a tar file. If you want to save some bits, you can also compress them by adding the following configuration in the configuration file: { \"...\" : \"...\" , \"compression\" : { \"strategy\" : \"gzip\" , \"level\" : 7 } }","title":"Compression"},{"location":"quick-start/#encrypt","text":"You need OpenGPG 2 in order to cypher files. And as well as the compression, will be used automatically to upload to storage providers. Most Linux distributions have installed gpg tools, but you must check the version. On macOS, install GPGTools . There\u2019s two ways to encrypt data, in this guide will be using the passphrase. Here is a configuration example: { \"...\" : \"...\" , \"cypher\" : { \"strategy\" : \"gpg-passphrase\" , \"passphrase\" : \"ThisIsAPassphrase321\" , \"algorithm\" : \"AES256\" } } But, if instead what you want is to encrypt a file in a task, there is an action for you: ... actions : - postgres-database : database : test1 - encrypt-gpg : passphrase : AVeryPowerfulPassw0rd:) - to-file : path : test1.sql.asc ... The encrypt - gpg action will use gpg to encrypt the input data. You may even compress and encrypt the data in a task (preferible in this order): ... actions : - postgres-database : database : test1 - compress-gz : {} - encrypt-gpg : passphrase : AVeryPowerfulPassw0rd:) - to-file : path : test1.sql.asc ... Note The algorithm changes between distributions. Check the available in gpg --version . In this guide we will be using AES256.","title":"Encrypt"},{"location":"quick-start/#upload-to-the-cloud","text":"Well, you can also use an FTP server for this, but it\u2019s cool to say \u201cTO THE CLOUD\u201d . If desired, the backups can be uploaded to what we call \u201cstorage provider\u201d. It can be a FTP or SFTP server, or a Cloud Storage (like S3 or Google Drive). This is run after the backup is done, and can be uploaded to one or more storage providers. Also to speed up the upload, the folders are written into a tar file, compressed (if configured) and encrypted (if configured). It is recommended to, at least, configure the compression in order to save some storage at the cloud. There are many storage providers to choose. In the guid will be using a FTP server to quickly show how to upload files. Also because this is the only storage provider that does not need to install any extra packages. This is the configuration for the FTP: { \"...\" : \"...\" , \"storage\" : [ { \"type\" : \"ftp\" , \"backupsPath\" : \"/backups\" , \"host\" : \"ftp.local\" , \"user\" : \"anonymous\" } ] } This will upload the files to the FTP server ftp . local , in the folder / backups using the anonymous user and no password.","title":"Upload to the cloud"},{"location":"quick-start/#the-end","text":"There\u2019s more to learn about the tool, but this is a rather good introduction to it. Take a look to the other sections of this documentation to learn and discover new stuff of the tool. Concerned about too much hardcoded credentials in the configuration file? Check out the Secret backends .","title":"The end"},{"location":"run-as-service/","text":"Automating running of backups \u00b6 In this section, systemd and cron ways are going to be explained. systemd is the preferred way in case your system has it. It is supposed you already have configured the environment and it works. For both ways, download the file automated-script.sh in some place. In this example, the same folder, where the venv , configuration and steps are located, is going to be used. curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/automated-script.sh > automated-script.sh chmod +x automated-script.sh systemd \u00b6 For systemd, you need to download backups.service and backups.timer , copy them to / etc / systemd / system and enable the timer. sudo bash - c \"curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/backups.service > /etc/systemd/system/backups.service\" sudo bash - c \"curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/backups.timer > /etc/systemd/system/backups.timer\" sudo nano / etc / systemd / system / backups . service # Modify the path to the script !! sudo nano / etc / systemd / system / backups . timer # Check when the timer is going to fire !! sudo systemctl enable backups . timer sudo systemctl start backups . timer nano automated - script . sh # Modify the path of CONFIG_FOLDER You must modify the path to the script in the backups . service and the CONFIG_FOLDER in the automated - script . sh . It is recommended to check if you like when the timer is going to fire (by default is every night at 1am). crontab \u00b6 For crontab, you need to edit the root s crontab and add a new entry. Also modify the automated - script . sh file to remove the comment in the last line to have logs visible in the same folder where the configuration and steps are placed. An example of crontab entry could be: 0 1 * * * / backups / tool / automated - script . sh","title":"Run as a service"},{"location":"run-as-service/#automating-running-of-backups","text":"In this section, systemd and cron ways are going to be explained. systemd is the preferred way in case your system has it. It is supposed you already have configured the environment and it works. For both ways, download the file automated-script.sh in some place. In this example, the same folder, where the venv , configuration and steps are located, is going to be used. curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/automated-script.sh > automated-script.sh chmod +x automated-script.sh","title":"Automating running of backups"},{"location":"run-as-service/#systemd","text":"For systemd, you need to download backups.service and backups.timer , copy them to / etc / systemd / system and enable the timer. sudo bash - c \"curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/backups.service > /etc/systemd/system/backups.service\" sudo bash - c \"curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/backups.timer > /etc/systemd/system/backups.timer\" sudo nano / etc / systemd / system / backups . service # Modify the path to the script !! sudo nano / etc / systemd / system / backups . timer # Check when the timer is going to fire !! sudo systemctl enable backups . timer sudo systemctl start backups . timer nano automated - script . sh # Modify the path of CONFIG_FOLDER You must modify the path to the script in the backups . service and the CONFIG_FOLDER in the automated - script . sh . It is recommended to check if you like when the timer is going to fire (by default is every night at 1am).","title":"systemd"},{"location":"run-as-service/#crontab","text":"For crontab, you need to edit the root s crontab and add a new entry. Also modify the automated - script . sh file to remove the comment in the last line to have logs visible in the same folder where the configuration and steps are placed. An example of crontab entry could be: 0 1 * * * / backups / tool / automated - script . sh","title":"crontab"},{"location":"tasks/","text":"Tasks \u00b6 A task is a group of actions that conforms a full backup of something (generates a file or a directory in the backup path for the current run). Tasks are defined inside a tasks definition file that contains a group of tasks that may share something in common - like all postgres backups will be in a file. The files can be defined using json or yaml syntax, and are located in the folder ${ configPath } /tasks . Tasks definition file \u00b6 YAML syntax name : Good name that identifies this group of tasks (optional - file name will be used instead) env : one-variable : 1 another-variable : true 101-variable : 'yes' : yes 'no' : no inside : this/folder tasks : - name : Task 1 env : more-variables : 'yes it is' stopOnFail : True actions : - from-file : /etc/hosts - compress-gz : {} - to-file : path : hosts.gz JSON syntax { \"name\" : \"Good name that identifies this group of tasks (optional - file name will be used instead)\" , \"env\" : { \"one-variable\" : 1 , \"another-variable\" : true , \"101-variable\" : { \"yes\" : true , \"no\" : false } }, \"inside\" : \"this/folder\" , \"tasks\" : [ { \"name\" : \"Task 1\" , \"env\" : { \"more-variables\" : \"yes it is\" }, \"stopOnFail\" : false , \"actions\" : [ { \"from-file\" : \"/etc/hosts\" }, { \"compress-gz\" : {} }, { \"to-file\" : { \"path\" : \"hosts.gz\" } } ] } ] } One file will look like the above example. The example has all possible options that can have. Let\u2019s treat each of them. Name \u00b6 The name of the tasks group/tasks definition file. To be able to identify each of them, a name is used. If name is not provided, then it will use the name of the file without extension as name. inside \u00b6 If defined, then all files and directory created by output actions will be stored inside this path in the current backup path. env \u00b6 Defines variables that can be used in actions as parameters. They can also refer to secrets using # secret - name . If a variable matches with a key of a parameter for an action, this will be used as default value if the parameter is not defined in the action. These variables can be referenced inside a string by using ${ VARIABLE_NAME } . tasks \u00b6 Defines all tasks that will be run in this file. One task contains its name and the actions to run. Optionally, it can define more variables in the env section. If stopOnFail is set to false and the task fails, it won\u2019t stop the whole backup. The actions are defined with one item in the list by action, and to identify the action, the key of the dictionary is used: - action-name : parameters - action-name : parameters Referring to secrets in env sections \u00b6 Every time a variable has a string starting with # , those will be treated as secret references. When the task is going to run, the references are resolved with the real values of the secrets. The reference refers to a key path that can be found in a envDefs of any of the secret backends. Examples are better: Imagine that the following secret backend config is set with this envDefs : ... envDefs : postgres : user : 'secret/databases/postgres/pg-charizard-01#username' password : 'secret/databases/postgres/pg-charizard-01#password' encrypt-passphrase : 'secret/backups/vm-do-charizard-01/passphrase#passphrase' ... So to refer to the user of postgres, this string will be used # postgres . user , as well as the password # postgres . password . For the passphrase, # encrypt - passphrase will be used. This way, secrets are referred from the tasks using a key, and changing the path in the envDefs , will be changed in all the tasks that references the secret. Environment variables in parameters \u00b6 In the action parameters and the env sections, environment variables anv variables defined in env sections can be referred to customize even further the tasks. To reference a environment variable, just use it as ${ ENV_VAR } in a string and it will be converted into its value. If a variable cannot be found, it will replace the token with an empty string (just delete the variable token). Variables in env sections can reference another variables in the same section because variable substitution is done just before running a task. env sections and variable substitution Strings and numbers are the only types supported for the variable substitution, any other type of variable will ignore it and a warning will be writen in the logs. When referencing variables from the same env section, order is important. Examples \u00b6 Simple file backup tasks : - name : File copy actions : - from-file : /path/to/the/file - to-file : path : file Run of a command, and the output is stored compressed in a file tasks : - name : Run a command actions : - command : args : [ command , parameter , parameter , parameter ] - compress-xz : threads : 2 - to-file : path : compressed-file.xz Copy a folder tasks : - name : Copy a folder actions : - from-directory : /path/to/the/folder - to-directory : path : folder reflink : yes Copy a folder, archive it, compress it, encrypt it and stored it in a file tasks : - name : Do a lot of things with a folder actions : - from-directory : /path/to/a/folder - tar : {} - compress-br : {} - encrypt-gpg : passphrase : '#gpg-password' - to-file : path : folderino.tar.br.asc Referrencing environment variables ```yaml tasks: - name: do stuff env: var1: yes var2: maybe {var1} # This should be defined after var1 actions: - from-directory: ' {var1} # This should be defined after var1 actions: - from-directory: ' /files\u2019 - tar: {} - compress-br: {} - to-file: path: ${var2}.tar.br","title":"Tasks"},{"location":"tasks/#tasks","text":"A task is a group of actions that conforms a full backup of something (generates a file or a directory in the backup path for the current run). Tasks are defined inside a tasks definition file that contains a group of tasks that may share something in common - like all postgres backups will be in a file. The files can be defined using json or yaml syntax, and are located in the folder ${ configPath } /tasks .","title":"Tasks"},{"location":"tasks/#tasks-definition-file","text":"YAML syntax name : Good name that identifies this group of tasks (optional - file name will be used instead) env : one-variable : 1 another-variable : true 101-variable : 'yes' : yes 'no' : no inside : this/folder tasks : - name : Task 1 env : more-variables : 'yes it is' stopOnFail : True actions : - from-file : /etc/hosts - compress-gz : {} - to-file : path : hosts.gz JSON syntax { \"name\" : \"Good name that identifies this group of tasks (optional - file name will be used instead)\" , \"env\" : { \"one-variable\" : 1 , \"another-variable\" : true , \"101-variable\" : { \"yes\" : true , \"no\" : false } }, \"inside\" : \"this/folder\" , \"tasks\" : [ { \"name\" : \"Task 1\" , \"env\" : { \"more-variables\" : \"yes it is\" }, \"stopOnFail\" : false , \"actions\" : [ { \"from-file\" : \"/etc/hosts\" }, { \"compress-gz\" : {} }, { \"to-file\" : { \"path\" : \"hosts.gz\" } } ] } ] } One file will look like the above example. The example has all possible options that can have. Let\u2019s treat each of them.","title":"Tasks definition file"},{"location":"tasks/#name","text":"The name of the tasks group/tasks definition file. To be able to identify each of them, a name is used. If name is not provided, then it will use the name of the file without extension as name.","title":"Name"},{"location":"tasks/#inside","text":"If defined, then all files and directory created by output actions will be stored inside this path in the current backup path.","title":"inside"},{"location":"tasks/#env","text":"Defines variables that can be used in actions as parameters. They can also refer to secrets using # secret - name . If a variable matches with a key of a parameter for an action, this will be used as default value if the parameter is not defined in the action. These variables can be referenced inside a string by using ${ VARIABLE_NAME } .","title":"env"},{"location":"tasks/#tasks_1","text":"Defines all tasks that will be run in this file. One task contains its name and the actions to run. Optionally, it can define more variables in the env section. If stopOnFail is set to false and the task fails, it won\u2019t stop the whole backup. The actions are defined with one item in the list by action, and to identify the action, the key of the dictionary is used: - action-name : parameters - action-name : parameters","title":"tasks"},{"location":"tasks/#referring-to-secrets-in-env-sections","text":"Every time a variable has a string starting with # , those will be treated as secret references. When the task is going to run, the references are resolved with the real values of the secrets. The reference refers to a key path that can be found in a envDefs of any of the secret backends. Examples are better: Imagine that the following secret backend config is set with this envDefs : ... envDefs : postgres : user : 'secret/databases/postgres/pg-charizard-01#username' password : 'secret/databases/postgres/pg-charizard-01#password' encrypt-passphrase : 'secret/backups/vm-do-charizard-01/passphrase#passphrase' ... So to refer to the user of postgres, this string will be used # postgres . user , as well as the password # postgres . password . For the passphrase, # encrypt - passphrase will be used. This way, secrets are referred from the tasks using a key, and changing the path in the envDefs , will be changed in all the tasks that references the secret.","title":"Referring to secrets in env sections"},{"location":"tasks/#environment-variables-in-parameters","text":"In the action parameters and the env sections, environment variables anv variables defined in env sections can be referred to customize even further the tasks. To reference a environment variable, just use it as ${ ENV_VAR } in a string and it will be converted into its value. If a variable cannot be found, it will replace the token with an empty string (just delete the variable token). Variables in env sections can reference another variables in the same section because variable substitution is done just before running a task. env sections and variable substitution Strings and numbers are the only types supported for the variable substitution, any other type of variable will ignore it and a warning will be writen in the logs. When referencing variables from the same env section, order is important.","title":"Environment variables in parameters"},{"location":"tasks/#examples","text":"Simple file backup tasks : - name : File copy actions : - from-file : /path/to/the/file - to-file : path : file Run of a command, and the output is stored compressed in a file tasks : - name : Run a command actions : - command : args : [ command , parameter , parameter , parameter ] - compress-xz : threads : 2 - to-file : path : compressed-file.xz Copy a folder tasks : - name : Copy a folder actions : - from-directory : /path/to/the/folder - to-directory : path : folder reflink : yes Copy a folder, archive it, compress it, encrypt it and stored it in a file tasks : - name : Do a lot of things with a folder actions : - from-directory : /path/to/a/folder - tar : {} - compress-br : {} - encrypt-gpg : passphrase : '#gpg-password' - to-file : path : folderino.tar.br.asc Referrencing environment variables ```yaml tasks: - name: do stuff env: var1: yes var2: maybe {var1} # This should be defined after var1 actions: - from-directory: ' {var1} # This should be defined after var1 actions: - from-directory: ' /files\u2019 - tar: {} - compress-br: {} - to-file: path: ${var2}.tar.br","title":"Examples"},{"location":"actions/","text":"Actions \u00b6 An action is a unit of work inside a backup that does one thing. An action can receive data or a folder and/or can produce data or a folder. Depending on the type of input and output, an action can be initial , transformer or final . By combining actions, a backup can be done (that is, a task ). An initial action is an action that does not receive anything as input and produces an output. These actions must be the first to appear in the task pipeline. A transformer action is an action that receives something as input and applies some transformation on it to produce a different output. These actions must appear in between the task pipeline. A final action is an action taht receives something as input and do not produce an output (for the pipeline). These actions must appear last in the task pipeline. Note that final actions in general produces a file or a folder in the backup folder. Initial and final actions An action can be initial and final. For example, copy a file or a folder to the backup folder is both initial and final action. If using these kind of actions can only appear one action in a task pipeline. Initial or transformer actions For example, command , ssh and docker actions can be either an initial or transformer actions, it just depends if the command to run needs an input from another source or not. For example the command echo hello world don\u2019t need an input, but cat needs one. Not yet mentioned, but unactions also exist. Think of unactions as a reversed action. If an action is to compress some data, its unaction will be decompress the data. unactions are like actions in all senses, but the behaviour is to revert an action (in general used in restores). Builtin actions \u00b6 There is a list of builtin actions that can be used to create the tasks pipelines or to create new actions by using these as base actions. The actions are grouped by category. See the list here: Archive Command Compress Database Directory Encrypt File Network Implementing actions \u00b6 In python, an action (and unaction ) is just a function that receives two arguments and returns something. An initial action will receive None and the parameters as arguments and should return a data stream or a DirEntry iterator. A transformer action will receive a data stream or a DirEntry iterator (depending on the expected input) and the parameters as arguments and will return a data stream or a DirEntry iterator. A final action will receive a data stream or a DirEntry iterator and must return the relative path of the file or folder that the action has created. An action can use another action internally to make a composition of actions. DirEntry Is a data structure used internally to represent a entry of a folder. The object is defined like this: class DirEntry : type : str path : Path stats : os . stat_result stream : Optional [ io . IOBase ] link_content : Optional [ str ] real_path : Optional [ Path ] def __init__ ( self , _type : str , path : Path , stats , stream = None , link_content = None , real_path = None , ** kwargs ): pass In actions, a data stream means a io . FileIO , io . BufferedIOBase or io . TextIOBase object that has a file descriptor associated to the stream, or a subprocess . Popen object with PIPE mode set to stdout and stderr . Currently, the file descriptor -thing is important because they are used to be used in external processes efficently (without using internal pipes). mdbackup checks if a data stream is invalid and raises an error for it. On the other hand, the DirEntry iterator is implemented using a generator function that returns DirEntry objects for each entry found in the folder. The type of a entry can be dir , symlink or file . A file will have the stream attribute filled pointing to the contents of the file. A symlink will have the link_content attribute set with the contents of the symlink. In all types, the path must be filled with a relative path to the entry, as well as stats , requiring st_mode , st_uid , st_gid , st_mtime , st_size properties to be filled. Example of initial action def action_read_file ( _ , params : dict ) -> io . FileIO : return open ( params [ 'path' ], 'rb' , buffering = 0 ) Example of transform action def action_compress_gzip ( inp : InputDataStream , params ) -> subprocess . Popen : compression_level = params . get ( 'compressionLevel' ) args = [ 'gzip' , '-c' ] if compression_level is not None : args . append ( f '-{compression_level}' ) # Composition of actions: using command action to create the compress-gz action return action_command ( inp , { 'args' : args }) def action_example_of_dir_entry ( inp , params ): for entry in inp : if entry . type == 'file' : entry . path = Path ( str ( entry . path ) + '.bak' ) yield entry Example of final action def action_write_file ( inp : InputDataStream , params ): # The _backup_path parameter is internal, and can be used freely full_path = Path ( params [ '_backup_path' ]) / Path ( params [ 'path' ]) file_object = open ( full_path , 'wb' , buffering = 0 ) chunk_size = params . get ( 'chunkSize' , 1024 * 8 ) data = inp . read ( chunk_size ) while data is not None and len ( data ) != 0 : file_object . write ( data ) data = inp . read ( chunk_size ) file_object . close () return full_path Registring actions \u00b6 Using the register_action function, an action (and its unaction counterpart) will be registered in the system and could be used in the tasks. There is also a quick way to register actions and unactions which is using the action and unaction decorators. For user-defined actions, those functions will be received in the function called when registering from a module. register_action \u00b6 The function registers an action into the actions container in order to be used when running tasks. Argument Type Description Optional Default identifier str Defines the identifier to be used to refer the action in the yaml files No action callable The function that implements the action No unaction callable The function that implements the unaction Yes None expected_input Optional [ str ] The expected input for the action Yes None output Optional [ str ] The output for the action Yes None The supported values for expected_input are: stream or directory . The supported values for output are: stream stream : file , stream : process , stream : pipe (only if using os . pipe () fds) or directory . action \u00b6 Decorator for actions that will register them automatically (once the module that contains the implementation is loaded). Is a shortcut for register_action and allows in-place registration of actions. Argument Type Description Optional Default identifier str Defines the identifier to be used to refer the action in the yaml files No input Optional [ str ] The expected input for the action Yes None output Optional [ str ] The output for the action Yes None unaction callable The function that implements the unaction (use it only if @unaction () won\u2019t be used) Yes None Parameters input is the same as expected_input from the register_action . All parameters expect the same as in register_action . unaction \u00b6 Decorator for unactions that will register them automatically (once the module that contains the implementation is loaded). Is a shortcut for register_action and allows in-place registration of unactions once its action counterpart is registered. Argument Type Description Optional Default identifier str Defines the identifier to be used to refer the action in the yaml files No Warning An unaction can be registered after the action has been registered. If the order is inverted, the unaction register will fail. Creating user-defined actions \u00b6 As mentioned in the configuration , the user-defined actions must be modules that can be loaded from python using the import syntax. The value in the configuration was module #function where module is the module to import and function is the name of the callable object that will register all actions. The function (or callable object) will receive some keyword arguments (aka kwargs ) with the register_action function and action and unaction decorators to be used quickly without the need to import them. You can also import them directly by importing the module mdbackup . actions . container . The function will also receive get_action and get_unaction that retrieves the implementation for the action/ unaction for the given identifier (the first parameter of both functions). And lastly, the kwarg dir_entry will be passed with the class DirEntry if needed. The function must register all actions by either using the function or the decorator. If anything fails, your function should raise an exception to notify the failure, and put some debug logs will also help. def register_my_actions ( action , unaction , get_action , ** kwargs ): action_ssh = get_action ( 'ssh' ) @action ( 'my-read-file' , output = 'stream:file' ) def action_read_file ( _ , params : dict ): return open ( params [ 'path' ], 'rb' , buffering = 0 ) @action ( 'pi-copy' , output = 'stream:process' ) def action_pi_copy ( _ , params : dict ): return action_ssh ( _ , { 'host' : 'pi' , 'user' : 'pi' , 'password' : 'raspberrypi' , 'knownHostsPolicy' : 'ignore' , # You should never do this 'args' : [ 'bash' , '-c' , f 'if [[ -f \"{params[\"path\"]}\" ]]; then cat \"{params[\"path\"]}\"; else cd \"{params[\"path\"]}\"; tar -c .; fi' ] })","title":"Actions overview"},{"location":"actions/#actions","text":"An action is a unit of work inside a backup that does one thing. An action can receive data or a folder and/or can produce data or a folder. Depending on the type of input and output, an action can be initial , transformer or final . By combining actions, a backup can be done (that is, a task ). An initial action is an action that does not receive anything as input and produces an output. These actions must be the first to appear in the task pipeline. A transformer action is an action that receives something as input and applies some transformation on it to produce a different output. These actions must appear in between the task pipeline. A final action is an action taht receives something as input and do not produce an output (for the pipeline). These actions must appear last in the task pipeline. Note that final actions in general produces a file or a folder in the backup folder. Initial and final actions An action can be initial and final. For example, copy a file or a folder to the backup folder is both initial and final action. If using these kind of actions can only appear one action in a task pipeline. Initial or transformer actions For example, command , ssh and docker actions can be either an initial or transformer actions, it just depends if the command to run needs an input from another source or not. For example the command echo hello world don\u2019t need an input, but cat needs one. Not yet mentioned, but unactions also exist. Think of unactions as a reversed action. If an action is to compress some data, its unaction will be decompress the data. unactions are like actions in all senses, but the behaviour is to revert an action (in general used in restores).","title":"Actions"},{"location":"actions/#builtin-actions","text":"There is a list of builtin actions that can be used to create the tasks pipelines or to create new actions by using these as base actions. The actions are grouped by category. See the list here: Archive Command Compress Database Directory Encrypt File Network","title":"Builtin actions"},{"location":"actions/#implementing-actions","text":"In python, an action (and unaction ) is just a function that receives two arguments and returns something. An initial action will receive None and the parameters as arguments and should return a data stream or a DirEntry iterator. A transformer action will receive a data stream or a DirEntry iterator (depending on the expected input) and the parameters as arguments and will return a data stream or a DirEntry iterator. A final action will receive a data stream or a DirEntry iterator and must return the relative path of the file or folder that the action has created. An action can use another action internally to make a composition of actions. DirEntry Is a data structure used internally to represent a entry of a folder. The object is defined like this: class DirEntry : type : str path : Path stats : os . stat_result stream : Optional [ io . IOBase ] link_content : Optional [ str ] real_path : Optional [ Path ] def __init__ ( self , _type : str , path : Path , stats , stream = None , link_content = None , real_path = None , ** kwargs ): pass In actions, a data stream means a io . FileIO , io . BufferedIOBase or io . TextIOBase object that has a file descriptor associated to the stream, or a subprocess . Popen object with PIPE mode set to stdout and stderr . Currently, the file descriptor -thing is important because they are used to be used in external processes efficently (without using internal pipes). mdbackup checks if a data stream is invalid and raises an error for it. On the other hand, the DirEntry iterator is implemented using a generator function that returns DirEntry objects for each entry found in the folder. The type of a entry can be dir , symlink or file . A file will have the stream attribute filled pointing to the contents of the file. A symlink will have the link_content attribute set with the contents of the symlink. In all types, the path must be filled with a relative path to the entry, as well as stats , requiring st_mode , st_uid , st_gid , st_mtime , st_size properties to be filled. Example of initial action def action_read_file ( _ , params : dict ) -> io . FileIO : return open ( params [ 'path' ], 'rb' , buffering = 0 ) Example of transform action def action_compress_gzip ( inp : InputDataStream , params ) -> subprocess . Popen : compression_level = params . get ( 'compressionLevel' ) args = [ 'gzip' , '-c' ] if compression_level is not None : args . append ( f '-{compression_level}' ) # Composition of actions: using command action to create the compress-gz action return action_command ( inp , { 'args' : args }) def action_example_of_dir_entry ( inp , params ): for entry in inp : if entry . type == 'file' : entry . path = Path ( str ( entry . path ) + '.bak' ) yield entry Example of final action def action_write_file ( inp : InputDataStream , params ): # The _backup_path parameter is internal, and can be used freely full_path = Path ( params [ '_backup_path' ]) / Path ( params [ 'path' ]) file_object = open ( full_path , 'wb' , buffering = 0 ) chunk_size = params . get ( 'chunkSize' , 1024 * 8 ) data = inp . read ( chunk_size ) while data is not None and len ( data ) != 0 : file_object . write ( data ) data = inp . read ( chunk_size ) file_object . close () return full_path","title":"Implementing actions"},{"location":"actions/#registring-actions","text":"Using the register_action function, an action (and its unaction counterpart) will be registered in the system and could be used in the tasks. There is also a quick way to register actions and unactions which is using the action and unaction decorators. For user-defined actions, those functions will be received in the function called when registering from a module.","title":"Registring actions"},{"location":"actions/#register_action","text":"The function registers an action into the actions container in order to be used when running tasks. Argument Type Description Optional Default identifier str Defines the identifier to be used to refer the action in the yaml files No action callable The function that implements the action No unaction callable The function that implements the unaction Yes None expected_input Optional [ str ] The expected input for the action Yes None output Optional [ str ] The output for the action Yes None The supported values for expected_input are: stream or directory . The supported values for output are: stream stream : file , stream : process , stream : pipe (only if using os . pipe () fds) or directory .","title":"register_action"},{"location":"actions/#action","text":"Decorator for actions that will register them automatically (once the module that contains the implementation is loaded). Is a shortcut for register_action and allows in-place registration of actions. Argument Type Description Optional Default identifier str Defines the identifier to be used to refer the action in the yaml files No input Optional [ str ] The expected input for the action Yes None output Optional [ str ] The output for the action Yes None unaction callable The function that implements the unaction (use it only if @unaction () won\u2019t be used) Yes None Parameters input is the same as expected_input from the register_action . All parameters expect the same as in register_action .","title":"action"},{"location":"actions/#unaction","text":"Decorator for unactions that will register them automatically (once the module that contains the implementation is loaded). Is a shortcut for register_action and allows in-place registration of unactions once its action counterpart is registered. Argument Type Description Optional Default identifier str Defines the identifier to be used to refer the action in the yaml files No Warning An unaction can be registered after the action has been registered. If the order is inverted, the unaction register will fail.","title":"unaction"},{"location":"actions/#creating-user-defined-actions","text":"As mentioned in the configuration , the user-defined actions must be modules that can be loaded from python using the import syntax. The value in the configuration was module #function where module is the module to import and function is the name of the callable object that will register all actions. The function (or callable object) will receive some keyword arguments (aka kwargs ) with the register_action function and action and unaction decorators to be used quickly without the need to import them. You can also import them directly by importing the module mdbackup . actions . container . The function will also receive get_action and get_unaction that retrieves the implementation for the action/ unaction for the given identifier (the first parameter of both functions). And lastly, the kwarg dir_entry will be passed with the class DirEntry if needed. The function must register all actions by either using the function or the decorator. If anything fails, your function should raise an exception to notify the failure, and put some debug logs will also help. def register_my_actions ( action , unaction , get_action , ** kwargs ): action_ssh = get_action ( 'ssh' ) @action ( 'my-read-file' , output = 'stream:file' ) def action_read_file ( _ , params : dict ): return open ( params [ 'path' ], 'rb' , buffering = 0 ) @action ( 'pi-copy' , output = 'stream:process' ) def action_pi_copy ( _ , params : dict ): return action_ssh ( _ , { 'host' : 'pi' , 'user' : 'pi' , 'password' : 'raspberrypi' , 'knownHostsPolicy' : 'ignore' , # You should never do this 'args' : [ 'bash' , '-c' , f 'if [[ -f \"{params[\"path\"]}\" ]]; then cat \"{params[\"path\"]}\"; else cd \"{params[\"path\"]}\"; tar -c .; fi' ] })","title":"Creating user-defined actions"},{"location":"actions/archive/","text":"Actions: Archive \u00b6 tar \u00b6 Input : directory Output : stream Unaction : yes Parameters ignored (you can pass whatever you want \ud83d\ude43) Description Archives a directory into a tar file. Only files, symbolic links and directories are stored, the rest are ignored. Example Compresses a folder using tar as archive. - name : tar task example actions : - from-directory : path : '/some/where' - tar : - compress-gz : {} - to-file : path : 'compressed-folder.tar.gz'","title":"Archive"},{"location":"actions/archive/#actions-archive","text":"","title":"Actions: Archive"},{"location":"actions/archive/#tar","text":"Input : directory Output : stream Unaction : yes Parameters ignored (you can pass whatever you want \ud83d\ude43) Description Archives a directory into a tar file. Only files, symbolic links and directories are stored, the rest are ignored. Example Compresses a folder using tar as archive. - name : tar task example actions : - from-directory : path : '/some/where' - tar : - compress-gz : {} - to-file : path : 'compressed-folder.tar.gz'","title":"tar"},{"location":"actions/command/","text":"Actions: Command \u00b6 command \u00b6 Input : Nothing or stream Output : stream Unaction : Yes, but to make it work, you need to defined the reverse command. To do so, just define a new section called reverse with args or command (see example below). If trying to do a restore with no reverse . args nor reverse . command will result in a failure. Parameters Can be a str or an object. The string will be interpreted as a sh-like command, the object with the folliwing structure: Name Type Description Optional args List [ str ] List of arguments (including the program) to run Yes command str sh-like command string to run Yes env Dict [ str , str ] Additional environment variables to set when running the program Yes Description Executes the command that produces an output, and may receive an input. The command can be defined by either using args or command parameter. If needed, the command can run with extra environment variables defined in the env parameter. The current working directory will be the backup path. The parameter can also be a string, in this case will be interpreted as command parameter. Example of command as initial action Does a backup of a partition and then compresses it. - name : command task example actions : - command : 'cat /dev/mmcblk0p2' - compress-xz : {} - to-file : path : 'full-backup.gz' Example of command as transform action Does a backup of a file transforming all instances of A to a (using sed ). - name : command task example 2 actions : - from-file : '/path/to/a/file/to/backup.txt' - command : 'sed -i \"s/A/a/g\"' - to-file : path : 'transformed-backup.txt' Example of command with reverse Does a backup of some program that exports its data using a CLI tool, but also defining how to restore the data. - name : command task example 3 actions : - command : command : 'tool-cli export -' reverse : command : 'tool-cli import -' - compress-zst : {} - to-file : path : 'tool-data.zst' ssh \u00b6 Input : Nothing or stream Output : stream Unaction : See command unaction section. Parameters Name Type Description Optional args List [ str ] List of arguments (including the program) to run Yes command str sh-like command string to run Yes host str Hostname to connect to via ssh No user str Username for the ssh session Yes password str Password, if required Yes port int Port of the ssh server Yes knownHostsPolicy str If set to ignore , will ignore any unknown hosts error Yes forwardAgent bool If set to true , will forward the local ssh agent Yes identityFile str If defined, this identity file will be used Yes configFile str Defines a custom config file to be used Yes Use of password in ssh It is not recommended to use the password authentication method with ssh in scripts like this. If you need the password method, ensure the host (where mdbackup runs) has installed sshpass . A properly authentication using ssh is made by configuring an ssh - agent before running mdbackup with the keys preloaded so the tool can run without issues. Description Executes the command through ssh, in another host, which must produce an output and it may receive some input. The command can be defined by either using args or command parameter. The current working directory will be the backup path. Example Does a backup of a file from another server. - name : ssh task example actions : - ssh : host : pi user : pi identityFile : /path/to/the/pi/identity/file args : [ 'cat' , '/etc/motd' ] - to-file : path : 'pi-motd' docker \u00b6 Input : Nothing or stream Output : stream Unaction : See command unaction section. Parameters Name Type Description Optional args List [ str ] List of arguments (including the program) to run Yes command str sh-like command string to run Yes image str The image to use to run the container No env Union [ Dict[str, str ] , List [ str ] ] Additional environment variables to set when running the program Yes volumes List [ str ] List of volumes defined as host - path : container - path or volume : container - path Yes user Union [ str , int ] UID or user name from which the container will run Yes group Union [ str , int ] GID or group name from which the container will run Yes network str Network to attach to (default is host ) Yes workdir str Changes the WorkDir of the container Yes pull bool Pulls the image before running the container (it can also be used to update the image) Yes Description Executes the command inside a Docker container, in the same host, which must produce an output and it may receive some input. The command can be defined by either using args or command parameter. If needed, the command can run with extra environment variables defined in the env parameter. The volumes list uses the same syntax as in docker container run -v argument . If pull is set to true, then the image will be pulled always, and will stop all the pipeline until the image has been pulled. Example Does a backup of a file from a volume. - name : ssh task example actions : - docker : args : [ 'cat' , '/data/some/file' ] image : alpine volumes : - my-volume:/data - to-file : path : 'some-data-from-a-volume.bin'","title":"Command"},{"location":"actions/command/#actions-command","text":"","title":"Actions: Command"},{"location":"actions/command/#command","text":"Input : Nothing or stream Output : stream Unaction : Yes, but to make it work, you need to defined the reverse command. To do so, just define a new section called reverse with args or command (see example below). If trying to do a restore with no reverse . args nor reverse . command will result in a failure. Parameters Can be a str or an object. The string will be interpreted as a sh-like command, the object with the folliwing structure: Name Type Description Optional args List [ str ] List of arguments (including the program) to run Yes command str sh-like command string to run Yes env Dict [ str , str ] Additional environment variables to set when running the program Yes Description Executes the command that produces an output, and may receive an input. The command can be defined by either using args or command parameter. If needed, the command can run with extra environment variables defined in the env parameter. The current working directory will be the backup path. The parameter can also be a string, in this case will be interpreted as command parameter. Example of command as initial action Does a backup of a partition and then compresses it. - name : command task example actions : - command : 'cat /dev/mmcblk0p2' - compress-xz : {} - to-file : path : 'full-backup.gz' Example of command as transform action Does a backup of a file transforming all instances of A to a (using sed ). - name : command task example 2 actions : - from-file : '/path/to/a/file/to/backup.txt' - command : 'sed -i \"s/A/a/g\"' - to-file : path : 'transformed-backup.txt' Example of command with reverse Does a backup of some program that exports its data using a CLI tool, but also defining how to restore the data. - name : command task example 3 actions : - command : command : 'tool-cli export -' reverse : command : 'tool-cli import -' - compress-zst : {} - to-file : path : 'tool-data.zst'","title":"command"},{"location":"actions/command/#ssh","text":"Input : Nothing or stream Output : stream Unaction : See command unaction section. Parameters Name Type Description Optional args List [ str ] List of arguments (including the program) to run Yes command str sh-like command string to run Yes host str Hostname to connect to via ssh No user str Username for the ssh session Yes password str Password, if required Yes port int Port of the ssh server Yes knownHostsPolicy str If set to ignore , will ignore any unknown hosts error Yes forwardAgent bool If set to true , will forward the local ssh agent Yes identityFile str If defined, this identity file will be used Yes configFile str Defines a custom config file to be used Yes Use of password in ssh It is not recommended to use the password authentication method with ssh in scripts like this. If you need the password method, ensure the host (where mdbackup runs) has installed sshpass . A properly authentication using ssh is made by configuring an ssh - agent before running mdbackup with the keys preloaded so the tool can run without issues. Description Executes the command through ssh, in another host, which must produce an output and it may receive some input. The command can be defined by either using args or command parameter. The current working directory will be the backup path. Example Does a backup of a file from another server. - name : ssh task example actions : - ssh : host : pi user : pi identityFile : /path/to/the/pi/identity/file args : [ 'cat' , '/etc/motd' ] - to-file : path : 'pi-motd'","title":"ssh"},{"location":"actions/command/#docker","text":"Input : Nothing or stream Output : stream Unaction : See command unaction section. Parameters Name Type Description Optional args List [ str ] List of arguments (including the program) to run Yes command str sh-like command string to run Yes image str The image to use to run the container No env Union [ Dict[str, str ] , List [ str ] ] Additional environment variables to set when running the program Yes volumes List [ str ] List of volumes defined as host - path : container - path or volume : container - path Yes user Union [ str , int ] UID or user name from which the container will run Yes group Union [ str , int ] GID or group name from which the container will run Yes network str Network to attach to (default is host ) Yes workdir str Changes the WorkDir of the container Yes pull bool Pulls the image before running the container (it can also be used to update the image) Yes Description Executes the command inside a Docker container, in the same host, which must produce an output and it may receive some input. The command can be defined by either using args or command parameter. If needed, the command can run with extra environment variables defined in the env parameter. The volumes list uses the same syntax as in docker container run -v argument . If pull is set to true, then the image will be pulled always, and will stop all the pipeline until the image has been pulled. Example Does a backup of a file from a volume. - name : ssh task example actions : - docker : args : [ 'cat' , '/data/some/file' ] image : alpine volumes : - my-volume:/data - to-file : path : 'some-data-from-a-volume.bin'","title":"docker"},{"location":"actions/compress/","text":"Actions: Compress \u00b6 compress - xz \u00b6 Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 0 to 9 (default 6) Yes extraCompression bool Enables the extra compression mode ( - e ) Yes cpus int Uses this amount of cores to compress the data Yes Description Compresses the input stream using xz utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 9). The extra compression mode uses even more resources to try to compress more (it does not affect decompression). This compression algorithm uses huge amount of CPU and the result is usually better than the rest of algorithms listed here, but it may not be worth the CPU usage with the result if the files are text. Warning Requires xz to be installed in the system. In most distros it is included by default. On macOS, you should install it via brew install xz . Example Compresses a file using xz - name : xz task example actions : - from-file : '/big/compressible/file' - compress-xz : compressionLevel : 7 extraCompression : true cpus : 4 - to-file : path : 'compressed-file.xz' compress - gz \u00b6 Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 1 to 9 (default 6) Yes Description Compresses the input stream using gzip utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 9). This compression algorithm is rather fast and good-balanced in resources consumption. It may not be really good compressing binary files. Warning Requires gzip to be installed in the system. In most distros and in macOS it is included by default. Example Compresses a file using gzip - name : gzip task example actions : - from-file : '/big/compressible/file' - compress-gz : compressionLevel : 7 - to-file : path : 'compressed-file.gz' compress - bz2 \u00b6 Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 1 to 9 (default 6) Yes Description Compresses the input stream using bzip2 utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 9). This compression algorithm is rather fast and good-balanced in resources consumption (is similar to gzip ). Warning Requires bzip2 to be installed in the system. In most distros and in macOS it is included by default. Example Compresses a file using bzip2 - name : bz task example actions : - from-file : '/big/compressible/file' - compress-bz2 : compressionLevel : 7 - to-file : path : 'compressed-file.bz2' compress - br \u00b6 Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 1 to 9 (default 6) Yes Description Compresses the input stream using brotli utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 9). This compression algorithm is fast and good-balanced in resources consumption. Can be used as a fast compression algorithm for text and binary files. Warning Requires brotli to be installed in the system. On Debian-based distros use apt install brotli , on Arch-based distros use pacman - S brotli , on macOS use brew install brotli . Example Compresses a file using brotli - name : br task example actions : - from-file : '/big/compressible/file' - compress-br : compressionLevel : 7 - to-file : path : 'compressed-file.br' compress - zst \u00b6 Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 1 to 19 (default 3) Yes Description Compresses the input stream using zstd utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 19). This compression algorithm is blazing fast and good in compression (similar to xz but faster). Can be used as a fast compression algorithm for text and binary files. Warning Requires zstd to be installed in the system. On Debian-based distros use apt install zstd , on Arch-based distros use pacman - S zstd , on macOS use brew install zstd . Example Compresses a file using zstd - name : zst task example actions : - from-file : '/big/compressible/file' - compress-zst : compressionLevel : 7 - to-file : path : 'compressed-file.zst'","title":"Compress"},{"location":"actions/compress/#actions-compress","text":"","title":"Actions: Compress"},{"location":"actions/compress/#compress-xz","text":"Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 0 to 9 (default 6) Yes extraCompression bool Enables the extra compression mode ( - e ) Yes cpus int Uses this amount of cores to compress the data Yes Description Compresses the input stream using xz utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 9). The extra compression mode uses even more resources to try to compress more (it does not affect decompression). This compression algorithm uses huge amount of CPU and the result is usually better than the rest of algorithms listed here, but it may not be worth the CPU usage with the result if the files are text. Warning Requires xz to be installed in the system. In most distros it is included by default. On macOS, you should install it via brew install xz . Example Compresses a file using xz - name : xz task example actions : - from-file : '/big/compressible/file' - compress-xz : compressionLevel : 7 extraCompression : true cpus : 4 - to-file : path : 'compressed-file.xz'","title":"compress-xz"},{"location":"actions/compress/#compress-gz","text":"Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 1 to 9 (default 6) Yes Description Compresses the input stream using gzip utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 9). This compression algorithm is rather fast and good-balanced in resources consumption. It may not be really good compressing binary files. Warning Requires gzip to be installed in the system. In most distros and in macOS it is included by default. Example Compresses a file using gzip - name : gzip task example actions : - from-file : '/big/compressible/file' - compress-gz : compressionLevel : 7 - to-file : path : 'compressed-file.gz'","title":"compress-gz"},{"location":"actions/compress/#compress-bz2","text":"Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 1 to 9 (default 6) Yes Description Compresses the input stream using bzip2 utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 9). This compression algorithm is rather fast and good-balanced in resources consumption (is similar to gzip ). Warning Requires bzip2 to be installed in the system. In most distros and in macOS it is included by default. Example Compresses a file using bzip2 - name : bz task example actions : - from-file : '/big/compressible/file' - compress-bz2 : compressionLevel : 7 - to-file : path : 'compressed-file.bz2'","title":"compress-bz2"},{"location":"actions/compress/#compress-br","text":"Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 1 to 9 (default 6) Yes Description Compresses the input stream using brotli utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 9). This compression algorithm is fast and good-balanced in resources consumption. Can be used as a fast compression algorithm for text and binary files. Warning Requires brotli to be installed in the system. On Debian-based distros use apt install brotli , on Arch-based distros use pacman - S brotli , on macOS use brew install brotli . Example Compresses a file using brotli - name : br task example actions : - from-file : '/big/compressible/file' - compress-br : compressionLevel : 7 - to-file : path : 'compressed-file.br'","title":"compress-br"},{"location":"actions/compress/#compress-zst","text":"Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional compressionLevel \u00ec nt Compression level from 1 to 19 (default 3) Yes Description Compresses the input stream using zstd utility. If the compression level increases, more resources will be used to compress and more compressed the file should result (it may not compress any further if increasing it to 19). This compression algorithm is blazing fast and good in compression (similar to xz but faster). Can be used as a fast compression algorithm for text and binary files. Warning Requires zstd to be installed in the system. On Debian-based distros use apt install zstd , on Arch-based distros use pacman - S zstd , on macOS use brew install zstd . Example Compresses a file using zstd - name : zst task example actions : - from-file : '/big/compressible/file' - compress-zst : compressionLevel : 7 - to-file : path : 'compressed-file.zst'","title":"compress-zst"},{"location":"actions/database/","text":"Actions: Database \u00b6 postgres - database \u00b6 Input : Nothing Output : stream Unaction : To be implemented\u2026 Parameters : Name Type Description Optional database str Database to connect to and make a backup No user str User for the database connection Yes password str Password for the database connection Yes host str Hostname of the psql server Yes port int Port of the server Yes docker bool If set to true , will use a container instead of native tools Yes runAs str If running outside docker, runs the command with sudo as the user provided Yes image str Changes the image to be used in the container Yes When docker is true , accepts any of the parameters of the docker action, except for args and command which is set automatically by this action. Description Uses pg_dump to make a full backup of a PostgreSQL database into a SQL file. By default uses the local unix socket to connect to the server, but it can be changed by setting the host parameter. By default, the tool runs as the current user, but it can be changed by setting the runAs parameter to the desired user (useful for unix socket connections). This action requires to have installed pg_dump on the host, but to avoid this, docker can be used instead. When using docker, the backup will be done in a container based on the postgres : alpine image. Warning If not using docker, pg_dump must be installed on the host. If using docker, docker must be installed on the host and the user running the backups must have access to the docker socket. Example Simple backup of a database. - name : postgres database task example actions : - postgres-database : database : 'app-database' - compress-gz : {} - to-file : path : 'app-database.sql.gz' Example Backup using docker to make a backup. - name : postgres database using docker task example actions : - postgres-database : database : 'app-database' docker : true user : postgres # by default uses this user (just to clarify) password : V3ryP0w3rf()lP4ssw0rd # better if secrets is used for this :) network : dbs - compress-gz : {} - to-file : path : 'app-database.sql.gz' mysql - database \u00b6 Input : Nothing Output : stream Unaction : To be implemented\u2026 Parameters Name Type Description Optional host str Hostname of the mysql/mariadb server No database str Database to connect to and make a backup No user str User for the database connection Yes password str Password for the database connection Yes port int Port of the server Yes docker bool If set to true , will use a container instead of native tools Yes image str Changes the image to be used in the container Yes When docker is true , accepts any of the parameters of the docker action, except for args and command which is set automatically by this action. Description Uses mysqldump to make a full backup of a MySQL/MariaDB database into a SQL file. By default, uses the current user as login user for the database connection. This action requires to have installed mysqldump on the host, but docker can be used to run the tool inside a container. By default uses mariadb : alpine image. Warning If not using docker, mysqldump must be installed on the host. If using docker, docker must be installed on the host and the user running the backups must have access to the docker socket. Example Simple backup of a database. - name : mysql database task example actions : - mysql-database : host : '127.0.0.1' user : 'backups' password : 'B4ck()psV3ryP0w3rf()lP4ssw0rd' # better if secrets is used for this :) database : 'app-database' - compress-gz : {} - to-file : path : 'app-database.sql.gz' Example Backup using docker to make a backup. - name : mysql database using docker task example actions : - mysql-database : docker : true host : '127.0.0.1' user : 'backups' password : 'B4ck()psV3ryP0w3rf()lP4ssw0rd' # better if secrets is used for this :) database : 'app-database' image : mysql:alpine - compress-gz : {} - to-file : path : 'app-database.sql.gz' influxdb \u00b6 Input : Nothing Output : Nothing Unaction : To be implemented\u2026 Parameters Name Type Description Optional host str Host:Port to the influxdb server No to str Folder where to place the backup inside the backup folder No database str Database to backup (if not set, will backup all databases) Yes retention str Retention policy for the backup (uses all by default) Yes shard str If retention is defined, will backup the selected shard (by ID) Yes start str Include all points starting with the specified timestamp ( RFC3339 format ) Yes end str Exclude all points after the specified timestamp ( RFC3339 format ) Yes docker bool If set to true , will use a container instead of native tools Yes image str Changes the image to be used in the container Yes When docker is true , accepts any of the parameters of the docker action, except for args and command which is set automatically by this action. Description Uses influxd to make a backup of one or all databases into a folder located in the current backup folder. The action requires influxd to be installed in the host, or instead docker can be used. Using docker, the influxdb : alpine image will be used by default. See Backup up and restoring in InfluxDB OSS (external) for more details for the parameters. Warning If not using docker, influxd must be installed on the host. If using docker, docker must be installed on the host and the user running the backups must have access to the docker socket. Info The backups uses the new backup format, which is compatible with the enterprise version of influxdb ( - portable ). Example Simple backup of a database. - name : influxdb database task example actions : - influxdb : to : 'influxdb/netdata' host : '127.0.0.1:8088' database : 'graphite'","title":"Database"},{"location":"actions/database/#actions-database","text":"","title":"Actions: Database"},{"location":"actions/database/#postgres-database","text":"Input : Nothing Output : stream Unaction : To be implemented\u2026 Parameters : Name Type Description Optional database str Database to connect to and make a backup No user str User for the database connection Yes password str Password for the database connection Yes host str Hostname of the psql server Yes port int Port of the server Yes docker bool If set to true , will use a container instead of native tools Yes runAs str If running outside docker, runs the command with sudo as the user provided Yes image str Changes the image to be used in the container Yes When docker is true , accepts any of the parameters of the docker action, except for args and command which is set automatically by this action. Description Uses pg_dump to make a full backup of a PostgreSQL database into a SQL file. By default uses the local unix socket to connect to the server, but it can be changed by setting the host parameter. By default, the tool runs as the current user, but it can be changed by setting the runAs parameter to the desired user (useful for unix socket connections). This action requires to have installed pg_dump on the host, but to avoid this, docker can be used instead. When using docker, the backup will be done in a container based on the postgres : alpine image. Warning If not using docker, pg_dump must be installed on the host. If using docker, docker must be installed on the host and the user running the backups must have access to the docker socket. Example Simple backup of a database. - name : postgres database task example actions : - postgres-database : database : 'app-database' - compress-gz : {} - to-file : path : 'app-database.sql.gz' Example Backup using docker to make a backup. - name : postgres database using docker task example actions : - postgres-database : database : 'app-database' docker : true user : postgres # by default uses this user (just to clarify) password : V3ryP0w3rf()lP4ssw0rd # better if secrets is used for this :) network : dbs - compress-gz : {} - to-file : path : 'app-database.sql.gz'","title":"postgres-database"},{"location":"actions/database/#mysql-database","text":"Input : Nothing Output : stream Unaction : To be implemented\u2026 Parameters Name Type Description Optional host str Hostname of the mysql/mariadb server No database str Database to connect to and make a backup No user str User for the database connection Yes password str Password for the database connection Yes port int Port of the server Yes docker bool If set to true , will use a container instead of native tools Yes image str Changes the image to be used in the container Yes When docker is true , accepts any of the parameters of the docker action, except for args and command which is set automatically by this action. Description Uses mysqldump to make a full backup of a MySQL/MariaDB database into a SQL file. By default, uses the current user as login user for the database connection. This action requires to have installed mysqldump on the host, but docker can be used to run the tool inside a container. By default uses mariadb : alpine image. Warning If not using docker, mysqldump must be installed on the host. If using docker, docker must be installed on the host and the user running the backups must have access to the docker socket. Example Simple backup of a database. - name : mysql database task example actions : - mysql-database : host : '127.0.0.1' user : 'backups' password : 'B4ck()psV3ryP0w3rf()lP4ssw0rd' # better if secrets is used for this :) database : 'app-database' - compress-gz : {} - to-file : path : 'app-database.sql.gz' Example Backup using docker to make a backup. - name : mysql database using docker task example actions : - mysql-database : docker : true host : '127.0.0.1' user : 'backups' password : 'B4ck()psV3ryP0w3rf()lP4ssw0rd' # better if secrets is used for this :) database : 'app-database' image : mysql:alpine - compress-gz : {} - to-file : path : 'app-database.sql.gz'","title":"mysql-database"},{"location":"actions/database/#influxdb","text":"Input : Nothing Output : Nothing Unaction : To be implemented\u2026 Parameters Name Type Description Optional host str Host:Port to the influxdb server No to str Folder where to place the backup inside the backup folder No database str Database to backup (if not set, will backup all databases) Yes retention str Retention policy for the backup (uses all by default) Yes shard str If retention is defined, will backup the selected shard (by ID) Yes start str Include all points starting with the specified timestamp ( RFC3339 format ) Yes end str Exclude all points after the specified timestamp ( RFC3339 format ) Yes docker bool If set to true , will use a container instead of native tools Yes image str Changes the image to be used in the container Yes When docker is true , accepts any of the parameters of the docker action, except for args and command which is set automatically by this action. Description Uses influxd to make a backup of one or all databases into a folder located in the current backup folder. The action requires influxd to be installed in the host, or instead docker can be used. Using docker, the influxdb : alpine image will be used by default. See Backup up and restoring in InfluxDB OSS (external) for more details for the parameters. Warning If not using docker, influxd must be installed on the host. If using docker, docker must be installed on the host and the user running the backups must have access to the docker socket. Info The backups uses the new backup format, which is compatible with the enterprise version of influxdb ( - portable ). Example Simple backup of a database. - name : influxdb database task example actions : - influxdb : to : 'influxdb/netdata' host : '127.0.0.1:8088' database : 'graphite'","title":"influxdb"},{"location":"actions/directory/","text":"Actions: Directory \u00b6 from - directory \u00b6 Input : Nothing Output : directory Unaction : Yes (Only works when using dictionary parameter - if using a string as parameter the restore will fail). Parameters Name Type Description Optional path str Path to the directory to read No followSymlinks bool Follow symbolic links Yes Also a string is accepted as parameter, in this case will be converted to the path parameter and followSymlinks will be false . Description Reads the contents of the folder to be used in another action. By default, the symlinks are left intact, but if followSymlinks is set to true then they will be followed and the resolved entry will be read instead. Example Read the folder contents, then archive and compress it into a file. - name : from-directory task example actions : - from-directory : /var/lib/docker - tar : yes - compress-gz : {} - to-file : path : 'dockerino.tar.gz' from - physical - docker - volume \u00b6 Input : Nothing Output : directory Unaction : Yes (Only works when using dictionary parameter - if using a string as parameter the restore will fail). Parameters Name Type Description Optional volume str Name of the volume to backup No followSymlinks bool Follow symbolic links Yes Also a string is accepted as parameter, in this case will be converted to the volume parameter and followSymlinks will be false . Description Determines which is the path to the folder of the desired volume (using docker volume inspect VOLUME_NAME --format {{ .Mountpoint }} command), and then calls the from - directory action to perform the read. The path is set automatically to the resolved path from the command. Warning Requires docker to run this action. The user running the backups must have access to the docker socket. Only local volume type is supported. Example Read the volume folder contents, then archive and compress it into a file. - name : from-physical-docker-directory task example actions : - from-physical-docker-directory : docker-volume-name - tar : yes - compress-gz : {} - to-file : path : 'dockerino-volumino.tar.gz' to - directory \u00b6 Input : directory Output : Nothing Unaction : Yes Parameters Name Type Description Optional path str Folder to place the folder contents into this path inside the backup folder No parents bool Create parent folders if they do not exist (when creating the output folder) Yes preserveStats Union [ bool , str ] Preserve some or all of the stats of the entry (see description) Yes Also accepts the same parameters of copy - file action, but with the from and to filled by this action. These settings only apply when a file is being written. Description Writes the full contents of the folder into the folder defined in path (which will be inside the backup folder). The stats of the entries can be preserved in several ways by defining the preserveStats property. By default is set to utime (see table below for all options). To combine multiple options, use , to split them ( chmod , chown , utime ). Writing a file uses the copy - file action, which by default will try to reduce copies if the same file exists in the previous backup and no modification is detected (that is why preserveStats is utime by default). Can be disabled by setting forceCopy to true . Preserve Stats value Meaning Requires root? true Preserves all stats Probably yes false Does not preserve any stats No chmod Only changes the permissions of the entry Probably yes chown Changes the UID and GID of the entry Yes utime Changes the access and modified times No, if can modify the files xattrs Preserves the extended attributes of the entries Yes Example Copy a folder (the explicit way). - name : to-directory task example actions : - from-directory : /folder/to/copy - to-directory : path : x-folder preserveStats : chmod,chown,utime reflink : true copy - directory \u00b6 Input : Nothing Output : Nothing Unaction : Yes Parameters Name Type Description Optional from str path parameter for from - directory action No to str path parameter for to - directory action No All supported parameters from from - directory and to - directory actions are supported. The path for both actions are filled from this action parameters (see above table). Description Shortcut for the example of to - directory . Copies a directory from somewhere to the backup folder using the same functionallity. Example Copy a folder (the good way). - name : copy-directory task example actions : - copy-directory : from : /folder/to/copy to : x-folder preserveStats : chmod,chown,utime reflink : true","title":"Directory"},{"location":"actions/directory/#actions-directory","text":"","title":"Actions: Directory"},{"location":"actions/directory/#from-directory","text":"Input : Nothing Output : directory Unaction : Yes (Only works when using dictionary parameter - if using a string as parameter the restore will fail). Parameters Name Type Description Optional path str Path to the directory to read No followSymlinks bool Follow symbolic links Yes Also a string is accepted as parameter, in this case will be converted to the path parameter and followSymlinks will be false . Description Reads the contents of the folder to be used in another action. By default, the symlinks are left intact, but if followSymlinks is set to true then they will be followed and the resolved entry will be read instead. Example Read the folder contents, then archive and compress it into a file. - name : from-directory task example actions : - from-directory : /var/lib/docker - tar : yes - compress-gz : {} - to-file : path : 'dockerino.tar.gz'","title":"from-directory"},{"location":"actions/directory/#from-physical-docker-volume","text":"Input : Nothing Output : directory Unaction : Yes (Only works when using dictionary parameter - if using a string as parameter the restore will fail). Parameters Name Type Description Optional volume str Name of the volume to backup No followSymlinks bool Follow symbolic links Yes Also a string is accepted as parameter, in this case will be converted to the volume parameter and followSymlinks will be false . Description Determines which is the path to the folder of the desired volume (using docker volume inspect VOLUME_NAME --format {{ .Mountpoint }} command), and then calls the from - directory action to perform the read. The path is set automatically to the resolved path from the command. Warning Requires docker to run this action. The user running the backups must have access to the docker socket. Only local volume type is supported. Example Read the volume folder contents, then archive and compress it into a file. - name : from-physical-docker-directory task example actions : - from-physical-docker-directory : docker-volume-name - tar : yes - compress-gz : {} - to-file : path : 'dockerino-volumino.tar.gz'","title":"from-physical-docker-volume"},{"location":"actions/directory/#to-directory","text":"Input : directory Output : Nothing Unaction : Yes Parameters Name Type Description Optional path str Folder to place the folder contents into this path inside the backup folder No parents bool Create parent folders if they do not exist (when creating the output folder) Yes preserveStats Union [ bool , str ] Preserve some or all of the stats of the entry (see description) Yes Also accepts the same parameters of copy - file action, but with the from and to filled by this action. These settings only apply when a file is being written. Description Writes the full contents of the folder into the folder defined in path (which will be inside the backup folder). The stats of the entries can be preserved in several ways by defining the preserveStats property. By default is set to utime (see table below for all options). To combine multiple options, use , to split them ( chmod , chown , utime ). Writing a file uses the copy - file action, which by default will try to reduce copies if the same file exists in the previous backup and no modification is detected (that is why preserveStats is utime by default). Can be disabled by setting forceCopy to true . Preserve Stats value Meaning Requires root? true Preserves all stats Probably yes false Does not preserve any stats No chmod Only changes the permissions of the entry Probably yes chown Changes the UID and GID of the entry Yes utime Changes the access and modified times No, if can modify the files xattrs Preserves the extended attributes of the entries Yes Example Copy a folder (the explicit way). - name : to-directory task example actions : - from-directory : /folder/to/copy - to-directory : path : x-folder preserveStats : chmod,chown,utime reflink : true","title":"to-directory"},{"location":"actions/directory/#copy-directory","text":"Input : Nothing Output : Nothing Unaction : Yes Parameters Name Type Description Optional from str path parameter for from - directory action No to str path parameter for to - directory action No All supported parameters from from - directory and to - directory actions are supported. The path for both actions are filled from this action parameters (see above table). Description Shortcut for the example of to - directory . Copies a directory from somewhere to the backup folder using the same functionallity. Example Copy a folder (the good way). - name : copy-directory task example actions : - copy-directory : from : /folder/to/copy to : x-folder preserveStats : chmod,chown,utime reflink : true","title":"copy-directory"},{"location":"actions/encrypt/","text":"Actions: Encrypt \u00b6 encrypt - gpg \u00b6 Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional passphrase str Defines a passphrase that will be used to decrypt the data Yes recipients List [ str ] List of emails that will be able to decrypt the data Yes cipherAlgorithm str Changes the cypher algorithm Yes compressAlgorithm str Changes the compress algorithm Yes At least one recipient must be defined. If no recipients are defined, a passphrase must be provided. Both can be defined. Description Encrypts the data using gpg (gpg 2) command. If recipients are defined, then the users that own that keys will be able to decrypt the data. If passphrase is defined, then this passphrase will be used to encrypt the data. Both can be defined so if a user owns a key and is able to decrypt the data, the passphrase will be asked as well. The cipher and compress valid algorithms can be queried by issuing the command gpg --version . By default the compress algorithm is uncompressed . Warning The recipient keys must be imported previously. The action will not import any key. Example Cypher a database backup - name : encrypt gpg task example actions : - postgres-database : database : 'example' - encrypt-gpg : passphrase : mdbackup recipients : [ user1@email.com , user2@email.com ] - to-file : path : 'example.sql.asc'","title":"Encrypt"},{"location":"actions/encrypt/#actions-encrypt","text":"","title":"Actions: Encrypt"},{"location":"actions/encrypt/#encrypt-gpg","text":"Input : stream Output : stream Unaction : yes Parameters Name Type Description Optional passphrase str Defines a passphrase that will be used to decrypt the data Yes recipients List [ str ] List of emails that will be able to decrypt the data Yes cipherAlgorithm str Changes the cypher algorithm Yes compressAlgorithm str Changes the compress algorithm Yes At least one recipient must be defined. If no recipients are defined, a passphrase must be provided. Both can be defined. Description Encrypts the data using gpg (gpg 2) command. If recipients are defined, then the users that own that keys will be able to decrypt the data. If passphrase is defined, then this passphrase will be used to encrypt the data. Both can be defined so if a user owns a key and is able to decrypt the data, the passphrase will be asked as well. The cipher and compress valid algorithms can be queried by issuing the command gpg --version . By default the compress algorithm is uncompressed . Warning The recipient keys must be imported previously. The action will not import any key. Example Cypher a database backup - name : encrypt gpg task example actions : - postgres-database : database : 'example' - encrypt-gpg : passphrase : mdbackup recipients : [ user1@email.com , user2@email.com ] - to-file : path : 'example.sql.asc'","title":"encrypt-gpg"},{"location":"actions/file/","text":"Actions: File \u00b6 from - file \u00b6 Input : Nothing Output : stream Unaction : Yes Parameters Expects a path as string or an object like { path : '/path/to/file' } . Description Initial action that opens a file to be read from the next actions. The user running the backups must be able to read the file. Example Simple copy (the bad way). - name : from file task example actions : - from-file : '/path/to/a/file.txt' - to-file : path : 'file.txt' from - file - ssh \u00b6 Input : Nothing Output : stream Unaction : Yes Parameters Name Type Description Optional host str Hostname of the host which has an ssh No path str Path to the remote file to copy No user str User to be used in the connection Yes password str Password for the authentication Yes port int Port of the ssh server Yes identityFile str Path to an identity file which will be used for authentication Yes configFile str Path to a ssh client configuration file Yes knownHostsPolicy str If set to true , will ignore unknown hosts errors Yes Use of password in ssh It is not recommended to use the password authentication method with ssh in scripts like this. If you need the password method, ensure the host (where mdbackup runs) has installed sshpass . A properly authentication using ssh is made by configuring an ssh - agent before running mdbackup with the keys preloaded so the tool can run without issues. Description Reads a file from the remote server using scp , and the output can be used in other actions as if it were a regular file in the same machine. Example Simple copy using scp . - name : from file ssh task example actions : - from-file-ssh : path : '/path/to/a/file/in/the/remote/server/file.txt' host : 'my-server' identityFile : '/root/.ssh/id_rsa' - to-file : path : 'file.txt' to - file \u00b6 Input : stream Output : Nothing Unaction : Yes Parameters Name Type Description Optional path str Path where the file will be written, inside the backup folder No mkdirParents bool If the path contains some folders and this parameter is set to true , then will create the folders Yes chunkSize int Changes the chunk size to be used internally while reading the data to write (default 8KiB = 8192) Yes Description Writes the data stream into a file, reading chunks of chunkSize bytes until end of the stream. Example Simple copy (the bad way), but a bit different. - name : from file task example actions : - from-file : '/path/to/a/file.txt' - to-file : path : 'file.txt' chunkSize : 65536 copy - file \u00b6 Input : Nothing Output : Nothing Unaction : Yes Parameters Name Type Description Optional from str Path to the file to be copied No to str Path to where the file will be copied inside the backup folder No mkdirParents bool If the path contains some folders and this parameter is set to true , then will create the folders Yes preserveStats Union [ bool , str ] Preserve some or all of the stats of the entry (see this ) Yes forceCopy bool If set to true , then it will always copy the file and will not try to clone it from a previous backup Yes reflink bool If set to true it will try to make copy of the original file using Copy on Write (if the file system supports this - see clone - file ) Yes chunkSize int The same as in to - file Yes Description Copies a file to the backup folder. It is an optimized version in which the previous backup file is checked in order to clone it (which is faster). In order to make this work, the file must have the utime at least in the preserveStats parameter (the default). The action will check the modified time of the previous backup version and if they both match, then it will use clone - file action. If the modification time is different or forceCopy is true , then it will make a normal copy using to - file action. If the clone - file action fails, it will try to make a normal copy using to - file action. Example Optimized copy (the right way). - name : copy file task example actions : - copy-file : from : '/path/to/a/file.txt' to : 'file.txt' preserveStats : True reflink : True # in btrfs, this will make a really fast copy chunkSize : 65536 clone - file \u00b6 Input : Nothing Output : Nothing Unaction : Yes Parameters Name Type Description Optional from str Path to the file to be copied No to str Path to where the file will be copied inside the backup folder No mkdirParents bool If the path contains some folders and this parameter is set to true , then will create the folders Yes preserveStats Union [ bool , str ] Preserve some or all of the stats of the entry (see this ) (only for reflink ) Yes reflink bool If set to true it will try to make copy of the original file using Copy on Write (if the file system supports this) Yes Description Does a \u201cclone\u201d of a file. By default, does a hard-link between both files. But if reflink is set to true , then it will try to make a light copy using Copy on Write (if the file system supports that - only available on Linux). If the CoW fails, then a hard-link will be done as fallback. About hard-links and CoW For both to work, from and to paths must be under the same partition. Cross partition hard-links are impossible, as well as CoW copies. Example Clone a file. - name : clone file task example actions : - clone-file : from : '/path/to/a/file.txt' to : 'file.txt' preserveStats : True reflink : True # in btrfs, this will make a really fast copy","title":"File"},{"location":"actions/file/#actions-file","text":"","title":"Actions: File"},{"location":"actions/file/#from-file","text":"Input : Nothing Output : stream Unaction : Yes Parameters Expects a path as string or an object like { path : '/path/to/file' } . Description Initial action that opens a file to be read from the next actions. The user running the backups must be able to read the file. Example Simple copy (the bad way). - name : from file task example actions : - from-file : '/path/to/a/file.txt' - to-file : path : 'file.txt'","title":"from-file"},{"location":"actions/file/#from-file-ssh","text":"Input : Nothing Output : stream Unaction : Yes Parameters Name Type Description Optional host str Hostname of the host which has an ssh No path str Path to the remote file to copy No user str User to be used in the connection Yes password str Password for the authentication Yes port int Port of the ssh server Yes identityFile str Path to an identity file which will be used for authentication Yes configFile str Path to a ssh client configuration file Yes knownHostsPolicy str If set to true , will ignore unknown hosts errors Yes Use of password in ssh It is not recommended to use the password authentication method with ssh in scripts like this. If you need the password method, ensure the host (where mdbackup runs) has installed sshpass . A properly authentication using ssh is made by configuring an ssh - agent before running mdbackup with the keys preloaded so the tool can run without issues. Description Reads a file from the remote server using scp , and the output can be used in other actions as if it were a regular file in the same machine. Example Simple copy using scp . - name : from file ssh task example actions : - from-file-ssh : path : '/path/to/a/file/in/the/remote/server/file.txt' host : 'my-server' identityFile : '/root/.ssh/id_rsa' - to-file : path : 'file.txt'","title":"from-file-ssh"},{"location":"actions/file/#to-file","text":"Input : stream Output : Nothing Unaction : Yes Parameters Name Type Description Optional path str Path where the file will be written, inside the backup folder No mkdirParents bool If the path contains some folders and this parameter is set to true , then will create the folders Yes chunkSize int Changes the chunk size to be used internally while reading the data to write (default 8KiB = 8192) Yes Description Writes the data stream into a file, reading chunks of chunkSize bytes until end of the stream. Example Simple copy (the bad way), but a bit different. - name : from file task example actions : - from-file : '/path/to/a/file.txt' - to-file : path : 'file.txt' chunkSize : 65536","title":"to-file"},{"location":"actions/file/#copy-file","text":"Input : Nothing Output : Nothing Unaction : Yes Parameters Name Type Description Optional from str Path to the file to be copied No to str Path to where the file will be copied inside the backup folder No mkdirParents bool If the path contains some folders and this parameter is set to true , then will create the folders Yes preserveStats Union [ bool , str ] Preserve some or all of the stats of the entry (see this ) Yes forceCopy bool If set to true , then it will always copy the file and will not try to clone it from a previous backup Yes reflink bool If set to true it will try to make copy of the original file using Copy on Write (if the file system supports this - see clone - file ) Yes chunkSize int The same as in to - file Yes Description Copies a file to the backup folder. It is an optimized version in which the previous backup file is checked in order to clone it (which is faster). In order to make this work, the file must have the utime at least in the preserveStats parameter (the default). The action will check the modified time of the previous backup version and if they both match, then it will use clone - file action. If the modification time is different or forceCopy is true , then it will make a normal copy using to - file action. If the clone - file action fails, it will try to make a normal copy using to - file action. Example Optimized copy (the right way). - name : copy file task example actions : - copy-file : from : '/path/to/a/file.txt' to : 'file.txt' preserveStats : True reflink : True # in btrfs, this will make a really fast copy chunkSize : 65536","title":"copy-file"},{"location":"actions/file/#clone-file","text":"Input : Nothing Output : Nothing Unaction : Yes Parameters Name Type Description Optional from str Path to the file to be copied No to str Path to where the file will be copied inside the backup folder No mkdirParents bool If the path contains some folders and this parameter is set to true , then will create the folders Yes preserveStats Union [ bool , str ] Preserve some or all of the stats of the entry (see this ) (only for reflink ) Yes reflink bool If set to true it will try to make copy of the original file using Copy on Write (if the file system supports this) Yes Description Does a \u201cclone\u201d of a file. By default, does a hard-link between both files. But if reflink is set to true , then it will try to make a light copy using Copy on Write (if the file system supports that - only available on Linux). If the CoW fails, then a hard-link will be done as fallback. About hard-links and CoW For both to work, from and to paths must be under the same partition. Cross partition hard-links are impossible, as well as CoW copies. Example Clone a file. - name : clone file task example actions : - clone-file : from : '/path/to/a/file.txt' to : 'file.txt' preserveStats : True reflink : True # in btrfs, this will make a really fast copy","title":"clone-file"},{"location":"actions/network/","text":"Actions: Network \u00b6 Network refers here to network devices like switches, routers, NAS\u2026 asuswrt \u00b6 Input : Nothing Output : stream Unaction : No Parameters Name Type Description Optional host str IP (or hostname) of the Asus router No user str Router\u2019s user No password str Router\u2019s password No backupType str Type of the backup: configuration or jffs No Description Does a backup from an AsusWRT router through its web admin panel. The file is downloaded and piped to the next action. Warning When doing this action, the router cannot have an active session open. The login from the tool can fail if an active session is still open elsewhere. requests package required In order to use this action, the requests package must be installed manually. Example Backup the jffs partition. - name : asuswrt task example actions : - asuswrt : host : 192.168.1.1 user : admin password : 'WhatPassword?' backupType : jffs - compress-gz : {} - to-file : path : 'router/jffs.tar.gz' Example Backup the configuration. - name : asuswrt task example actions : - asuswrt : host : 192.168.1.1 user : admin password : 'WhatPassword?' backupType : configuration - to-file : path : 'router/asus.cnf' mikrotik \u00b6 Input : Nothing Output : stream Unaction : No Parameters Name Type Description Optional backupType str Type of the backup (see description) Yes It also accepts any of the parameters of ssh and from - file - ssh . Important parameters Even though not all parameters are listed, required parameters are host , user and either password or identityFile . Description Does a backup of a Mikrotik device with RouterOS. The type of backup can be full - backup , scripts , system - config or global - config . The action first runs the command to make the backup and then runs the action from - file - ssh which will download the file from the device. Warning The action writes files into the device storage but it does not remove them. They can be removed using a post task hook. Example Full backup of the device. - name : mikrotik task example actions : - mikrotik : host : 192.168.15.1 user : admin password : 'WhatPassword?' backupType : full-backup - to-file : path : 'router/full.backup'","title":"Network"},{"location":"actions/network/#actions-network","text":"Network refers here to network devices like switches, routers, NAS\u2026","title":"Actions: Network"},{"location":"actions/network/#asuswrt","text":"Input : Nothing Output : stream Unaction : No Parameters Name Type Description Optional host str IP (or hostname) of the Asus router No user str Router\u2019s user No password str Router\u2019s password No backupType str Type of the backup: configuration or jffs No Description Does a backup from an AsusWRT router through its web admin panel. The file is downloaded and piped to the next action. Warning When doing this action, the router cannot have an active session open. The login from the tool can fail if an active session is still open elsewhere. requests package required In order to use this action, the requests package must be installed manually. Example Backup the jffs partition. - name : asuswrt task example actions : - asuswrt : host : 192.168.1.1 user : admin password : 'WhatPassword?' backupType : jffs - compress-gz : {} - to-file : path : 'router/jffs.tar.gz' Example Backup the configuration. - name : asuswrt task example actions : - asuswrt : host : 192.168.1.1 user : admin password : 'WhatPassword?' backupType : configuration - to-file : path : 'router/asus.cnf'","title":"asuswrt"},{"location":"actions/network/#mikrotik","text":"Input : Nothing Output : stream Unaction : No Parameters Name Type Description Optional backupType str Type of the backup (see description) Yes It also accepts any of the parameters of ssh and from - file - ssh . Important parameters Even though not all parameters are listed, required parameters are host , user and either password or identityFile . Description Does a backup of a Mikrotik device with RouterOS. The type of backup can be full - backup , scripts , system - config or global - config . The action first runs the command to make the backup and then runs the action from - file - ssh which will download the file from the device. Warning The action writes files into the device storage but it does not remove them. They can be removed using a post task hook. Example Full backup of the device. - name : mikrotik task example actions : - mikrotik : host : 192.168.15.1 user : admin password : 'WhatPassword?' backupType : full-backup - to-file : path : 'router/full.backup'","title":"mikrotik"},{"location":"secrets/","text":"Secrets providers \u00b6 To improve security, some of the configuration can be retrieved from a secrets backend. To be more precise, some environment variable can be added and storage providers can also be added from the backends. File Vault","title":"Overview"},{"location":"secrets/#secrets-providers","text":"To improve security, some of the configuration can be retrieved from a secrets backend. To be more precise, some environment variable can be added and storage providers can also be added from the backends. File Vault","title":"Secrets providers"},{"location":"secrets/file/","text":"File \u00b6 This is a simple secrets provider. It is not suitable to use in production, but can be useful to be used in Docker containers. Every environment variable to inject, will be read from the file specified as value. In fact, every value (which are paths) will be transformed to their values. The paths can be absolute, or relative. To resolve relative paths, basePath should be defined, if not then it will look for secrets inside the folder secrets on the config folder. For cloud storage providers, the backend will read json or yaml files, which must have the configuration for a backend. They must use the same structure shown in the storage providers section of the example json. If an object is used instead of a string, then the path must be in the key attribute and the rest of the object is treated as an extension to the configuration that will be loaded from the file. So it is possible to use same credentials between different configurations, but to define specific parameters for each. Note If the basePath is not defined, then it will use ${ configFolder } /secrets as default value. If the basePath is a relative path, then it will be relative to the configFolder . Configuration schema \u00b6 { \"envDefs\" : { \"pg\" : { \"password\" : \"/path/to/pg-password\" , }, \"mysqlpassword\" : \"mysql-password\" }, \"config\" : { \"basePath\" : \"/backups/secrets\" }, \"storage\" : [ \"providers/digital-ocean.json\" , \"providers/gdrive.json\" , { \"key\" : \"providers/amazon.json\" , \"backupsPath\" : \"/backups/server-amg-1\" } ] }","title":"File"},{"location":"secrets/file/#file","text":"This is a simple secrets provider. It is not suitable to use in production, but can be useful to be used in Docker containers. Every environment variable to inject, will be read from the file specified as value. In fact, every value (which are paths) will be transformed to their values. The paths can be absolute, or relative. To resolve relative paths, basePath should be defined, if not then it will look for secrets inside the folder secrets on the config folder. For cloud storage providers, the backend will read json or yaml files, which must have the configuration for a backend. They must use the same structure shown in the storage providers section of the example json. If an object is used instead of a string, then the path must be in the key attribute and the rest of the object is treated as an extension to the configuration that will be loaded from the file. So it is possible to use same credentials between different configurations, but to define specific parameters for each. Note If the basePath is not defined, then it will use ${ configFolder } /secrets as default value. If the basePath is a relative path, then it will be relative to the configFolder .","title":"File"},{"location":"secrets/file/#configuration-schema","text":"{ \"envDefs\" : { \"pg\" : { \"password\" : \"/path/to/pg-password\" , }, \"mysqlpassword\" : \"mysql-password\" }, \"config\" : { \"basePath\" : \"/backups/secrets\" }, \"storage\" : [ \"providers/digital-ocean.json\" , \"providers/gdrive.json\" , { \"key\" : \"providers/amazon.json\" , \"backupsPath\" : \"/backups/server-amg-1\" } ] }","title":"Configuration schema"},{"location":"secrets/vault/","text":"Vault \u00b6 Vault is a production-ready secrets backend, really useful to have credentials stored in a centralized server, but retrievable from any client in a network. Dependency In order to use Vault backend, you must install requests : pip install requests . Currently, it only supports KV backend for reading secrets. Environment variables are replaced by their values from the path in the KV. As every path in the KV storage is Key-Value, you must define which key should get to obtain the value. secrets / backups / env / postgres # user will retrieve the path secrets / backups / env / postgres and key user . If the key is not set, will use value by default. For cloud storage providers, the KV in the path should contain the same structure as expected in the storage provider configuration. In this case, no key must be defined, it will take the whole path as configuration. If an object is used instead of a string, then the key path must be in the key attribute and the rest of the object is treated as an extension to the configuration that will be loaded from Vault. So it is possible to use same credentials between different configurations and servers, but to define specific parameters for each. Configuration schema \u00b6 { \"envDefs\" : { \"pg\" : { \"user\" : \"secret/backups/env/pg#user\" , \"password\" : \"secret/backups/env/pg#password\" }, \"mysql\" : { \"user\" : \"secret/backups/env/mysql#user\" , \"password\" : \"secret/backups/env/mysql#password\" } }, \"config\" : { \"apiBaseUrl\" : \"http://localhost:8200\" , \"roleId\" : \"56c90891-83d5-81da-ac71-02ad8ed7fbfe\" , \"secretId\" : \"9d261dc7-1bef-5759-6c72-63d57e58ffec\" , \"cert\" : \"Path to a certificate bundle or false to disable TLS certificate validation\" }, \"storage\" : [ { \"key\" : \"secret/backups/providers/digital-ocean\" , \"backupsPath\" : \"/backups/pi\" }, \"secret/backups/providers/gdrive\" , \"secret/backups/providers/amazon\" ] }","title":"Vault"},{"location":"secrets/vault/#vault","text":"Vault is a production-ready secrets backend, really useful to have credentials stored in a centralized server, but retrievable from any client in a network. Dependency In order to use Vault backend, you must install requests : pip install requests . Currently, it only supports KV backend for reading secrets. Environment variables are replaced by their values from the path in the KV. As every path in the KV storage is Key-Value, you must define which key should get to obtain the value. secrets / backups / env / postgres # user will retrieve the path secrets / backups / env / postgres and key user . If the key is not set, will use value by default. For cloud storage providers, the KV in the path should contain the same structure as expected in the storage provider configuration. In this case, no key must be defined, it will take the whole path as configuration. If an object is used instead of a string, then the key path must be in the key attribute and the rest of the object is treated as an extension to the configuration that will be loaded from Vault. So it is possible to use same credentials between different configurations and servers, but to define specific parameters for each.","title":"Vault"},{"location":"secrets/vault/#configuration-schema","text":"{ \"envDefs\" : { \"pg\" : { \"user\" : \"secret/backups/env/pg#user\" , \"password\" : \"secret/backups/env/pg#password\" }, \"mysql\" : { \"user\" : \"secret/backups/env/mysql#user\" , \"password\" : \"secret/backups/env/mysql#password\" } }, \"config\" : { \"apiBaseUrl\" : \"http://localhost:8200\" , \"roleId\" : \"56c90891-83d5-81da-ac71-02ad8ed7fbfe\" , \"secretId\" : \"9d261dc7-1bef-5759-6c72-63d57e58ffec\" , \"cert\" : \"Path to a certificate bundle or false to disable TLS certificate validation\" }, \"storage\" : [ { \"key\" : \"secret/backups/providers/digital-ocean\" , \"backupsPath\" : \"/backups/pi\" }, \"secret/backups/providers/gdrive\" , \"secret/backups/providers/amazon\" ] }","title":"Configuration schema"},{"location":"storage/","text":"Storage providers \u00b6 A storage provider is server or service provider that offers a place to store files remotely. Can be a cloud storage provider (like S3 or Google Drive) or servers (like FTP or SFTP). Each provider is different and has different configurations and behaviours, but the goal of the tool is to simplify the upload of backups and the cleanup of them with simple settings. Each storage provider has dependencies that are not installed by default. If you intend to use some of them, first check out which are their dependencies and install them. Otherwise, the tool will fail and it will require you to install the packages. The configuration of a storage provider can be located in the config.json file or provided by a secret provider . List of providers \u00b6 Google Drive Amazon S3 or S3-like Backblaze B2 FTP FTPS SFTP","title":"Overview"},{"location":"storage/#storage-providers","text":"A storage provider is server or service provider that offers a place to store files remotely. Can be a cloud storage provider (like S3 or Google Drive) or servers (like FTP or SFTP). Each provider is different and has different configurations and behaviours, but the goal of the tool is to simplify the upload of backups and the cleanup of them with simple settings. Each storage provider has dependencies that are not installed by default. If you intend to use some of them, first check out which are their dependencies and install them. Otherwise, the tool will fail and it will require you to install the packages. The configuration of a storage provider can be located in the config.json file or provided by a secret provider .","title":"Storage providers"},{"location":"storage/#list-of-providers","text":"Google Drive Amazon S3 or S3-like Backblaze B2 FTP FTPS SFTP","title":"List of providers"},{"location":"storage/b2/","text":"Backblaze B2 \u00b6 Similar to S3 , B2 is a Object cloud storage that offers high performance, high availability and simple interface to store objects in the cloud. Like S3, it also uses keys to identify the objects and slashes / can be used to organize the objects. The backupsPath in B2 is like a prefix for the object keys. It is recommended to put something here to easily organise the backups from the rest of files in the bucket. The initial slash / is removed when uploading the files. You don\u2019t need to create anything under the backupsPath prefix, it will be created automatically with the upload. The content type of the files will be guessed and set in the metadata when uploading. To start using B2, you must get and define the keyId , appKey and bucket . If password is defined, the files will be protected using this setting as password. Warning The backupsPath must start with initial slash / . Dependencies \u00b6 In order to use B2, you must install the following python packages: b2sdk python - magic Configuration schema \u00b6 { \"type\" : \"b2\" , \"backupsPath\" : \"Path in B2 where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"keyId\" : \"B2 Key ID\" , \"appKey\" : \"B2 Application Key\" , \"bucket\" : \"Name of the bucket\" , \"password\" : \"(optional) Protects files with passwords\" }","title":"B2"},{"location":"storage/b2/#backblaze-b2","text":"Similar to S3 , B2 is a Object cloud storage that offers high performance, high availability and simple interface to store objects in the cloud. Like S3, it also uses keys to identify the objects and slashes / can be used to organize the objects. The backupsPath in B2 is like a prefix for the object keys. It is recommended to put something here to easily organise the backups from the rest of files in the bucket. The initial slash / is removed when uploading the files. You don\u2019t need to create anything under the backupsPath prefix, it will be created automatically with the upload. The content type of the files will be guessed and set in the metadata when uploading. To start using B2, you must get and define the keyId , appKey and bucket . If password is defined, the files will be protected using this setting as password. Warning The backupsPath must start with initial slash / .","title":"Backblaze B2"},{"location":"storage/b2/#dependencies","text":"In order to use B2, you must install the following python packages: b2sdk python - magic","title":"Dependencies"},{"location":"storage/b2/#configuration-schema","text":"{ \"type\" : \"b2\" , \"backupsPath\" : \"Path in B2 where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"keyId\" : \"B2 Key ID\" , \"appKey\" : \"B2 Application Key\" , \"bucket\" : \"Name of the bucket\" , \"password\" : \"(optional) Protects files with passwords\" }","title":"Configuration schema"},{"location":"storage/ftp/","text":"FTP \u00b6 FTP is a protocol to transfer files between computers over the network, but it\u2019s not encrypted. It\u2019s quite common protocol used by many hostings and easy to install in servers. To use a FTP server to store the backups remotely, at least the host must be defined. This setting can also contain the port of the server. The username and password can be provided to use them to connect to the server. The backupPath must exist in the server, and the user must have write permissions on it. Take into account that if the user is \u201c chroot ed\u201d, the path will be different from the real path in the server. FTPS \u00b6 FTPS is FTP over SSL/TLS that adds a layer of security over the FTP protocol. Is an extension of the FTP provider, that adds the SSL/TLS layer, and uses the same settings as in the FTP plus two new ones. If custom certificate chains are being used, keyFile and certFile probably will be needed. The certificate chain file allows the tool to identify the server properly and allow it to be used. If client certificate is needed, keyFile can be defined and must point to a private key file. Currently both files must exist in the file system, and certificates cannot be stored in base64 in the configuration. They can be stored in config folder is desired. Dependencies \u00b6 No extra python packages are required to use this provider. Uses ftplib , which is bundled in Python. Configuration schema \u00b6 { \"type\" : \"ftp\" , \"backupsPath\" : \"Path in the FTP where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host (with or without the port) where the FTP server is located\" , \"user\" : \"(optional) User to connect to the FTP server\" , \"password\" : \"(optional) Password for the user\" , \"acct\" : \"(optional) Account information for the user\" } { \"type\" : \"ftps\" , \"backupsPath\" : \"Path in the FTPS where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host (with or without the port) where the FTPS server is located\" , \"user\" : \"(optional) User to connect to the FTPS server\" , \"password\" : \"(optional) Password for the user\" , \"acct\" : \"(optional) Account information for the user\" , \"keyFile\" : \"(optional) If needed, define a custom key file\" , \"certFile\" : \"(optional) If needed, define a custom certificate file\" }","title":"FTP(S)"},{"location":"storage/ftp/#ftp","text":"FTP is a protocol to transfer files between computers over the network, but it\u2019s not encrypted. It\u2019s quite common protocol used by many hostings and easy to install in servers. To use a FTP server to store the backups remotely, at least the host must be defined. This setting can also contain the port of the server. The username and password can be provided to use them to connect to the server. The backupPath must exist in the server, and the user must have write permissions on it. Take into account that if the user is \u201c chroot ed\u201d, the path will be different from the real path in the server.","title":"FTP"},{"location":"storage/ftp/#ftps","text":"FTPS is FTP over SSL/TLS that adds a layer of security over the FTP protocol. Is an extension of the FTP provider, that adds the SSL/TLS layer, and uses the same settings as in the FTP plus two new ones. If custom certificate chains are being used, keyFile and certFile probably will be needed. The certificate chain file allows the tool to identify the server properly and allow it to be used. If client certificate is needed, keyFile can be defined and must point to a private key file. Currently both files must exist in the file system, and certificates cannot be stored in base64 in the configuration. They can be stored in config folder is desired.","title":"FTPS"},{"location":"storage/ftp/#dependencies","text":"No extra python packages are required to use this provider. Uses ftplib , which is bundled in Python.","title":"Dependencies"},{"location":"storage/ftp/#configuration-schema","text":"{ \"type\" : \"ftp\" , \"backupsPath\" : \"Path in the FTP where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host (with or without the port) where the FTP server is located\" , \"user\" : \"(optional) User to connect to the FTP server\" , \"password\" : \"(optional) Password for the user\" , \"acct\" : \"(optional) Account information for the user\" } { \"type\" : \"ftps\" , \"backupsPath\" : \"Path in the FTPS where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host (with or without the port) where the FTPS server is located\" , \"user\" : \"(optional) User to connect to the FTPS server\" , \"password\" : \"(optional) Password for the user\" , \"acct\" : \"(optional) Account information for the user\" , \"keyFile\" : \"(optional) If needed, define a custom key file\" , \"certFile\" : \"(optional) If needed, define a custom certificate file\" }","title":"Configuration schema"},{"location":"storage/gdrive/","text":"Google Drive \u00b6 Google Drive is a cloud storage for all your files, like your USB or external hard-drive where you store stuff, but in the cloud. To be able to use Google Drive as storage provider, You will need the client_secrets . json . You should get them from the Google Developer\u2019s Console , by going to Credentials and creating a new OAuth 2.0 Client IDs or using an existing one. Every OAuth 2.0 entry have a download icon, this will download that file. The file must be stored somewhere in the local file system (for example, in the config folder). The only drawback is that the JSON cannot be stored in-place, inside the configuration. The auth_tokens . json is created when a user logs in. You must use a path accessible from the tool to write the file. If the file does not exist, run the utility manually and (in some point) it will ask you to go to an URL. Here is where you log in with an account and Google will give you a token. Copy and paste it into the terminal. Now you will see the files uploading to Google Drive and the auth tokens file created. The backupsFolder must exist before running the tool. Dependencies \u00b6 In order to use Google Drive, you must install the following python packages: boto3 python - magic Configuration schema \u00b6 { \"type\" : \"gdrive\" , \"backupsPath\" : \"Path in Google Drive where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"clientSecrets\" : \"config/client_secrets.json\" , \"authTokens\" : \"config/auth_tokens.json\" }","title":"Google Drive"},{"location":"storage/gdrive/#google-drive","text":"Google Drive is a cloud storage for all your files, like your USB or external hard-drive where you store stuff, but in the cloud. To be able to use Google Drive as storage provider, You will need the client_secrets . json . You should get them from the Google Developer\u2019s Console , by going to Credentials and creating a new OAuth 2.0 Client IDs or using an existing one. Every OAuth 2.0 entry have a download icon, this will download that file. The file must be stored somewhere in the local file system (for example, in the config folder). The only drawback is that the JSON cannot be stored in-place, inside the configuration. The auth_tokens . json is created when a user logs in. You must use a path accessible from the tool to write the file. If the file does not exist, run the utility manually and (in some point) it will ask you to go to an URL. Here is where you log in with an account and Google will give you a token. Copy and paste it into the terminal. Now you will see the files uploading to Google Drive and the auth tokens file created. The backupsFolder must exist before running the tool.","title":"Google Drive"},{"location":"storage/gdrive/#dependencies","text":"In order to use Google Drive, you must install the following python packages: boto3 python - magic","title":"Dependencies"},{"location":"storage/gdrive/#configuration-schema","text":"{ \"type\" : \"gdrive\" , \"backupsPath\" : \"Path in Google Drive where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"clientSecrets\" : \"config/client_secrets.json\" , \"authTokens\" : \"config/auth_tokens.json\" }","title":"Configuration schema"},{"location":"storage/s3/","text":"Amazon S3 or S3-like storage \u00b6 Object cloud storage that offers scalability, high availability and security using a simple but powerful interface. Objects are stored using keys, and have some attributes and metadata associated. The keys can contain / in their names, and this creates folder-like structures. Every storage is is defined in terms of buckets. Each bucket is like a new drive that will hold different objects. You can use Amazon S3 or any S3 compatible cloud storage provider (like DigitalOcean ) to store the backups. The backupsPath in S3 is like a prefix for the object keys. It is recommended to put something here to easily organise the backups from the rest of files in the bucket. The initial slash / is removed when uploading the files. You don\u2019t need to create anything under the backupsPath prefix, it will be created automatically with the upload. The content type of the files will be guessed and set in the metadata when uploading. To start using it, you must define accessKeyId , accessSecretKey , region and bucket . If using something different from Amazon S3 (like DigitalOcean\u2019s Spaces), you must define the endpoint as well. For example, in Spaces, the endpoint will be https:// ${ region } .digitaloceanspaces.com , where ${ region } is the region you choose when creating the space. Warning The backupsPath must start with initial slash / . Dependencies \u00b6 In order to use S3, you must install the following python packages: boto3 python - magic Configuration schema \u00b6 { \"type\" : \"s3\" , \"backupsPath\" : \"Path in S3 where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"region\" : \"Region of the S3 storage\" , \"endpoint\" : \"Endpoint (if not set, uses Amazon S3 endpoint)\" , \"accessKeyId\" : \"Access Key ID\" , \"accessSecretKey\" : \"Access Secret Key\" , \"bucket\" : \"Name of the bucket\" }","title":"S3"},{"location":"storage/s3/#amazon-s3-or-s3-like-storage","text":"Object cloud storage that offers scalability, high availability and security using a simple but powerful interface. Objects are stored using keys, and have some attributes and metadata associated. The keys can contain / in their names, and this creates folder-like structures. Every storage is is defined in terms of buckets. Each bucket is like a new drive that will hold different objects. You can use Amazon S3 or any S3 compatible cloud storage provider (like DigitalOcean ) to store the backups. The backupsPath in S3 is like a prefix for the object keys. It is recommended to put something here to easily organise the backups from the rest of files in the bucket. The initial slash / is removed when uploading the files. You don\u2019t need to create anything under the backupsPath prefix, it will be created automatically with the upload. The content type of the files will be guessed and set in the metadata when uploading. To start using it, you must define accessKeyId , accessSecretKey , region and bucket . If using something different from Amazon S3 (like DigitalOcean\u2019s Spaces), you must define the endpoint as well. For example, in Spaces, the endpoint will be https:// ${ region } .digitaloceanspaces.com , where ${ region } is the region you choose when creating the space. Warning The backupsPath must start with initial slash / .","title":"Amazon S3 or S3-like storage"},{"location":"storage/s3/#dependencies","text":"In order to use S3, you must install the following python packages: boto3 python - magic","title":"Dependencies"},{"location":"storage/s3/#configuration-schema","text":"{ \"type\" : \"s3\" , \"backupsPath\" : \"Path in S3 where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"region\" : \"Region of the S3 storage\" , \"endpoint\" : \"Endpoint (if not set, uses Amazon S3 endpoint)\" , \"accessKeyId\" : \"Access Key ID\" , \"accessSecretKey\" : \"Access Secret Key\" , \"bucket\" : \"Name of the bucket\" }","title":"Configuration schema"},{"location":"storage/sftp/","text":"SFTP \u00b6 SSH File Transfer Protocol SFTP is a protocol that works over SSH to transfer and manage files over the network. Is not the same as FTP or FTPS, but its similar. Most servers have enabled the SFTP mode if they have a SSH server. OpenSSH implementation have the SFTP module and can be enabled if desired. In order to connect to a SFTP server and use it as provider, the host and user must be defined, and use one of the following authentication methods. The backupPath must exist in the server, and the user must have write permissions on it. The authentication is attempted using the following order: If privateKey or privateKeyFile is defined, then this method will be used. If the private key is cyphered, use password as the passphrase of the key. If allowAgent is true, then will use the SSH Agent to connect to the server. Any key found in ~/ . ssh . If the password is defined, then the classic username/password login will be used (discouraged). The known-hosts list is loaded by default from the default location ~/ . ssh / known_hosts . If hostKeysFilePath is defined, then this file will be used instead. If enableHostKeys is set to false, then no known-hosts will be loaded. The knownHostsPolicy will set the policy that will be used when the SSH connection is set, but the host is being checked as a known or not-known host. The following policies are allowed: reject will close the connection if the host is not-known (default behaviour). auto - add will add to the list of known-hosts if the host is not-known. ignore will print a warning if the host is not-known. Note : if the knownHostsPolicy is auto - add and hostKeysFilePath is defined, then the new host will be saved into the file. Dependencies \u00b6 In order to use SFTP, you must install the following python packages: paramiko Configuration schema \u00b6 { \"type\" : \"sftp\" , \"backupsPath\" : \"Path in the SFTP where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host where the SFTP server is located\" , \"port\" : \"(optional) Port of the SFTP server (by default 22)\" , \"user\" : \"User to connect to the SFTP server\" , \"password\" : \"(optional) Password for the user\" , \"privateKey\" : \"(optional) Private Key in base64\" , \"privateKeyPath\" : \"(optional) Private Key file path\" , \"allowAgent\" : \"(optional) if true, then the connection will interact with the SSH Agent, false if this behaviour is not desired (false by default)\" , \"compress\" : \"(optional) if true, then the connection is compressed (false by default)\" , \"knownHostsPolicy\" : \"(optional) Changes the Known Hosts Policy. 'reject' will reject any connection to a server that is not known (default behaviour), 'auto-add' will add to the known-hosts list this server, 'ignore' will print a warning but it will let you connect.\" , \"hostKeysFilePath\" : \"(optional) Path to the known-hosts file\" , \"enableHostKeys\" : \"(optional) If set to false, it won't load any known-hosts file (by default is true)\" }","title":"SFTP"},{"location":"storage/sftp/#sftp","text":"SSH File Transfer Protocol SFTP is a protocol that works over SSH to transfer and manage files over the network. Is not the same as FTP or FTPS, but its similar. Most servers have enabled the SFTP mode if they have a SSH server. OpenSSH implementation have the SFTP module and can be enabled if desired. In order to connect to a SFTP server and use it as provider, the host and user must be defined, and use one of the following authentication methods. The backupPath must exist in the server, and the user must have write permissions on it. The authentication is attempted using the following order: If privateKey or privateKeyFile is defined, then this method will be used. If the private key is cyphered, use password as the passphrase of the key. If allowAgent is true, then will use the SSH Agent to connect to the server. Any key found in ~/ . ssh . If the password is defined, then the classic username/password login will be used (discouraged). The known-hosts list is loaded by default from the default location ~/ . ssh / known_hosts . If hostKeysFilePath is defined, then this file will be used instead. If enableHostKeys is set to false, then no known-hosts will be loaded. The knownHostsPolicy will set the policy that will be used when the SSH connection is set, but the host is being checked as a known or not-known host. The following policies are allowed: reject will close the connection if the host is not-known (default behaviour). auto - add will add to the list of known-hosts if the host is not-known. ignore will print a warning if the host is not-known. Note : if the knownHostsPolicy is auto - add and hostKeysFilePath is defined, then the new host will be saved into the file.","title":"SFTP"},{"location":"storage/sftp/#dependencies","text":"In order to use SFTP, you must install the following python packages: paramiko","title":"Dependencies"},{"location":"storage/sftp/#configuration-schema","text":"{ \"type\" : \"sftp\" , \"backupsPath\" : \"Path in the SFTP where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host where the SFTP server is located\" , \"port\" : \"(optional) Port of the SFTP server (by default 22)\" , \"user\" : \"User to connect to the SFTP server\" , \"password\" : \"(optional) Password for the user\" , \"privateKey\" : \"(optional) Private Key in base64\" , \"privateKeyPath\" : \"(optional) Private Key file path\" , \"allowAgent\" : \"(optional) if true, then the connection will interact with the SSH Agent, false if this behaviour is not desired (false by default)\" , \"compress\" : \"(optional) if true, then the connection is compressed (false by default)\" , \"knownHostsPolicy\" : \"(optional) Changes the Known Hosts Policy. 'reject' will reject any connection to a server that is not known (default behaviour), 'auto-add' will add to the known-hosts list this server, 'ignore' will print a warning but it will let you connect.\" , \"hostKeysFilePath\" : \"(optional) Path to the known-hosts file\" , \"enableHostKeys\" : \"(optional) If set to false, it won't load any known-hosts file (by default is true)\" }","title":"Configuration schema"}]}