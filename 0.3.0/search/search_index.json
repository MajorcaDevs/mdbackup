{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Backups utility \u00b6 Small but customizable utility to create backups and store them in cloud storage providers. How to install \u00b6 Download from releases the latest wheel package and install it. It is recommended to use a virtual environment to do that. We will show you this way. What do yo need? : An OS different from Windows (Windows is unsupported) :( Python 3.6 or higher rsync and ssh installed (on macOS they are in general installed by default, on Linux distros you may need to install them) bash must be installed, used to run the scripts First select a folder where all the needed files will be stored. It is important not to move (or rename) this folder after installation. Run one of those commands. If both fail, try to install python3-virtualenv (debian based) or pip3 install virtualenv (on macOS). python3 -m venv .venv python3 -m virtualenv .venv When you have the virtual environment created, you have to activate it. With this, you can run python commands and everything you do, will alter the virtual env, not the real one (and so, you don\u2019t need sudo to do things). . .venv/bin/activate # Download the .whl package pip install --upgrade setuptools wheel pip install mdbackup*.whl Now you can run the utility (only if you have enabled the virtual env) with mdbackup . In this folder it is recommended to store the config and steps folders. Note: to be able to use some of the cloud storage and secrets backends, you will be requested to install some packages. Go to the documentation to see what is needed. Documentation \u00b6 Can be found at docs folder. To make the documentation, install the requirements in docs/requirements.txt and run mkdocs build --config-file=mkdocs.yaml .","title":"Home"},{"location":"#backups-utility","text":"Small but customizable utility to create backups and store them in cloud storage providers.","title":"Backups utility"},{"location":"#how-to-install","text":"Download from releases the latest wheel package and install it. It is recommended to use a virtual environment to do that. We will show you this way. What do yo need? : An OS different from Windows (Windows is unsupported) :( Python 3.6 or higher rsync and ssh installed (on macOS they are in general installed by default, on Linux distros you may need to install them) bash must be installed, used to run the scripts First select a folder where all the needed files will be stored. It is important not to move (or rename) this folder after installation. Run one of those commands. If both fail, try to install python3-virtualenv (debian based) or pip3 install virtualenv (on macOS). python3 -m venv .venv python3 -m virtualenv .venv When you have the virtual environment created, you have to activate it. With this, you can run python commands and everything you do, will alter the virtual env, not the real one (and so, you don\u2019t need sudo to do things). . .venv/bin/activate # Download the .whl package pip install --upgrade setuptools wheel pip install mdbackup*.whl Now you can run the utility (only if you have enabled the virtual env) with mdbackup . In this folder it is recommended to store the config and steps folders. Note: to be able to use some of the cloud storage and secrets backends, you will be requested to install some packages. Go to the documentation to see what is needed.","title":"How to install"},{"location":"#documentation","text":"Can be found at docs folder. To make the documentation, install the requirements in docs/requirements.txt and run mkdocs build --config-file=mkdocs.yaml .","title":"Documentation"},{"location":"arguments/","text":"Arguments \u00b6 The tool has a few arguments, in general you may not use them, but to test some parts, could be really useful. usage : mdbackup [- h ] [- c CONFIG ] [-- backup - only ] [-- upload - current - only ] [-- cleanup - only ] Small but customizable utility to create backups and store them in cloud storage providers optional arguments : - h , -- help show this help message and exit - c CONFIG , -- config CONFIG Path to configuration ( default : config / config . json ) -- backup - only Only does the backup actions -- upload - current - only Only uploads the last backup -- cleanup - only Only does the backup cleanup","title":"Arguments"},{"location":"arguments/#arguments","text":"The tool has a few arguments, in general you may not use them, but to test some parts, could be really useful. usage : mdbackup [- h ] [- c CONFIG ] [-- backup - only ] [-- upload - current - only ] [-- cleanup - only ] Small but customizable utility to create backups and store them in cloud storage providers optional arguments : - h , -- help show this help message and exit - c CONFIG , -- config CONFIG Path to configuration ( default : config / config . json ) -- backup - only Only does the backup actions -- upload - current - only Only uploads the last backup -- cleanup - only Only does the backup cleanup","title":"Arguments"},{"location":"configuration/","text":"Configuration \u00b6 You have available under config/config.schema.json the JSON schema of the configuration file. You can use it like this on an app like Visual Studio Code or PyCharm: { \"$schema\" : \"./config.schema.json\" } If you are going to use the $schema , you should download it or reference the URL of the file from the repository directly. This allows you to auto-complete with the elements available in the configuration. But in case you cannot use an app with schema support, here\u2019s it is the (maybe not updated) list of options: { \"backupsPath\" : \"Path where the backups will be stored\" , \"logLevel\" : \"Log level for the app, valid values are: CRITICAL,ERROR,WARNING,INFO,DEBUG\" , \"customUtilsScript\" : \"(optional) Define an additional utilities script that will be loaded in every step script\" , \"maxBackupsKept\" : 7 , \"env\" : { \"docker\" : \"If set, the utilities will run in a docker container instead of using native commands\" , \"pgnetwork\" : \"[Docker] Defines which network will use to connect to the database (default host)\" , \"pgimage\" : \"[Docker] Defines which image will use to run the container (default postgres)\" , \"pghost\" : \"The host of the database (default localhost)\" , \"pguser\" : \"The user to connect in the database (must exist)\" , \"pgpassword\" : \"If set, will use this as password for connecting to the database\" , \"mysqlnetwork\" : \"[Docker] Defines which network will use to connect to the database (default host)\" , \"mysqlimage\" : \"[Docker] Defines which image will use to run the container (default mariadb)\" , \"mysqlhost\" : \"127.0.0.1\" , \"mysqluser\" : \"The username to connect to the database\" , \"mysqlpassword\" : \"If defined, sets the password which will be used to connect to the database\" }, \"secrets\" : { \"secret-provider\" : { \"env\" : { \"pgpassword\" : \"/path/to/pg-password\" , \"mysqlpassword\" : \"mysql-password\" }, \"config\" : { \"setting-1\" : \"value\" , \"setting-2\" : true }, \"storage\" : [ \"storage/digital-ocean\" , { \"key\" : \"storage/gdrive\" , \"backupsPath\" : \"/Backups/mbp\" }, \"storage/amazon\" ] } }, \"compression\" : { \"strategy\" : \"gzip|xz\" , \"level\" : 8 }, \"cypher\" : { \"strategy\" : \"gpg-keys|gpg-passphrase\" , \"passphrase\" : \"If using gpg-passphrase, this will be used as passphrase for the cypher\" , \"keys\" : \"If using gpg-keys, this will be used as recipients option for the gpg cypher (emails)\" , \"algorithm\" : \"Defines the algorithm to use in the cypher process, depends in the strategy (currently one of `gpg --version` cyphers)\" }, \"storage\" : [ { \"type\" : \"provider-type-1\" , \"backupsPath\" : \"Path in the storage provider where to store the backups\" , \"maxBackupsKept\" : 30 , \"provider-specific-param-1\" : \"config/client_secrets.json\" , \"provider-specific-param-2\" : false }, { \"type\" : \"provider-type-2\" , \"backupsPath\" : \"Path in the storage provider where to store the backups\" , \"maxBackupsKept\" : 7 , \"provider-specific-param-1\" : \"THIS_IS-NOT-AN-API-KEY\" , \"provider-specific-param-2\" : \"THIS_IS_NOT_AN-API-S3Cr3t\" , \"provider-specific-param-3\" : 10 } ], \"hooks\" : { \"backup:before\" : \"echo $@\" , \"backup:after\" : \"path/to/script\" , \"backup:error\" : \"wombo combo $1 $2\" , \"upload:before\" : \"echo $@\" , \"upload:after\" : \"echo $@\" , \"upload:error\" : \"echo $@\" , \"oldBackup:deleting\" : \"echo $@\" , \"oldBackup:deleted\" : \"echo $@\" , \"oldBackup:error\" : \"echo $@\" } } The configuration file must be located in config/config.json . It is recommended to put inside config folder other configuration files (like API tokens) or use a secret provider directly. backupsPath \u00b6 The path where all the backups will be stored locally. It will contain all the past backups plus the in-process (if any). When a backup is being done, it will create a .partial folder inside backupsPath and inside the folder, all the copied files and directories will be stored. After a backup, the folder will be renamed to YYYY-MM-DDThh:mm , matching the time when the backup was completed. logLevel \u00b6 Configures the log level. Every log issued to the logger that is below the configured log level will be ignored. By default is set to INFO . The available levels, ordered by importance, are: CRITICAL ERROR WARNING INFO DEBUG customUtilsScript \u00b6 If defined, this script will be included using source ${ customUtilsScript } in every step . Useful to include custom functions to your flow. The script must be compatible with bash . maxBackupsKept \u00b6 Defines how many backups will be kept in the local folder. By default is set to 7. To disable the cleanup, use 0 or null as value of this setting. env \u00b6 This section defines environment variables that will be available when running the steps scripts. It have some predefined (see below), but feel free to fill with any variables you want. The values must be string, int, float or bool. Lists and dictionaries will have undesired behaviours when used. The predefined environment variables, that are used in the predefined functions for the steps are the following: docker \u00b6 Optional If set, some of the function utilities will run in a docker container instead of using native commands. See their documentation to check which functions can be run inside a container. pgnetwork \u00b6 Docker, Optional Defines which network will use to connect to the PostgreSQL database server (by default host ). pgimage \u00b6 Docker, Optional Defines which image will use to run the container to connect to PostgreSQL database server (by default postgres ). pghost \u00b6 Optional The host of the PostgreSQL database server (by default localhost ). pguser \u00b6 Optional The user to connect to the PostgreSQL server (by default postgres ). pgpassword \u00b6 Optional If set, will use this as password for connecting to the PostgreSQL server. mysqlnetwork \u00b6 Docker, Optional Defines which network will use to connect to the MySQL/MariaDB database (by default host ). mysqlimage \u00b6 Docker, Optional Defines which image will use to run the container to connect to the MySQL/MariaDB (default mariadb ). mysqlhost \u00b6 Optional The host of the MySQL/MariaDB database server (by default 127.0.0.1 ). mysqluser \u00b6 Mandatory The username to connect to the MySQL/MariaDB server. mysqlpassword \u00b6 Mandatory If defined, sets the password which will be used to connect to the MySQL/MariaDB server. mikrotikdir \u00b6 Optional Folder name pattern where to store the backups in local for a Mikrotik backup. By default will use mikrotik- ${ host } , where host is the host of the Mikrotik device. mikrotiksshkey \u00b6 Optional If set, will use this SSH Identity Key to connect to the Mikrotik devices. mikrotikpass \u00b6 Optional If set, will use this password to connect to the Mikrotik device ( requires to have installed sshpass ). mikrotikfullbackup \u00b6 If set, will do a full backup of the Mikrotik devices. mikrotikexportscripts \u00b6 If set, will do a scripts backup of the Mikrotik devices. mikrotikexportsystemconfig \u00b6 If set, will do a system config backup of the Mikrotik devices. mikrotikexportglobalconfig \u00b6 If set, will do a global config backup of the Mikrotik devices. secrets \u00b6 Defines all secrets providers available to run the tool. Can obtain values for environment and storage providers from the secret providers in runtime, improving security by having in different places all the secrets. It is optional, but it is recommended to use any secret provider. Every type of secret provider is defined inside the secrets section, where the key of the object is the type, and the object contains the configuration of the provider and what to inject from it. config \u00b6 The configuration section contains provider-specific configuration which allows the provider to work. See secret providers documentation to see the available providers and their configuration. env \u00b6 The environment section defines which environment variables will be populated from the secret provider. The key, like in the env section, is the environment variable, and the value is a provider-specific url/path/identifier that tells the provider where to look for the value. storage \u00b6 The storage section defines storage provider configurations that will be grabbed from the secret provider. Each value of the list is a (secret) provider-specific url/path/identifier that tells the provider where to look for the configuration. The value must have the same structure of the configuration of the storage provider. compression \u00b6 If defined, some function utilities for the steps will generate a compressed file using the configuration defined in this setting. Also, if the backups are uploaded to a storage provider, folders will be compressed using this configuration. Can be used with or without cyphering . strategy \u00b6 The strategy defines which compression algorithm is going to be used. Currently, the algorithms supported are gzip (which requires gzip to be installed) and xz (which requires xz to be installed). In general, a lot of Linux distributions includes these commands, as well as in macOS. But it\u2019s worth to check their existence before using them. level \u00b6 The compression level. Higher values indicates better but slower compressions. Values accepted for gzip are from 1 to 9. Values accepted for xz are from 0 to 9 (by default is 6, 7-9 are not recommended). cypher \u00b6 If defined, some function utilities for the steps will generate a encrypted file using the configuration defined in this setting. Also, if the backups are uploaded to a storage provider, folders will be encrypted using this configuration. Can be used with or without compression . strategy \u00b6 Defines which strategy to use to encrypt the data. Currently the supported cypher strategies are: gpg-passphrase - Uses a passphrase to encrypt and decrypt the data (requires gpg2 ). The passphrase setting will be used as passphrase. gpg-keys - Uses the keys associated to the list of emails to encrypt the data (requires gpg2 ). The keys list will be used as recipients/emails list that will be used to protect the data. The people in the list will be able to decrypt the data and no one else. Recommended over passphrase . algorithm \u00b6 If defined, will use this algorithm to encrypt the data. The supported algorithms and the default algorithm can be found in gpg --version gpg2 --version . storage \u00b6 If defined, the last backup will be uploaded to the configured storage providers . Each provider must define the type , the backupsPath and maxBackupsKept , as well as the provider specific configuration. Unknown types will be ignored. type \u00b6 Defines the type of the storage provider for the entry. The list of storage providers can be found in the \u2018Storage providers\u2019 section. backupsPath \u00b6 Path in the storage provider where to save the backups. Is the same concept as the backupsPath from above. Some providers need this folder to exist, while others no. If possible, try to ensure that the folder is created before uploading any backup. maxBackupsKept \u00b6 Defines how many backups will be kept in the storage provider. If set to 0 or null will not clean anything. There\u2019s no default value for this, so a value must be always provided. hooks \u00b6 Hooks run a script or program when something is going to happen or just happened. Can be useful to trigger some post-action things, to send messages through Slack or to manipulate the output of a backup. Each key defines the hook type, and their values is the script, program or one-line script that will be run in when the hook is triggered. The hooks run on a sh shell, so hooks like echo $@ will work out of the box. See hooks section.","title":"Configuration"},{"location":"configuration/#configuration","text":"You have available under config/config.schema.json the JSON schema of the configuration file. You can use it like this on an app like Visual Studio Code or PyCharm: { \"$schema\" : \"./config.schema.json\" } If you are going to use the $schema , you should download it or reference the URL of the file from the repository directly. This allows you to auto-complete with the elements available in the configuration. But in case you cannot use an app with schema support, here\u2019s it is the (maybe not updated) list of options: { \"backupsPath\" : \"Path where the backups will be stored\" , \"logLevel\" : \"Log level for the app, valid values are: CRITICAL,ERROR,WARNING,INFO,DEBUG\" , \"customUtilsScript\" : \"(optional) Define an additional utilities script that will be loaded in every step script\" , \"maxBackupsKept\" : 7 , \"env\" : { \"docker\" : \"If set, the utilities will run in a docker container instead of using native commands\" , \"pgnetwork\" : \"[Docker] Defines which network will use to connect to the database (default host)\" , \"pgimage\" : \"[Docker] Defines which image will use to run the container (default postgres)\" , \"pghost\" : \"The host of the database (default localhost)\" , \"pguser\" : \"The user to connect in the database (must exist)\" , \"pgpassword\" : \"If set, will use this as password for connecting to the database\" , \"mysqlnetwork\" : \"[Docker] Defines which network will use to connect to the database (default host)\" , \"mysqlimage\" : \"[Docker] Defines which image will use to run the container (default mariadb)\" , \"mysqlhost\" : \"127.0.0.1\" , \"mysqluser\" : \"The username to connect to the database\" , \"mysqlpassword\" : \"If defined, sets the password which will be used to connect to the database\" }, \"secrets\" : { \"secret-provider\" : { \"env\" : { \"pgpassword\" : \"/path/to/pg-password\" , \"mysqlpassword\" : \"mysql-password\" }, \"config\" : { \"setting-1\" : \"value\" , \"setting-2\" : true }, \"storage\" : [ \"storage/digital-ocean\" , { \"key\" : \"storage/gdrive\" , \"backupsPath\" : \"/Backups/mbp\" }, \"storage/amazon\" ] } }, \"compression\" : { \"strategy\" : \"gzip|xz\" , \"level\" : 8 }, \"cypher\" : { \"strategy\" : \"gpg-keys|gpg-passphrase\" , \"passphrase\" : \"If using gpg-passphrase, this will be used as passphrase for the cypher\" , \"keys\" : \"If using gpg-keys, this will be used as recipients option for the gpg cypher (emails)\" , \"algorithm\" : \"Defines the algorithm to use in the cypher process, depends in the strategy (currently one of `gpg --version` cyphers)\" }, \"storage\" : [ { \"type\" : \"provider-type-1\" , \"backupsPath\" : \"Path in the storage provider where to store the backups\" , \"maxBackupsKept\" : 30 , \"provider-specific-param-1\" : \"config/client_secrets.json\" , \"provider-specific-param-2\" : false }, { \"type\" : \"provider-type-2\" , \"backupsPath\" : \"Path in the storage provider where to store the backups\" , \"maxBackupsKept\" : 7 , \"provider-specific-param-1\" : \"THIS_IS-NOT-AN-API-KEY\" , \"provider-specific-param-2\" : \"THIS_IS_NOT_AN-API-S3Cr3t\" , \"provider-specific-param-3\" : 10 } ], \"hooks\" : { \"backup:before\" : \"echo $@\" , \"backup:after\" : \"path/to/script\" , \"backup:error\" : \"wombo combo $1 $2\" , \"upload:before\" : \"echo $@\" , \"upload:after\" : \"echo $@\" , \"upload:error\" : \"echo $@\" , \"oldBackup:deleting\" : \"echo $@\" , \"oldBackup:deleted\" : \"echo $@\" , \"oldBackup:error\" : \"echo $@\" } } The configuration file must be located in config/config.json . It is recommended to put inside config folder other configuration files (like API tokens) or use a secret provider directly.","title":"Configuration"},{"location":"configuration/#backupspath","text":"The path where all the backups will be stored locally. It will contain all the past backups plus the in-process (if any). When a backup is being done, it will create a .partial folder inside backupsPath and inside the folder, all the copied files and directories will be stored. After a backup, the folder will be renamed to YYYY-MM-DDThh:mm , matching the time when the backup was completed.","title":"backupsPath"},{"location":"configuration/#loglevel","text":"Configures the log level. Every log issued to the logger that is below the configured log level will be ignored. By default is set to INFO . The available levels, ordered by importance, are: CRITICAL ERROR WARNING INFO DEBUG","title":"logLevel"},{"location":"configuration/#customutilsscript","text":"If defined, this script will be included using source ${ customUtilsScript } in every step . Useful to include custom functions to your flow. The script must be compatible with bash .","title":"customUtilsScript"},{"location":"configuration/#maxbackupskept","text":"Defines how many backups will be kept in the local folder. By default is set to 7. To disable the cleanup, use 0 or null as value of this setting.","title":"maxBackupsKept"},{"location":"configuration/#env","text":"This section defines environment variables that will be available when running the steps scripts. It have some predefined (see below), but feel free to fill with any variables you want. The values must be string, int, float or bool. Lists and dictionaries will have undesired behaviours when used. The predefined environment variables, that are used in the predefined functions for the steps are the following:","title":"env"},{"location":"configuration/#docker","text":"Optional If set, some of the function utilities will run in a docker container instead of using native commands. See their documentation to check which functions can be run inside a container.","title":"docker"},{"location":"configuration/#pgnetwork","text":"Docker, Optional Defines which network will use to connect to the PostgreSQL database server (by default host ).","title":"pgnetwork"},{"location":"configuration/#pgimage","text":"Docker, Optional Defines which image will use to run the container to connect to PostgreSQL database server (by default postgres ).","title":"pgimage"},{"location":"configuration/#pghost","text":"Optional The host of the PostgreSQL database server (by default localhost ).","title":"pghost"},{"location":"configuration/#pguser","text":"Optional The user to connect to the PostgreSQL server (by default postgres ).","title":"pguser"},{"location":"configuration/#pgpassword","text":"Optional If set, will use this as password for connecting to the PostgreSQL server.","title":"pgpassword"},{"location":"configuration/#mysqlnetwork","text":"Docker, Optional Defines which network will use to connect to the MySQL/MariaDB database (by default host ).","title":"mysqlnetwork"},{"location":"configuration/#mysqlimage","text":"Docker, Optional Defines which image will use to run the container to connect to the MySQL/MariaDB (default mariadb ).","title":"mysqlimage"},{"location":"configuration/#mysqlhost","text":"Optional The host of the MySQL/MariaDB database server (by default 127.0.0.1 ).","title":"mysqlhost"},{"location":"configuration/#mysqluser","text":"Mandatory The username to connect to the MySQL/MariaDB server.","title":"mysqluser"},{"location":"configuration/#mysqlpassword","text":"Mandatory If defined, sets the password which will be used to connect to the MySQL/MariaDB server.","title":"mysqlpassword"},{"location":"configuration/#mikrotikdir","text":"Optional Folder name pattern where to store the backups in local for a Mikrotik backup. By default will use mikrotik- ${ host } , where host is the host of the Mikrotik device.","title":"mikrotikdir"},{"location":"configuration/#mikrotiksshkey","text":"Optional If set, will use this SSH Identity Key to connect to the Mikrotik devices.","title":"mikrotiksshkey"},{"location":"configuration/#mikrotikpass","text":"Optional If set, will use this password to connect to the Mikrotik device ( requires to have installed sshpass ).","title":"mikrotikpass"},{"location":"configuration/#mikrotikfullbackup","text":"If set, will do a full backup of the Mikrotik devices.","title":"mikrotikfullbackup"},{"location":"configuration/#mikrotikexportscripts","text":"If set, will do a scripts backup of the Mikrotik devices.","title":"mikrotikexportscripts"},{"location":"configuration/#mikrotikexportsystemconfig","text":"If set, will do a system config backup of the Mikrotik devices.","title":"mikrotikexportsystemconfig"},{"location":"configuration/#mikrotikexportglobalconfig","text":"If set, will do a global config backup of the Mikrotik devices.","title":"mikrotikexportglobalconfig"},{"location":"configuration/#secrets","text":"Defines all secrets providers available to run the tool. Can obtain values for environment and storage providers from the secret providers in runtime, improving security by having in different places all the secrets. It is optional, but it is recommended to use any secret provider. Every type of secret provider is defined inside the secrets section, where the key of the object is the type, and the object contains the configuration of the provider and what to inject from it.","title":"secrets"},{"location":"configuration/#config","text":"The configuration section contains provider-specific configuration which allows the provider to work. See secret providers documentation to see the available providers and their configuration.","title":"config"},{"location":"configuration/#env_1","text":"The environment section defines which environment variables will be populated from the secret provider. The key, like in the env section, is the environment variable, and the value is a provider-specific url/path/identifier that tells the provider where to look for the value.","title":"env"},{"location":"configuration/#storage","text":"The storage section defines storage provider configurations that will be grabbed from the secret provider. Each value of the list is a (secret) provider-specific url/path/identifier that tells the provider where to look for the configuration. The value must have the same structure of the configuration of the storage provider.","title":"storage"},{"location":"configuration/#compression","text":"If defined, some function utilities for the steps will generate a compressed file using the configuration defined in this setting. Also, if the backups are uploaded to a storage provider, folders will be compressed using this configuration. Can be used with or without cyphering .","title":"compression"},{"location":"configuration/#strategy","text":"The strategy defines which compression algorithm is going to be used. Currently, the algorithms supported are gzip (which requires gzip to be installed) and xz (which requires xz to be installed). In general, a lot of Linux distributions includes these commands, as well as in macOS. But it\u2019s worth to check their existence before using them.","title":"strategy"},{"location":"configuration/#level","text":"The compression level. Higher values indicates better but slower compressions. Values accepted for gzip are from 1 to 9. Values accepted for xz are from 0 to 9 (by default is 6, 7-9 are not recommended).","title":"level"},{"location":"configuration/#cypher","text":"If defined, some function utilities for the steps will generate a encrypted file using the configuration defined in this setting. Also, if the backups are uploaded to a storage provider, folders will be encrypted using this configuration. Can be used with or without compression .","title":"cypher"},{"location":"configuration/#strategy_1","text":"Defines which strategy to use to encrypt the data. Currently the supported cypher strategies are: gpg-passphrase - Uses a passphrase to encrypt and decrypt the data (requires gpg2 ). The passphrase setting will be used as passphrase. gpg-keys - Uses the keys associated to the list of emails to encrypt the data (requires gpg2 ). The keys list will be used as recipients/emails list that will be used to protect the data. The people in the list will be able to decrypt the data and no one else. Recommended over passphrase .","title":"strategy"},{"location":"configuration/#algorithm","text":"If defined, will use this algorithm to encrypt the data. The supported algorithms and the default algorithm can be found in gpg --version gpg2 --version .","title":"algorithm"},{"location":"configuration/#storage_1","text":"If defined, the last backup will be uploaded to the configured storage providers . Each provider must define the type , the backupsPath and maxBackupsKept , as well as the provider specific configuration. Unknown types will be ignored.","title":"storage"},{"location":"configuration/#type","text":"Defines the type of the storage provider for the entry. The list of storage providers can be found in the \u2018Storage providers\u2019 section.","title":"type"},{"location":"configuration/#backupspath_1","text":"Path in the storage provider where to save the backups. Is the same concept as the backupsPath from above. Some providers need this folder to exist, while others no. If possible, try to ensure that the folder is created before uploading any backup.","title":"backupsPath"},{"location":"configuration/#maxbackupskept_1","text":"Defines how many backups will be kept in the storage provider. If set to 0 or null will not clean anything. There\u2019s no default value for this, so a value must be always provided.","title":"maxBackupsKept"},{"location":"configuration/#hooks","text":"Hooks run a script or program when something is going to happen or just happened. Can be useful to trigger some post-action things, to send messages through Slack or to manipulate the output of a backup. Each key defines the hook type, and their values is the script, program or one-line script that will be run in when the hook is triggered. The hooks run on a sh shell, so hooks like echo $@ will work out of the box. See hooks section.","title":"hooks"},{"location":"docker/","text":"Docker \u00b6 Use the image \u00b6 It is recommended to have a look to the Quick Start if you did not do it yet\u2026 To be done\u2026 Build image \u00b6 There are two flavours for the mdbackup image: one based on Alpine Linux and the other based on Debian (slim). To build the Debian version, run: docker image build -t mdbackup:slim -f docker/Dockerfile-slim . To build the Alpine Linux version, then run: docker image build -t mdbackup:slim -f docker/Dockerfile-alpine .","title":"Docker"},{"location":"docker/#docker","text":"","title":"Docker"},{"location":"docker/#use-the-image","text":"It is recommended to have a look to the Quick Start if you did not do it yet\u2026 To be done\u2026","title":"Use the image"},{"location":"docker/#build-image","text":"There are two flavours for the mdbackup image: one based on Alpine Linux and the other based on Debian (slim). To build the Debian version, run: docker image build -t mdbackup:slim -f docker/Dockerfile-slim . To build the Alpine Linux version, then run: docker image build -t mdbackup:slim -f docker/Dockerfile-alpine .","title":"Build image"},{"location":"hooks/","text":"Hooks \u00b6 Hooks are scripts that run when some event is going to happen or just happened. It is useful to define custom actions with your own scripts, including one-liner scrips. The hook is run with sh so scripts can be defined inlined. If a hook is not defined, won\u2019t run anything. To define the hooks, see hooks in the configuration . The output of the script is redirected to the logger using the DEBUG level. If you have some issues with your hook script, set the log level to DEBUG . backup:before \u00b6 When : Before starting doing backups. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). backup:after \u00b6 When : After all backups are done. Parameters : The folder where the backups are stored. backup:error \u00b6 When : When the backup process failed but before the partial folder is deleted. Parameters : The folder where the backups were being stored. Exception message. (Optional) Step name. backup:step:${step_name}:before \u00b6 Note: ${ step_name } must be the filename of the step. i.e.: 01 - web.sh is a filename and step name. When : Before starting running the step script. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). backup:step:${step_name}:after \u00b6 Note: ${ step_name } must be the filename of the step. i.e.: 01 - web.sh is a filename and step name. When : After running the step script. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). backup:step:${step_name}:error \u00b6 Note: ${ step_name } must be the filename of the step. i.e.: 01 - web.sh is a filename and step name. When : After running the step script but the script failed to run. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). The exception message. upload:before \u00b6 When : Before uploading the backup to a storage provider. Parameters : The type of the provider. The path to the backups folder. upload:after \u00b6 When : After uploading the backup to a storage provider. Parameters : The type of the provider. The path to the backups folder. The path in the storage provider where the backup is stored. upload:error \u00b6 When : After uploading the backup to a storage provider but it failed. Parameters : The type of the provider. The path to the backups folder. The exception message. oldBackup:deleting \u00b6 When : Just before a backup folder is going to be deleted. Parameters : The path of the backup to be deleted. oldBackup:deleted \u00b6 When : After a backup folder was deleted. Parameters : The path of the backup deleted. oldBackup:error \u00b6 When : After a backup folder was going to be deleted, but it failed to do so. Parameters : The path of the backup to be deleted.","title":"Hooks"},{"location":"hooks/#hooks","text":"Hooks are scripts that run when some event is going to happen or just happened. It is useful to define custom actions with your own scripts, including one-liner scrips. The hook is run with sh so scripts can be defined inlined. If a hook is not defined, won\u2019t run anything. To define the hooks, see hooks in the configuration . The output of the script is redirected to the logger using the DEBUG level. If you have some issues with your hook script, set the log level to DEBUG .","title":"Hooks"},{"location":"hooks/#backupbefore","text":"When : Before starting doing backups. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ).","title":"backup:before"},{"location":"hooks/#backupafter","text":"When : After all backups are done. Parameters : The folder where the backups are stored.","title":"backup:after"},{"location":"hooks/#backuperror","text":"When : When the backup process failed but before the partial folder is deleted. Parameters : The folder where the backups were being stored. Exception message. (Optional) Step name.","title":"backup:error"},{"location":"hooks/#backupstepstep_namebefore","text":"Note: ${ step_name } must be the filename of the step. i.e.: 01 - web.sh is a filename and step name. When : Before starting running the step script. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ).","title":"backup:step:${step_name}:before"},{"location":"hooks/#backupstepstep_nameafter","text":"Note: ${ step_name } must be the filename of the step. i.e.: 01 - web.sh is a filename and step name. When : After running the step script. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ).","title":"backup:step:${step_name}:after"},{"location":"hooks/#backupstepstep_nameerror","text":"Note: ${ step_name } must be the filename of the step. i.e.: 01 - web.sh is a filename and step name. When : After running the step script but the script failed to run. Parameters : The folder where the backups are going to be stored during the process (will end with .partial ). The exception message.","title":"backup:step:${step_name}:error"},{"location":"hooks/#uploadbefore","text":"When : Before uploading the backup to a storage provider. Parameters : The type of the provider. The path to the backups folder.","title":"upload:before"},{"location":"hooks/#uploadafter","text":"When : After uploading the backup to a storage provider. Parameters : The type of the provider. The path to the backups folder. The path in the storage provider where the backup is stored.","title":"upload:after"},{"location":"hooks/#uploaderror","text":"When : After uploading the backup to a storage provider but it failed. Parameters : The type of the provider. The path to the backups folder. The exception message.","title":"upload:error"},{"location":"hooks/#oldbackupdeleting","text":"When : Just before a backup folder is going to be deleted. Parameters : The path of the backup to be deleted.","title":"oldBackup:deleting"},{"location":"hooks/#oldbackupdeleted","text":"When : After a backup folder was deleted. Parameters : The path of the backup deleted.","title":"oldBackup:deleted"},{"location":"hooks/#oldbackuperror","text":"When : After a backup folder was going to be deleted, but it failed to do so. Parameters : The path of the backup to be deleted.","title":"oldBackup:error"},{"location":"quick-start/","text":"Quick start guide \u00b6 mdbackup is tested under Linux and macOS, but it should work on any platform where Python has support and has bash , except for Windows. Before installing the tool, make sure to have installed at least rsync and bash . Most Linux distributions have both installed, some only bash . On macOS, both come installed by default. Also check that Python 3.6 or higher is installed (use [ brew ] on macOS for that). In this guide, a virtual environment will be used to install and use the tool. It is not recommended to install it directly in the system. First prepare the virtual environment. You can use venv or virtualenv , but the first will be used. python -m venv .venv python -m virtualenv .venv Note: python here it is referred to the python 3 executable. In some platforms will be python3 . Once the environment is created, we must \u201cactivate\u201d it: . .venv/bin/activate Now you can use python and pip and everything will work from and install to the virtual environment. Now you can download the tool and install it: #Download using curl... curl -sSL https://github.com/MajorcaDevs/mdbackup/releases/latest/download/mdbackup.whl > mdbackup.whl #...or wget wget https://github.com/MajorcaDevs/mdbackup/releases/latest/download/mdbackup.whl #If they don't work, go to https://github.com/MajorcaDevs/mdbackup/releases and copy the URL from the latest release pip install mdbackup.whl mdbackup --help To check if the tool is installed properly, run the help of the tool and you should get something like this . You can also use the Docker container . But it is recommended to read the guide to get an idea. First configuration \u00b6 Note: this will get through getting an initial configuration for backups. To get in more detail, check out the Configuration page. In order to get your first backup, the tool must be configured properly. To achieve this, you will learn the core concepts used in the tool and how to use them to fit your needs. The tool needs three folders to work: config : a folder where the configuration, and other files related to configuration, tokens or cookies are going to be stored. steps : a folder where the backup logic is stored in form of bash scripts. *put a full path here*: the folder, placed in some folder, where the backups are going to be stored. The folder in where you are right now should have the following structure: .venv / \u2026 config/ config.json steps/ 01.sh mdbackup.whl And the third folder, it does not matter where is placed, but it will be used soon to store backups. It can be a network storage, an external drive or a partition in some local drive. It is recommended to store them outside the root partition ( / ), if possible. Did you notice the config.json ? This file holds the configuration of the tool. Write in it the following: { \"backupsPath\" : \"/the/path/to/the/folder/where/the/backups/are/going/to/be/stored\" , \"logLevel\" : \"DEBUG\" , \"env\" : {} } Note: you can download the JSON Schema and use it to validate the structure: \"$schema\": \"./config.schema.json\", . You can grab it from the latest release. This configuration is really basic and tells the tool where to place the backups, which log level to use (will be very verbose, but it is OK for now) and to inject no extra environment variables. Now we need to define the logic to create backups. We called it steps . A step is just a bash script (without the shebang !# ) that do some actions to copy/backup any data, files or folders to the backup folder (referring \u201cbackup folder\u201d to the folder where the current backup is going to be stored). The tool defines some predefined functions that are useful to make most common backup operations. Some of them requires to have installed extra CLIs or to have installed Docker. But if these functions are not used, no errors will arise. The steps are executed following the natural order (alphanumeric order) of the names of the files. For example, 01.sh will run before 02.sh . It can be as many scripts as desired, or just one. It does not matter. So now we will give some contents to 01.sh (the file shown in the tree). backup-folder \"/home/YourUser\" home #macOS users, use \"/Users/YourUser\" This step will copy your home directory and all its contents into the backup folder using rsync . With one line, you will get a full backup of a folder! You can use any other folder you want just to try, this is an example. Check that the script has execution permissions for the user , and possible for all : chmod ua+x ./steps/01.sh . Now try running the tool: mdbackup . If everything is well configured, you will have a new folder in the backups folder with the date and time of now and with your folder copied. Well, try now to make a backup again. If the folder being copied is large enough, you will notice that this time, the backup took less time than the first time. This is because the mode in which rsync runs checks which files have been modified and just copies these ones. The rest of unmodified files are hard-linked from the latest backup. It\u2019s an \u201cincremental\u201d backup! Note that current folder is always present and is a soft link to the latest backup. So it\u2019s easy to access to the latest backup from the file explorer or from the command line :) Now you have backups of whatever you want! Just configure the tool and write the right scripts to fit your needs. Note: it is possible that you will need to run the tool as root to access some system folders. Remember that the virtual environment is not inherited when using sudo . Make your own script, or checkout one of the contrib folder . Injecting environment variables \u00b6 Some of the steps have as parameters some environment variables. These can be defined in the env section of the json. The object must be a simple Key/Value structure. Complex structures (like objects or arrays) are not allowed. Let\u2019s try to make a backup from a postgres database with the predefined function. Also these variables can be used natively with some postgres tools because they also understand them. For this example, pg_dump must be installed on the system. First, the env must be changed to add the new settings: { \"...\" : \"...\" , \"env\" : { \"pghost\" : \"localhost\" , \"pguser\" : \"postgres\" , \"pgpassword\" : \"WonderfulPassword123\" } } Then, we will add a new step script called 02.sh with the following contents: backup-postgres-database \"databasename\" The tool will read the environment variables, and inject them in the scripts environment when they are running. The key are transformed to be upper case, so you don\u2019t need to. If everything went well, you now will have a databasename.sql file in the backup folder. Compression \u00b6 What if your postgres backup takes some MB and if the file were compressed, will be a few KBs? Or what if you want to upload to a cloud storage and you want to save some bytes if possible when uploading them? (That\u2019s an spoiler, yes) The tool, with the help of tar , xz and gzip commands, can compress whatever you want. And some of the predefined functions takes advantage of this and compress for you some of the files. The configuration for this is simple: { \"...\" : \"...\" , \"compression\" : { \"strategy\" : \"gzip\" , \"level\" : 7 } } With this simple configuration, the backup-postgres-database and many more will output a compressed file. You can even take advantage of this by using the helper function compress-encrypt that will run a command that must output the data to stdout and will compress (and encrypt [oh no, another spoiler]) it using the configuration. Encrypt \u00b6 You already know that you can encrypt, but you don\u2019t know how yet. You need OpenGPG 2 in order to cypher files. And as well as the compression, will be used automatically by some functions, and in the upload to storage providers. Most Linux distributions have installed gpg tools, but you must check the version. On macOS, install GPGTools . There\u2019s two ways to encrypt data, in this guide will be using the passphrase. Here is a configuration example: { \"...\" : \"...\" , \"cypher\" : { \"strategy\" : \"gpg-passphrase\" , \"passphrase\" : \"ThisIsAPassphrase321\" , \"algorithm\" : \"AES256\" } } Note: the algorithm changes between distributions. Check the available in gpg --version . In this guide we will be using AES256. Now the backup-postgres-database will be compressed and encrypted using a passphrase and gpg , and any of the functions that uses compress-encrypt . Upload to the cloud \u00b6 Well, you can also use an FTP server for this, but it\u2019s cool to say \u201cTO THE CLOUD\u201d . If desired, the backups can be uploaded to what we call \u201cstorage provider\u201d. It can be a FTP or SFTP server, or a Cloud Storage (like S3 or Google Drive). This is run after the backup is done, and can be uploaded to one or more storage providers. Also to speed up the upload, the folders are written into a tar file, compressed (if configured) and encrypted (if configured). It is recommended to, at least, configure the compression in order to save some storage at the cloud. There are many storage providers to choose. In the guid will be using a FTP server to quickly show how to upload files. Also because this is the only storage provider that does not need to install any extra packages. This is the configuration for the FTP: { \"...\" : \"...\" , \"storage\" : [ { \"type\" : \"ftp\" , \"backupsPath\" : \"/backups\" , \"host\" : \"ftp.local\" , \"user\" : \"anonymous\" } ] } This will upload the files to the FTP server ftp.local , in the folder /backups using the anonymous user and no password. The end \u00b6 There\u2019s more to learn about the tool, but this is a rather good introduction to it. Take a look to the other sections of this documentation to learn and discover new stuff of the tool. Concerned about too much hardcoded credentials in the configuration file? Check out the Secret backends .","title":"Quick start"},{"location":"quick-start/#quick-start-guide","text":"mdbackup is tested under Linux and macOS, but it should work on any platform where Python has support and has bash , except for Windows. Before installing the tool, make sure to have installed at least rsync and bash . Most Linux distributions have both installed, some only bash . On macOS, both come installed by default. Also check that Python 3.6 or higher is installed (use [ brew ] on macOS for that). In this guide, a virtual environment will be used to install and use the tool. It is not recommended to install it directly in the system. First prepare the virtual environment. You can use venv or virtualenv , but the first will be used. python -m venv .venv python -m virtualenv .venv Note: python here it is referred to the python 3 executable. In some platforms will be python3 . Once the environment is created, we must \u201cactivate\u201d it: . .venv/bin/activate Now you can use python and pip and everything will work from and install to the virtual environment. Now you can download the tool and install it: #Download using curl... curl -sSL https://github.com/MajorcaDevs/mdbackup/releases/latest/download/mdbackup.whl > mdbackup.whl #...or wget wget https://github.com/MajorcaDevs/mdbackup/releases/latest/download/mdbackup.whl #If they don't work, go to https://github.com/MajorcaDevs/mdbackup/releases and copy the URL from the latest release pip install mdbackup.whl mdbackup --help To check if the tool is installed properly, run the help of the tool and you should get something like this . You can also use the Docker container . But it is recommended to read the guide to get an idea.","title":"Quick start guide"},{"location":"quick-start/#first-configuration","text":"Note: this will get through getting an initial configuration for backups. To get in more detail, check out the Configuration page. In order to get your first backup, the tool must be configured properly. To achieve this, you will learn the core concepts used in the tool and how to use them to fit your needs. The tool needs three folders to work: config : a folder where the configuration, and other files related to configuration, tokens or cookies are going to be stored. steps : a folder where the backup logic is stored in form of bash scripts. *put a full path here*: the folder, placed in some folder, where the backups are going to be stored. The folder in where you are right now should have the following structure: .venv / \u2026 config/ config.json steps/ 01.sh mdbackup.whl And the third folder, it does not matter where is placed, but it will be used soon to store backups. It can be a network storage, an external drive or a partition in some local drive. It is recommended to store them outside the root partition ( / ), if possible. Did you notice the config.json ? This file holds the configuration of the tool. Write in it the following: { \"backupsPath\" : \"/the/path/to/the/folder/where/the/backups/are/going/to/be/stored\" , \"logLevel\" : \"DEBUG\" , \"env\" : {} } Note: you can download the JSON Schema and use it to validate the structure: \"$schema\": \"./config.schema.json\", . You can grab it from the latest release. This configuration is really basic and tells the tool where to place the backups, which log level to use (will be very verbose, but it is OK for now) and to inject no extra environment variables. Now we need to define the logic to create backups. We called it steps . A step is just a bash script (without the shebang !# ) that do some actions to copy/backup any data, files or folders to the backup folder (referring \u201cbackup folder\u201d to the folder where the current backup is going to be stored). The tool defines some predefined functions that are useful to make most common backup operations. Some of them requires to have installed extra CLIs or to have installed Docker. But if these functions are not used, no errors will arise. The steps are executed following the natural order (alphanumeric order) of the names of the files. For example, 01.sh will run before 02.sh . It can be as many scripts as desired, or just one. It does not matter. So now we will give some contents to 01.sh (the file shown in the tree). backup-folder \"/home/YourUser\" home #macOS users, use \"/Users/YourUser\" This step will copy your home directory and all its contents into the backup folder using rsync . With one line, you will get a full backup of a folder! You can use any other folder you want just to try, this is an example. Check that the script has execution permissions for the user , and possible for all : chmod ua+x ./steps/01.sh . Now try running the tool: mdbackup . If everything is well configured, you will have a new folder in the backups folder with the date and time of now and with your folder copied. Well, try now to make a backup again. If the folder being copied is large enough, you will notice that this time, the backup took less time than the first time. This is because the mode in which rsync runs checks which files have been modified and just copies these ones. The rest of unmodified files are hard-linked from the latest backup. It\u2019s an \u201cincremental\u201d backup! Note that current folder is always present and is a soft link to the latest backup. So it\u2019s easy to access to the latest backup from the file explorer or from the command line :) Now you have backups of whatever you want! Just configure the tool and write the right scripts to fit your needs. Note: it is possible that you will need to run the tool as root to access some system folders. Remember that the virtual environment is not inherited when using sudo . Make your own script, or checkout one of the contrib folder .","title":"First configuration"},{"location":"quick-start/#injecting-environment-variables","text":"Some of the steps have as parameters some environment variables. These can be defined in the env section of the json. The object must be a simple Key/Value structure. Complex structures (like objects or arrays) are not allowed. Let\u2019s try to make a backup from a postgres database with the predefined function. Also these variables can be used natively with some postgres tools because they also understand them. For this example, pg_dump must be installed on the system. First, the env must be changed to add the new settings: { \"...\" : \"...\" , \"env\" : { \"pghost\" : \"localhost\" , \"pguser\" : \"postgres\" , \"pgpassword\" : \"WonderfulPassword123\" } } Then, we will add a new step script called 02.sh with the following contents: backup-postgres-database \"databasename\" The tool will read the environment variables, and inject them in the scripts environment when they are running. The key are transformed to be upper case, so you don\u2019t need to. If everything went well, you now will have a databasename.sql file in the backup folder.","title":"Injecting environment variables"},{"location":"quick-start/#compression","text":"What if your postgres backup takes some MB and if the file were compressed, will be a few KBs? Or what if you want to upload to a cloud storage and you want to save some bytes if possible when uploading them? (That\u2019s an spoiler, yes) The tool, with the help of tar , xz and gzip commands, can compress whatever you want. And some of the predefined functions takes advantage of this and compress for you some of the files. The configuration for this is simple: { \"...\" : \"...\" , \"compression\" : { \"strategy\" : \"gzip\" , \"level\" : 7 } } With this simple configuration, the backup-postgres-database and many more will output a compressed file. You can even take advantage of this by using the helper function compress-encrypt that will run a command that must output the data to stdout and will compress (and encrypt [oh no, another spoiler]) it using the configuration.","title":"Compression"},{"location":"quick-start/#encrypt","text":"You already know that you can encrypt, but you don\u2019t know how yet. You need OpenGPG 2 in order to cypher files. And as well as the compression, will be used automatically by some functions, and in the upload to storage providers. Most Linux distributions have installed gpg tools, but you must check the version. On macOS, install GPGTools . There\u2019s two ways to encrypt data, in this guide will be using the passphrase. Here is a configuration example: { \"...\" : \"...\" , \"cypher\" : { \"strategy\" : \"gpg-passphrase\" , \"passphrase\" : \"ThisIsAPassphrase321\" , \"algorithm\" : \"AES256\" } } Note: the algorithm changes between distributions. Check the available in gpg --version . In this guide we will be using AES256. Now the backup-postgres-database will be compressed and encrypted using a passphrase and gpg , and any of the functions that uses compress-encrypt .","title":"Encrypt"},{"location":"quick-start/#upload-to-the-cloud","text":"Well, you can also use an FTP server for this, but it\u2019s cool to say \u201cTO THE CLOUD\u201d . If desired, the backups can be uploaded to what we call \u201cstorage provider\u201d. It can be a FTP or SFTP server, or a Cloud Storage (like S3 or Google Drive). This is run after the backup is done, and can be uploaded to one or more storage providers. Also to speed up the upload, the folders are written into a tar file, compressed (if configured) and encrypted (if configured). It is recommended to, at least, configure the compression in order to save some storage at the cloud. There are many storage providers to choose. In the guid will be using a FTP server to quickly show how to upload files. Also because this is the only storage provider that does not need to install any extra packages. This is the configuration for the FTP: { \"...\" : \"...\" , \"storage\" : [ { \"type\" : \"ftp\" , \"backupsPath\" : \"/backups\" , \"host\" : \"ftp.local\" , \"user\" : \"anonymous\" } ] } This will upload the files to the FTP server ftp.local , in the folder /backups using the anonymous user and no password.","title":"Upload to the cloud"},{"location":"quick-start/#the-end","text":"There\u2019s more to learn about the tool, but this is a rather good introduction to it. Take a look to the other sections of this documentation to learn and discover new stuff of the tool. Concerned about too much hardcoded credentials in the configuration file? Check out the Secret backends .","title":"The end"},{"location":"run-as-service/","text":"Automating running of backups \u00b6 In this section, systemd and cron ways are going to be explained. systemd is the preferred way in case your system has it. It is supposed you already have configured the environment and it works. For both ways, download the file automated-script.sh in some place. In this example, the same folder, where the venv , configuration and steps are located, is going to be used. curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/automated-script.sh > automated-script.sh chmod +x automated-script.sh systemd \u00b6 For systemd, you need to download backups.service and backups.timer , copy them to /etc/systemd/system and enable the timer. sudo bash -c \"curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/backups.service > /etc/systemd/system/backups.service\" sudo bash -c \"curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/backups.timer > /etc/systemd/system/backups.timer\" sudo nano /etc/systemd/system/backups.service #Modify the path to the script !! sudo nano /etc/systemd/system/backups.timer #Check when the timer is going to fire !! sudo systemctl enable backups.timer sudo systemctl start backups.timer nano automated-script.sh #Modify the path of CONFIG_FOLDER You must modify the path to the script in the backups.service and the CONFIG_FOLDER in the automated-script.sh . It is recommended to check if you like when the timer is going to fire (by default is every night at 1am). crontab \u00b6 For crontab, you need to edit the root s crontab and add a new entry. Also modify the automated-script.sh file to remove the comment in the last line to have logs visible in the same folder where the configuration and steps are placed. An example of crontab entry could be: 0 1 * * * /backups/tool/automated-script.sh","title":"Run as a service"},{"location":"run-as-service/#automating-running-of-backups","text":"In this section, systemd and cron ways are going to be explained. systemd is the preferred way in case your system has it. It is supposed you already have configured the environment and it works. For both ways, download the file automated-script.sh in some place. In this example, the same folder, where the venv , configuration and steps are located, is going to be used. curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/automated-script.sh > automated-script.sh chmod +x automated-script.sh","title":"Automating running of backups"},{"location":"run-as-service/#systemd","text":"For systemd, you need to download backups.service and backups.timer , copy them to /etc/systemd/system and enable the timer. sudo bash -c \"curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/backups.service > /etc/systemd/system/backups.service\" sudo bash -c \"curl -sSL https://github.com/MajorcaDevs/mdbackup/raw/master/contrib/backups.timer > /etc/systemd/system/backups.timer\" sudo nano /etc/systemd/system/backups.service #Modify the path to the script !! sudo nano /etc/systemd/system/backups.timer #Check when the timer is going to fire !! sudo systemctl enable backups.timer sudo systemctl start backups.timer nano automated-script.sh #Modify the path of CONFIG_FOLDER You must modify the path to the script in the backups.service and the CONFIG_FOLDER in the automated-script.sh . It is recommended to check if you like when the timer is going to fire (by default is every night at 1am).","title":"systemd"},{"location":"run-as-service/#crontab","text":"For crontab, you need to edit the root s crontab and add a new entry. Also modify the automated-script.sh file to remove the comment in the last line to have logs visible in the same folder where the configuration and steps are placed. An example of crontab entry could be: 0 1 * * * /backups/tool/automated-script.sh","title":"crontab"},{"location":"secrets/","text":"Secrets providers \u00b6 To improve security, some of the configuration can be retrieved from a secrets backend. To be more precise, some environment variable can be added and storage providers can also be added from the backends. File Vault","title":"Overview"},{"location":"secrets/#secrets-providers","text":"To improve security, some of the configuration can be retrieved from a secrets backend. To be more precise, some environment variable can be added and storage providers can also be added from the backends. File Vault","title":"Secrets providers"},{"location":"secrets/file/","text":"File \u00b6 This is a simple secrets provider. It is not suitable to use in production, but can be useful to be used in Docker containers. Every environment variable to inject, will be read from the file specified as value. In fact, every value (which are paths) will be transformed to their values. The paths can be absolute, or relative. To resolve relative paths, you must define basePath . For cloud storage providers, the backend will read json or yaml files, which must have the configuration for a backend. They must use the same structure shown in the storage providers section of the example json. If an object is used instead of a string, then the path must be in the key attribute and the rest of the object is treated as an extension to the configuration that will be loaded from the file. So it is possible to use same credentials between different configurations, but to define specific parameters for each. To be able to read yaml files, you must install pyyaml : pip install pyyaml Configuration schema \u00b6 { \"env\" : { \"pgpassword\" : \"/path/to/pg-password\" , \"mysqlpassword\" : \"mysql-password\" }, \"config\" : { \"basePath\" : \"/backups/secrets\" }, \"storage\" : [ \"providers/digital-ocean.json\" , \"providers/gdrive.json\" , { \"key\" : \"providers/amazon.json\" , \"backupsPath\" : \"/backups/server-amg-1\" } ] }","title":"File"},{"location":"secrets/file/#file","text":"This is a simple secrets provider. It is not suitable to use in production, but can be useful to be used in Docker containers. Every environment variable to inject, will be read from the file specified as value. In fact, every value (which are paths) will be transformed to their values. The paths can be absolute, or relative. To resolve relative paths, you must define basePath . For cloud storage providers, the backend will read json or yaml files, which must have the configuration for a backend. They must use the same structure shown in the storage providers section of the example json. If an object is used instead of a string, then the path must be in the key attribute and the rest of the object is treated as an extension to the configuration that will be loaded from the file. So it is possible to use same credentials between different configurations, but to define specific parameters for each. To be able to read yaml files, you must install pyyaml : pip install pyyaml","title":"File"},{"location":"secrets/file/#configuration-schema","text":"{ \"env\" : { \"pgpassword\" : \"/path/to/pg-password\" , \"mysqlpassword\" : \"mysql-password\" }, \"config\" : { \"basePath\" : \"/backups/secrets\" }, \"storage\" : [ \"providers/digital-ocean.json\" , \"providers/gdrive.json\" , { \"key\" : \"providers/amazon.json\" , \"backupsPath\" : \"/backups/server-amg-1\" } ] }","title":"Configuration schema"},{"location":"secrets/vault/","text":"Vault \u00b6 Vault is a production-ready secrets backend, really useful to have credentials stored in a centralized server, but retrievable from any client in a network. In order to use Vault backend, you must install requests : pip install requests Currently, it only supports KV backend for reading secrets. Environment variables are replaced by their values from the path in the KV. As every path in the KV storage is Key-Value, you must define which key should get to obtain the value. secrets/backups/env/postgres#user will retrieve the path secrets/backups/env/postgres and key user . If the key is not set, will use value by default. For cloud storage providers, the KV in the path should contain the same structure as expected in the storage provider configuration. In this case, no key must be defined, it will take the whole path as configuration. If an object is used instead of a string, then the key path must be in the key attribute and the rest of the object is treated as an extension to the configuration that will be loaded from Vault. So it is possible to use same credentials between different configurations and servers, but to define specific parameters for each. Configuration schema \u00b6 { \"env\" : { \"pguser\" : \"secret/backups/env/pg#user\" , \"pgpassword\" : \"secret/backups/env/pg#password\" , \"mysqluser\" : \"secret/backups/env/mysql#user\" , \"mysqlpassword\" : \"secret/backups/env/mysql#password\" }, \"config\" : { \"apiBaseUrl\" : \"http://localhost:8200\" , \"roleId\" : \"56c90891-83d5-81da-ac71-02ad8ed7fbfe\" , \"secretId\" : \"9d261dc7-1bef-5759-6c72-63d57e58ffec\" , \"cert\" : \"Path to a certificate bundle or false to disable TLS certificate validation\" }, \"storage\" : [ { \"key\" : \"secret/backups/providers/digital-ocean\" , \"backupsPath\" : \"/backups/pi\" }, \"secret/backups/providers/gdrive\" , \"secret/backups/providers/amazon\" ] }","title":"Vault"},{"location":"secrets/vault/#vault","text":"Vault is a production-ready secrets backend, really useful to have credentials stored in a centralized server, but retrievable from any client in a network. In order to use Vault backend, you must install requests : pip install requests Currently, it only supports KV backend for reading secrets. Environment variables are replaced by their values from the path in the KV. As every path in the KV storage is Key-Value, you must define which key should get to obtain the value. secrets/backups/env/postgres#user will retrieve the path secrets/backups/env/postgres and key user . If the key is not set, will use value by default. For cloud storage providers, the KV in the path should contain the same structure as expected in the storage provider configuration. In this case, no key must be defined, it will take the whole path as configuration. If an object is used instead of a string, then the key path must be in the key attribute and the rest of the object is treated as an extension to the configuration that will be loaded from Vault. So it is possible to use same credentials between different configurations and servers, but to define specific parameters for each.","title":"Vault"},{"location":"secrets/vault/#configuration-schema","text":"{ \"env\" : { \"pguser\" : \"secret/backups/env/pg#user\" , \"pgpassword\" : \"secret/backups/env/pg#password\" , \"mysqluser\" : \"secret/backups/env/mysql#user\" , \"mysqlpassword\" : \"secret/backups/env/mysql#password\" }, \"config\" : { \"apiBaseUrl\" : \"http://localhost:8200\" , \"roleId\" : \"56c90891-83d5-81da-ac71-02ad8ed7fbfe\" , \"secretId\" : \"9d261dc7-1bef-5759-6c72-63d57e58ffec\" , \"cert\" : \"Path to a certificate bundle or false to disable TLS certificate validation\" }, \"storage\" : [ { \"key\" : \"secret/backups/providers/digital-ocean\" , \"backupsPath\" : \"/backups/pi\" }, \"secret/backups/providers/gdrive\" , \"secret/backups/providers/amazon\" ] }","title":"Configuration schema"},{"location":"steps/","text":"Steps \u00b6 To make backups, some shell scripts must be create inside the steps folder, located in the current working directory cwd . They will be run in alphabetical order one by one, but these scripts are not full scripts. The tool prepares the script with some environment variables (based on the configuration, the env section and the injected env from the secret providers ) and some functions that will be available inside the step script. By default, there\u2019s some functions available, but you can add new functions by defining your own script and adding its path to customUtilsScript setting. A simple step script that copies the content of a folder will be this: 01 - web.sh backup-folder \"/var/web/\" web || exit $? You can define as many steps as you wish. The idea is to keep every step as simple as possible to simplify debugging. If the logLevel is set to DEBUG it will log all the output of the scripts. If set to INFO part of the output will be logged. Function utilities \u00b6 The steps runs with some function utilities defined that can be used to simplify the backup task. All functions receives parameters via arguments or via environment variables. The environment variables in general are defined by the tool through the configuration and secret providers, but can be overridden in the same script (take care when doing this). The environment variables declared in each function must be defined in lowercase in the configuration. Some of the environment variables are defined automatically, it is not a good idea to override them. compress-encrypt \u00b6 Description : Executes a command and the output it generates, will apply the compression and encrypt strategies, based on the configuration. If no of them are defined, then the input and output will be the same (won\u2019t do anything). Parameters : Command to run that will output something Base file name that will be created Environment variables : COMPRESSION_STRATEGY : Defined by the compression configuration . COMPRESSION_LEVEL : Defined by the compression configuration . CYPHER_STRATEGY : Defined by the cypher configuration . CYPHER_KEYS : Defined by the cypher configuration . CYPHER_PASSPHRASE : Defined by the cypher configuration . Example : compress-encrypt \"cat /dev/random\" \"random.bin\" Requirements : If using the compression strategy gzip , gzip and tar must be installed on the system. If using the compression strategy xz , xz and tar must be installed on the system. If using any cypher strategy, gpg (v2.x.x series) must be installed on the system. The receivers used in the keys setting must have been imported previously. backup-folder \u00b6 Description : Copies a folder to another that will be inside the backup folder, using rsync . Parameters : Source path of the backup Name of the folder where will be stored the copy of the source inside the backup folder Extra arguments will be passed to rsync Example : backup-folder \"/var/www/html\" my-web will copy the contents of /var/www/html into the folder .../my-web . Requirements : rsync must be installed on the system. backup-remote-folder \u00b6 Description : Copies a remote folder to a local folder placed inside the backup folder. Uses rsync and ssh . To make it work, the server that is doing the backup with the user that will do the backups (in general, root ) must create a pair of public/private keys for ssh ( ssh-keygen -f ~/.ssh/id_rsa -q -P \"\" for example). If you already have one, you don\u2019t need to create one if your key has no passphrase. When the key is generated, you can copy the public key ( cat ~/.ssh/id_rsa.pub ) and paste it inside the ~/.ssh/authorized_keys file of the remote server using the same user (generally root ). Parameters : Domain or IP of the remote server Source path of the backup Name of the folder where will be stored the copy of the source inside the backup folder Extra arguments will be passed to rsync Example : backup-remote-folder '10.10.10.254' \"/var/lib/unifi/backup/autobackup/\" unifi will copy the contents of /var/lib/unifi/backup/autobackup/ from the server 10.10.10.254 to the local directory unifi . Requirements : rsync must be installed on the system. ssh (the client) must be installed on the system. backup-postgres-database \u00b6 Description : Makes a backup of a database in PostgreSQL. By default uses the tools installed in the system, but if docker is define in the settings, it will use a docker container. Uses the configured compress and cypher strategies to create the backup. Parameters : Database to backup Environment variables : PGNETWORK : (Docker only) Defines in which network the container will be attached. Default is host . PGIMAGE : (Docker only) Defines which image will be used to run the container. Default is postgres . PGHOST : Defines the host where the database is located. Default localhost . PGUSER : Defines the user from who the connection will be made. Must be an existing user in the OS. Defaults to postgres . PGPASSWORD : If set, this password will be used to connect to the database. Example : backup-postgres-database \"postgres\" Will copy the database postgres into a compressed (and maybe encrypted) sql script named the same as the database. Requirements : If using Docker , docker must be installed, and the image must exist. The image must have pg_dump installed. The image will be pulled automatically if it is not in the system, but it won\u2019t be updated. You can use a hook to do it. If not using Docker , pg_dump must be installed on the system. Uses compress-encrypt internally. backup-mysql-database \u00b6 Description : Makes a backup of a database in MySQL/MariaDB. By default uses the tools installed in the system, but if docker is define in the settings, it will use a docker container. Uses the configured compress and cypher strategies to create the backup. Parameters : Database to backup Environment variables : MYSQLNETWORK : (Docker only) Defines in which network the container will be attached. Default is host . MYSQLIMAGE : (Docker only) Defines which image will be used to run the container. Default is mariadb . MYSQLHOST : Defines the host where the database is located. Default localhost . MYSQLUSER : Defines the user from who the connection will be made. Defaults to user that runs the utility. MYSQLPASSWORD : If set, this password will be used to connect to the database. Example : backup-mysql-database \"wordpress\" Will copy the database wordpress into a compressed (and maybe encrypted) sql script named the same as the database. Requirements : If using Docker , docker must be installed, and the image must exist. The image must have mysqldump installed and be compatible with the MySQL/MariaDB server. The image will be pulled automatically if it is not in the system, but it won\u2019t be updated. You can use a hook to do it. If not using Docker , mysqldump must be installed on the system and be compatible with the MySQL/MariaDB server. Uses compress-encrypt internally. backup-docker-volume \u00b6 Description : Makes a backup (in a .tar file) of a Docker volume given its name. Uses the configured compress and cypher strategies to create the backup. Parameters : Name of the volume to backup Example : backup-docker-volume \"wordpress-content\" Requirements : Requires docker to be installed and running. Uses alpine image to make the copy. The image is pulled automatically, but not updated. To update it, hooks can be used to, or run docker image pull alpine before any volume backup. Uses compress-encrypt internally. backup-docker-volume-physically \u00b6 Description : Makes a backup of a Docker volume given its name copying the folder given in docker volume inspect json. Parameters : Name of the volume to backup Example : backup-docker-volume-physically \"wordpress-content\" Requires docker to be installed and running. The volume must use the local storage, and must be accessible by the tool. Uses backup-folder internally. backup-file \u00b6 Description : Makes a backup of a file. Compares with the previous backup to avoid a copy and hard-link with the previous one, or copies it if there are differences.. Parameters : File to backup (optional) Folder where to put the file when copying Example : backup-file /etc/ssh/sshd sys-config/etc/ssh backup-file-encrypted \u00b6 Description : Makes a backup of a file and compress and encrypts it using the configuration in strategies defined. Compares with the previous backup to avoid a copy and hard-link with the previous one, or copies it if there are differences. Parameters : File to backup (optional) Folder where to put the file when copying Example : backup-file-encrypted /etc/ssh/sshd sys-config/etc/ssh Requirements : Uses compress-encrypt internally. backup-mikrotik \u00b6 Description : Makes a backup of a Mikrotik device via SSH. The files will be stored in the device and then downloaded into the local backups folder. To connect to the device, mikrotiksshkey or mikrotikpass must be defined in order to work. Parameters : User of the mikrotik device Host of the mikrotik device (Optional) Port to the SSH of the device - by default 22 Environment variables : MIKROTIKDIR : (Optional) Folder where to store the backups in local. MIKROTIKSSHKEY : (Either) Will use this SSH Identity Key to connect to the device. MIKROTIKPASS : (Either) Will use this password to connect to the device (requires sshpass ). MIKROTIKFULLBACKUP : (Optional) If set, will do a full backup. MIKROTIKEXPORTSCRIPTS : (Optional) If set, will do a scripts backup. MIKROTIKEXPORTSYSTEMCONFIG : (Optional) If set, will do a system config backup. MIKROTIKEXPORTGLOBALCONFIG : (Optional) If set, will do a global config backup. Example : backup-mikrotik \"mdbackup\" \"192.168.1.1\" 2222 Create custom functions \u00b6 You can create custom functions and let the tool load them in each step script for you. Here it is some advices to help you get through. Remember that your function should be specific and do only one thing, but it should be as generic as possible. If you intend to get your function in as a predefined function, this must be a must . If the output is a single file or a few files not in a folder, it is recommended to use the helper function compress-encrypt , so it will output a compressed and/or encrypted file, depending on the configuration. Or if you change some configuration of the compression or cyphering, the changes will be reflected in your code as well. If there are credentials involved, get them from environment variables, never from parameters which someone could hardcode them. Remember that the environment variables can be obtained from secret backends which are more secure thatn hardcoded in any script nor the configuration file. If you have to change the directory, keep track of the initial current working directory (cwd), because all the files and folders must be written inside this folder. Don\u2019t hardcode this path, get it from cwd or $PWD . Docker can be used? Then could be a good idea to add an implementation that uses a docker container instead of the tools in the system. Using PostgreSQL or MySQL/MariaDB tools? Well, there are undocumented helpers that you can take advantage: __run_mysql and __run_psql . Just append this at the beginning of the command ( __run_psql pg_dump database ) and will take advantage of some of the configurations from the environment variables. You can even combine this with compress-encrypt : compress-encrypt \"__run_mysql mysqldump -h $MYSQLHOST database\" \"database.sql\" . Ask for new functions \u00b6 Do you think the tool should include any function predefined? Did you create a custom function and you think it should be included in the tool? Don\u2019t hesitate in filling an issue or sending us a pull request .","title":"Steps"},{"location":"steps/#steps","text":"To make backups, some shell scripts must be create inside the steps folder, located in the current working directory cwd . They will be run in alphabetical order one by one, but these scripts are not full scripts. The tool prepares the script with some environment variables (based on the configuration, the env section and the injected env from the secret providers ) and some functions that will be available inside the step script. By default, there\u2019s some functions available, but you can add new functions by defining your own script and adding its path to customUtilsScript setting. A simple step script that copies the content of a folder will be this: 01 - web.sh backup-folder \"/var/web/\" web || exit $? You can define as many steps as you wish. The idea is to keep every step as simple as possible to simplify debugging. If the logLevel is set to DEBUG it will log all the output of the scripts. If set to INFO part of the output will be logged.","title":"Steps"},{"location":"steps/#function-utilities","text":"The steps runs with some function utilities defined that can be used to simplify the backup task. All functions receives parameters via arguments or via environment variables. The environment variables in general are defined by the tool through the configuration and secret providers, but can be overridden in the same script (take care when doing this). The environment variables declared in each function must be defined in lowercase in the configuration. Some of the environment variables are defined automatically, it is not a good idea to override them.","title":"Function utilities"},{"location":"steps/#compress-encrypt","text":"Description : Executes a command and the output it generates, will apply the compression and encrypt strategies, based on the configuration. If no of them are defined, then the input and output will be the same (won\u2019t do anything). Parameters : Command to run that will output something Base file name that will be created Environment variables : COMPRESSION_STRATEGY : Defined by the compression configuration . COMPRESSION_LEVEL : Defined by the compression configuration . CYPHER_STRATEGY : Defined by the cypher configuration . CYPHER_KEYS : Defined by the cypher configuration . CYPHER_PASSPHRASE : Defined by the cypher configuration . Example : compress-encrypt \"cat /dev/random\" \"random.bin\" Requirements : If using the compression strategy gzip , gzip and tar must be installed on the system. If using the compression strategy xz , xz and tar must be installed on the system. If using any cypher strategy, gpg (v2.x.x series) must be installed on the system. The receivers used in the keys setting must have been imported previously.","title":"compress-encrypt"},{"location":"steps/#backup-folder","text":"Description : Copies a folder to another that will be inside the backup folder, using rsync . Parameters : Source path of the backup Name of the folder where will be stored the copy of the source inside the backup folder Extra arguments will be passed to rsync Example : backup-folder \"/var/www/html\" my-web will copy the contents of /var/www/html into the folder .../my-web . Requirements : rsync must be installed on the system.","title":"backup-folder"},{"location":"steps/#backup-remote-folder","text":"Description : Copies a remote folder to a local folder placed inside the backup folder. Uses rsync and ssh . To make it work, the server that is doing the backup with the user that will do the backups (in general, root ) must create a pair of public/private keys for ssh ( ssh-keygen -f ~/.ssh/id_rsa -q -P \"\" for example). If you already have one, you don\u2019t need to create one if your key has no passphrase. When the key is generated, you can copy the public key ( cat ~/.ssh/id_rsa.pub ) and paste it inside the ~/.ssh/authorized_keys file of the remote server using the same user (generally root ). Parameters : Domain or IP of the remote server Source path of the backup Name of the folder where will be stored the copy of the source inside the backup folder Extra arguments will be passed to rsync Example : backup-remote-folder '10.10.10.254' \"/var/lib/unifi/backup/autobackup/\" unifi will copy the contents of /var/lib/unifi/backup/autobackup/ from the server 10.10.10.254 to the local directory unifi . Requirements : rsync must be installed on the system. ssh (the client) must be installed on the system.","title":"backup-remote-folder"},{"location":"steps/#backup-postgres-database","text":"Description : Makes a backup of a database in PostgreSQL. By default uses the tools installed in the system, but if docker is define in the settings, it will use a docker container. Uses the configured compress and cypher strategies to create the backup. Parameters : Database to backup Environment variables : PGNETWORK : (Docker only) Defines in which network the container will be attached. Default is host . PGIMAGE : (Docker only) Defines which image will be used to run the container. Default is postgres . PGHOST : Defines the host where the database is located. Default localhost . PGUSER : Defines the user from who the connection will be made. Must be an existing user in the OS. Defaults to postgres . PGPASSWORD : If set, this password will be used to connect to the database. Example : backup-postgres-database \"postgres\" Will copy the database postgres into a compressed (and maybe encrypted) sql script named the same as the database. Requirements : If using Docker , docker must be installed, and the image must exist. The image must have pg_dump installed. The image will be pulled automatically if it is not in the system, but it won\u2019t be updated. You can use a hook to do it. If not using Docker , pg_dump must be installed on the system. Uses compress-encrypt internally.","title":"backup-postgres-database"},{"location":"steps/#backup-mysql-database","text":"Description : Makes a backup of a database in MySQL/MariaDB. By default uses the tools installed in the system, but if docker is define in the settings, it will use a docker container. Uses the configured compress and cypher strategies to create the backup. Parameters : Database to backup Environment variables : MYSQLNETWORK : (Docker only) Defines in which network the container will be attached. Default is host . MYSQLIMAGE : (Docker only) Defines which image will be used to run the container. Default is mariadb . MYSQLHOST : Defines the host where the database is located. Default localhost . MYSQLUSER : Defines the user from who the connection will be made. Defaults to user that runs the utility. MYSQLPASSWORD : If set, this password will be used to connect to the database. Example : backup-mysql-database \"wordpress\" Will copy the database wordpress into a compressed (and maybe encrypted) sql script named the same as the database. Requirements : If using Docker , docker must be installed, and the image must exist. The image must have mysqldump installed and be compatible with the MySQL/MariaDB server. The image will be pulled automatically if it is not in the system, but it won\u2019t be updated. You can use a hook to do it. If not using Docker , mysqldump must be installed on the system and be compatible with the MySQL/MariaDB server. Uses compress-encrypt internally.","title":"backup-mysql-database"},{"location":"steps/#backup-docker-volume","text":"Description : Makes a backup (in a .tar file) of a Docker volume given its name. Uses the configured compress and cypher strategies to create the backup. Parameters : Name of the volume to backup Example : backup-docker-volume \"wordpress-content\" Requirements : Requires docker to be installed and running. Uses alpine image to make the copy. The image is pulled automatically, but not updated. To update it, hooks can be used to, or run docker image pull alpine before any volume backup. Uses compress-encrypt internally.","title":"backup-docker-volume"},{"location":"steps/#backup-docker-volume-physically","text":"Description : Makes a backup of a Docker volume given its name copying the folder given in docker volume inspect json. Parameters : Name of the volume to backup Example : backup-docker-volume-physically \"wordpress-content\" Requires docker to be installed and running. The volume must use the local storage, and must be accessible by the tool. Uses backup-folder internally.","title":"backup-docker-volume-physically"},{"location":"steps/#backup-file","text":"Description : Makes a backup of a file. Compares with the previous backup to avoid a copy and hard-link with the previous one, or copies it if there are differences.. Parameters : File to backup (optional) Folder where to put the file when copying Example : backup-file /etc/ssh/sshd sys-config/etc/ssh","title":"backup-file"},{"location":"steps/#backup-file-encrypted","text":"Description : Makes a backup of a file and compress and encrypts it using the configuration in strategies defined. Compares with the previous backup to avoid a copy and hard-link with the previous one, or copies it if there are differences. Parameters : File to backup (optional) Folder where to put the file when copying Example : backup-file-encrypted /etc/ssh/sshd sys-config/etc/ssh Requirements : Uses compress-encrypt internally.","title":"backup-file-encrypted"},{"location":"steps/#backup-mikrotik","text":"Description : Makes a backup of a Mikrotik device via SSH. The files will be stored in the device and then downloaded into the local backups folder. To connect to the device, mikrotiksshkey or mikrotikpass must be defined in order to work. Parameters : User of the mikrotik device Host of the mikrotik device (Optional) Port to the SSH of the device - by default 22 Environment variables : MIKROTIKDIR : (Optional) Folder where to store the backups in local. MIKROTIKSSHKEY : (Either) Will use this SSH Identity Key to connect to the device. MIKROTIKPASS : (Either) Will use this password to connect to the device (requires sshpass ). MIKROTIKFULLBACKUP : (Optional) If set, will do a full backup. MIKROTIKEXPORTSCRIPTS : (Optional) If set, will do a scripts backup. MIKROTIKEXPORTSYSTEMCONFIG : (Optional) If set, will do a system config backup. MIKROTIKEXPORTGLOBALCONFIG : (Optional) If set, will do a global config backup. Example : backup-mikrotik \"mdbackup\" \"192.168.1.1\" 2222","title":"backup-mikrotik"},{"location":"steps/#create-custom-functions","text":"You can create custom functions and let the tool load them in each step script for you. Here it is some advices to help you get through. Remember that your function should be specific and do only one thing, but it should be as generic as possible. If you intend to get your function in as a predefined function, this must be a must . If the output is a single file or a few files not in a folder, it is recommended to use the helper function compress-encrypt , so it will output a compressed and/or encrypted file, depending on the configuration. Or if you change some configuration of the compression or cyphering, the changes will be reflected in your code as well. If there are credentials involved, get them from environment variables, never from parameters which someone could hardcode them. Remember that the environment variables can be obtained from secret backends which are more secure thatn hardcoded in any script nor the configuration file. If you have to change the directory, keep track of the initial current working directory (cwd), because all the files and folders must be written inside this folder. Don\u2019t hardcode this path, get it from cwd or $PWD . Docker can be used? Then could be a good idea to add an implementation that uses a docker container instead of the tools in the system. Using PostgreSQL or MySQL/MariaDB tools? Well, there are undocumented helpers that you can take advantage: __run_mysql and __run_psql . Just append this at the beginning of the command ( __run_psql pg_dump database ) and will take advantage of some of the configurations from the environment variables. You can even combine this with compress-encrypt : compress-encrypt \"__run_mysql mysqldump -h $MYSQLHOST database\" \"database.sql\" .","title":"Create custom functions"},{"location":"steps/#ask-for-new-functions","text":"Do you think the tool should include any function predefined? Did you create a custom function and you think it should be included in the tool? Don\u2019t hesitate in filling an issue or sending us a pull request .","title":"Ask for new functions"},{"location":"storage/","text":"Storage providers \u00b6 A storage provider is server or service provider that offers a place to store files remotely. Can be a cloud storage provider (like S3 or Google Drive) or servers (like FTP or SFTP). Each provider is different and has different configurations and behaviours, but the goal of the tool is to simplify the upload of backups and the cleanup of them with simple settings. Each storage provider has dependencies that are not installed by default. If you intend to use some of them, first check out which are their dependencies and install them. Otherwise, the tool will fail and it will require you to install the packages. The configuration of a storage provider can be located in the config.json file or provided by a secret provider . List of providers \u00b6 Google Drive Amazon S3 or S3-like Backblaze B2 FTP FTPS SFTP","title":"Overview"},{"location":"storage/#storage-providers","text":"A storage provider is server or service provider that offers a place to store files remotely. Can be a cloud storage provider (like S3 or Google Drive) or servers (like FTP or SFTP). Each provider is different and has different configurations and behaviours, but the goal of the tool is to simplify the upload of backups and the cleanup of them with simple settings. Each storage provider has dependencies that are not installed by default. If you intend to use some of them, first check out which are their dependencies and install them. Otherwise, the tool will fail and it will require you to install the packages. The configuration of a storage provider can be located in the config.json file or provided by a secret provider .","title":"Storage providers"},{"location":"storage/#list-of-providers","text":"Google Drive Amazon S3 or S3-like Backblaze B2 FTP FTPS SFTP","title":"List of providers"},{"location":"storage/b2/","text":"Backblaze B2 \u00b6 Similar to S3 , B2 is a Object cloud storage that offers high performance, high availability and simple interface to store objects in the cloud. Like S3, it also uses keys to identify the objects and slashes / can be used to organize the objects. The backupsPath in B2 is like a prefix for the object keys. It is recommended to put something here to easily organise the backups from the rest of files in the bucket. The initial slash / is removed when uploading the files. You don\u2019t need to create anything under the backupsPath prefix, it will be created automatically with the upload. The content type of the files will be guessed and set in the metadata when uploading. To start using B2, you must get and define the keyId , appKey and bucket . If password is defined, the files will be protected using this setting as password. Dependencies \u00b6 In order to use B2, you must install the following python packages: b2sdk python-magic Configuration schema \u00b6 { \"type\" : \"b2\" , \"backupsPath\" : \"Path in B2 where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"keyId\" : \"B2 Key ID\" , \"appKey\" : \"B2 Application Key\" , \"bucket\" : \"Name of the bucket\" , \"password\" : \"(optional) Protects files with passwords\" }","title":"B2"},{"location":"storage/b2/#backblaze-b2","text":"Similar to S3 , B2 is a Object cloud storage that offers high performance, high availability and simple interface to store objects in the cloud. Like S3, it also uses keys to identify the objects and slashes / can be used to organize the objects. The backupsPath in B2 is like a prefix for the object keys. It is recommended to put something here to easily organise the backups from the rest of files in the bucket. The initial slash / is removed when uploading the files. You don\u2019t need to create anything under the backupsPath prefix, it will be created automatically with the upload. The content type of the files will be guessed and set in the metadata when uploading. To start using B2, you must get and define the keyId , appKey and bucket . If password is defined, the files will be protected using this setting as password.","title":"Backblaze B2"},{"location":"storage/b2/#dependencies","text":"In order to use B2, you must install the following python packages: b2sdk python-magic","title":"Dependencies"},{"location":"storage/b2/#configuration-schema","text":"{ \"type\" : \"b2\" , \"backupsPath\" : \"Path in B2 where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"keyId\" : \"B2 Key ID\" , \"appKey\" : \"B2 Application Key\" , \"bucket\" : \"Name of the bucket\" , \"password\" : \"(optional) Protects files with passwords\" }","title":"Configuration schema"},{"location":"storage/ftp/","text":"FTP \u00b6 FTP is a protocol to transfer files between computers over the network, but it\u2019s not encrypted. It\u2019s quite common protocol used by many hostings and easy to install in servers. To use a FTP server to store the backups remotely, at least the host must be defined. This setting can also contain the port of the server. The username and password can be provided to use them to connect to the server. The backupPath must exist in the server, and the user must have write permissions on it. Take into account that if the user is \u201c chroot ed\u201d, the path will be different from the real path in the server. FTPS \u00b6 FTPS is FTP over SSL/TLS that adds a layer of security over the FTP protocol. Is an extension of the FTP provider, that adds the SSL/TLS layer, and uses the same settings as in the FTP plus two new ones. If custom certificate chains are being used, keyFile and certFile probably will be needed. The certificate chain file allows the tool to identify the server properly and allow it to be used. If client certificate is needed, keyFile can be defined and must point to a private key file. Currently both files must exist in the file system, and certificates cannot be stored in base64 in the configuration. They can be stored in config folder is desired. Dependencies \u00b6 No extra python packages are required to use this provider. Uses ftplib , which is bundled in Python. Configuration schema \u00b6 { \"type\" : \"ftp\" , \"backupsPath\" : \"Path in the FTP where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host (with or without the port) where the FTP server is located\" , \"user\" : \"(optional) User to connect to the FTP server\" , \"password\" : \"(optional) Password for the user\" , \"acct\" : \"(optional) Account information for the user\" } { \"type\" : \"ftps\" , \"backupsPath\" : \"Path in the FTPS where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host (with or without the port) where the FTPS server is located\" , \"user\" : \"(optional) User to connect to the FTPS server\" , \"password\" : \"(optional) Password for the user\" , \"acct\" : \"(optional) Account information for the user\" , \"keyFile\" : \"(optional) If needed, define a custom key file\" , \"certFile\" : \"(optional) If needed, define a custom certificate file\" }","title":"FTP(S)"},{"location":"storage/ftp/#ftp","text":"FTP is a protocol to transfer files between computers over the network, but it\u2019s not encrypted. It\u2019s quite common protocol used by many hostings and easy to install in servers. To use a FTP server to store the backups remotely, at least the host must be defined. This setting can also contain the port of the server. The username and password can be provided to use them to connect to the server. The backupPath must exist in the server, and the user must have write permissions on it. Take into account that if the user is \u201c chroot ed\u201d, the path will be different from the real path in the server.","title":"FTP"},{"location":"storage/ftp/#ftps","text":"FTPS is FTP over SSL/TLS that adds a layer of security over the FTP protocol. Is an extension of the FTP provider, that adds the SSL/TLS layer, and uses the same settings as in the FTP plus two new ones. If custom certificate chains are being used, keyFile and certFile probably will be needed. The certificate chain file allows the tool to identify the server properly and allow it to be used. If client certificate is needed, keyFile can be defined and must point to a private key file. Currently both files must exist in the file system, and certificates cannot be stored in base64 in the configuration. They can be stored in config folder is desired.","title":"FTPS"},{"location":"storage/ftp/#dependencies","text":"No extra python packages are required to use this provider. Uses ftplib , which is bundled in Python.","title":"Dependencies"},{"location":"storage/ftp/#configuration-schema","text":"{ \"type\" : \"ftp\" , \"backupsPath\" : \"Path in the FTP where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host (with or without the port) where the FTP server is located\" , \"user\" : \"(optional) User to connect to the FTP server\" , \"password\" : \"(optional) Password for the user\" , \"acct\" : \"(optional) Account information for the user\" } { \"type\" : \"ftps\" , \"backupsPath\" : \"Path in the FTPS where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host (with or without the port) where the FTPS server is located\" , \"user\" : \"(optional) User to connect to the FTPS server\" , \"password\" : \"(optional) Password for the user\" , \"acct\" : \"(optional) Account information for the user\" , \"keyFile\" : \"(optional) If needed, define a custom key file\" , \"certFile\" : \"(optional) If needed, define a custom certificate file\" }","title":"Configuration schema"},{"location":"storage/gdrive/","text":"Google Drive \u00b6 Google Drive is a cloud storage for all your files, like your USB or external hard-drive where you store stuff, but in the cloud. To be able to use Google Drive as storage provider, You will need the client_secrets.json . You should get them from the Google Developer\u2019s Console , by going to Credentials and creating a new OAuth 2.0 Client IDs or using an existing one. Every OAuth 2.0 entry have a download icon, this will download that file. The file must be stored somewhere in the local file system (for example, in the config folder). The only drawback is that the JSON cannot be stored in-place, inside the configuration. The auth_tokens.json is created when a user logs in. You must use a path accessible from the tool to write the file. If the file does not exist, run the utility manually and (in some point) it will ask you to go to an URL. Here is where you log in with an account and Google will give you a token. Copy and paste it into the terminal. Now you will see the files uploading to Google Drive and the auth tokens file created. The backupsFolder must exist before running the tool. Dependencies \u00b6 In order to use Google Drive, you must install the following python packages: boto3 python-magic Configuration schema \u00b6 { \"type\" : \"gdrive\" , \"backupsPath\" : \"Path in Google Drive where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"clientSecrets\" : \"config/client_secrets.json\" , \"authTokens\" : \"config/auth_tokens.json\" }","title":"Google Drive"},{"location":"storage/gdrive/#google-drive","text":"Google Drive is a cloud storage for all your files, like your USB or external hard-drive where you store stuff, but in the cloud. To be able to use Google Drive as storage provider, You will need the client_secrets.json . You should get them from the Google Developer\u2019s Console , by going to Credentials and creating a new OAuth 2.0 Client IDs or using an existing one. Every OAuth 2.0 entry have a download icon, this will download that file. The file must be stored somewhere in the local file system (for example, in the config folder). The only drawback is that the JSON cannot be stored in-place, inside the configuration. The auth_tokens.json is created when a user logs in. You must use a path accessible from the tool to write the file. If the file does not exist, run the utility manually and (in some point) it will ask you to go to an URL. Here is where you log in with an account and Google will give you a token. Copy and paste it into the terminal. Now you will see the files uploading to Google Drive and the auth tokens file created. The backupsFolder must exist before running the tool.","title":"Google Drive"},{"location":"storage/gdrive/#dependencies","text":"In order to use Google Drive, you must install the following python packages: boto3 python-magic","title":"Dependencies"},{"location":"storage/gdrive/#configuration-schema","text":"{ \"type\" : \"gdrive\" , \"backupsPath\" : \"Path in Google Drive where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"clientSecrets\" : \"config/client_secrets.json\" , \"authTokens\" : \"config/auth_tokens.json\" }","title":"Configuration schema"},{"location":"storage/s3/","text":"Amazon S3 or S3-like storage \u00b6 Object cloud storage that offers scalability, high availability and security using a simple but powerful interface. Objects are stored using keys, and have some attributes and metadata associated. The keys can contain / in their names, and this creates folder-like structures. Every storage is is defined in terms of buckets. Each bucket is like a new drive that will hold different objects. You can use Amazon S3 or any S3 compatible cloud storage provider (like DigitalOcean ) to store the backups. The backupsPath in S3 is like a prefix for the object keys. It is recommended to put something here to easily organise the backups from the rest of files in the bucket. The initial slash / is removed when uploading the files. You don\u2019t need to create anything under the backupsPath prefix, it will be created automatically with the upload. The content type of the files will be guessed and set in the metadata when uploading. To start using it, you must define accessKeyId , accessSecretKey , region and bucket . If using something different from Amazon S3 (like DigitalOcean\u2019s Spaces), you must define the endpoint as well. For example, in Spaces, the endpoint will be https:// ${ region } .digitaloceanspaces.com , where ${ region } is the region you choose when creating the space. Dependencies \u00b6 In order to use S3, you must install the following python packages: boto3 python-magic Configuration schema \u00b6 { \"type\" : \"s3\" , \"backupsPath\" : \"Path in S3 where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"region\" : \"Region of the S3 storage\" , \"endpoint\" : \"Endpoint (if not set, uses Amazon S3 endpoint)\" , \"accessKeyId\" : \"Access Key ID\" , \"accessSecretKey\" : \"Access Secret Key\" , \"bucket\" : \"Name of the bucket\" }","title":"S3"},{"location":"storage/s3/#amazon-s3-or-s3-like-storage","text":"Object cloud storage that offers scalability, high availability and security using a simple but powerful interface. Objects are stored using keys, and have some attributes and metadata associated. The keys can contain / in their names, and this creates folder-like structures. Every storage is is defined in terms of buckets. Each bucket is like a new drive that will hold different objects. You can use Amazon S3 or any S3 compatible cloud storage provider (like DigitalOcean ) to store the backups. The backupsPath in S3 is like a prefix for the object keys. It is recommended to put something here to easily organise the backups from the rest of files in the bucket. The initial slash / is removed when uploading the files. You don\u2019t need to create anything under the backupsPath prefix, it will be created automatically with the upload. The content type of the files will be guessed and set in the metadata when uploading. To start using it, you must define accessKeyId , accessSecretKey , region and bucket . If using something different from Amazon S3 (like DigitalOcean\u2019s Spaces), you must define the endpoint as well. For example, in Spaces, the endpoint will be https:// ${ region } .digitaloceanspaces.com , where ${ region } is the region you choose when creating the space.","title":"Amazon S3 or S3-like storage"},{"location":"storage/s3/#dependencies","text":"In order to use S3, you must install the following python packages: boto3 python-magic","title":"Dependencies"},{"location":"storage/s3/#configuration-schema","text":"{ \"type\" : \"s3\" , \"backupsPath\" : \"Path in S3 where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"region\" : \"Region of the S3 storage\" , \"endpoint\" : \"Endpoint (if not set, uses Amazon S3 endpoint)\" , \"accessKeyId\" : \"Access Key ID\" , \"accessSecretKey\" : \"Access Secret Key\" , \"bucket\" : \"Name of the bucket\" }","title":"Configuration schema"},{"location":"storage/sftp/","text":"SFTP \u00b6 SSH File Transfer Protocol SFTP is a protocol that works over SSH to transfer and manage files over the network. Is not the same as FTP or FTPS, but its similar. Most servers have enabled the SFTP mode if they have a SSH server. OpenSSH implementation have the SFTP module and can be enabled if desired. In order to connect to a SFTP server and use it as provider, the host and user must be defined, and use one of the following authentication methods. The backupPath must exist in the server, and the user must have write permissions on it. The authentication is attempted using the following order: If privateKey or privateKeyFile is defined, then this method will be used. If the private key is cyphered, use password as the passphrase of the key. If allowAgent is true, then will use the SSH Agent to connect to the server. Any key found in ~/.ssh . If the password is defined, then the classic username/password login will be used (discouraged). The known-hosts list is loaded by default from the default location ~/.ssh/known_hosts . If hostKeysFilePath is defined, then this file will be used instead. If disableHostKeys is set to false, then no known-hosts will be loaded. The knownHostsPolicy will set the policy that will be used when the SSH connection is set, but the host is being checked as a known or not-known host. The following policies are allowed: reject will close the connection if the host is not-known (default behaviour). auto-add will add to the list of known-hosts if the host is not-known. ignore will print a warning if the host is not-known. Note : if the knownHostsPolicy is auto-add and hostKeysFilePath is defined, then the new host will be saved into the file. Dependencies \u00b6 In order to use SFTP, you must install the following python packages: paramiko Configuration schema \u00b6 { \"type\" : \"sftp\" , \"backupsPath\" : \"Path in the SFTP where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host where the SFTP server is located\" , \"port\" : \"(optional) Port of the SFTP server (by default 22)\" , \"user\" : \"User to connect to the SFTP server\" , \"password\" : \"(optional) Password for the user\" , \"privateKey\" : \"(optional) Private Key in base64\" , \"privateKeyPath\" : \"(optional) Private Key file path\" , \"allowAgent\" : \"(optional) if true, then the connection will interact with the SSH Agent, false if this behaviour is not desired (false by default)\" , \"compress\" : \"(optional) if true, then the connection is compressed (false by default)\" , \"knownHostsPolicy\" : \"(optional) Changes the Known Hosts Policy. 'reject' will reject any connection to a server that is not known (default behaviour), 'auto-add' will add to the known-hosts list this server, 'ignore' will print a warning but it will let you connect.\" , \"hostKeysFilePath\" : \"(optional) Path to the known-hosts file\" , \"disableHostKeys\" : \"(optional) If set to false, it won't load any known-hosts file (by default is true)\" }","title":"SFTP"},{"location":"storage/sftp/#sftp","text":"SSH File Transfer Protocol SFTP is a protocol that works over SSH to transfer and manage files over the network. Is not the same as FTP or FTPS, but its similar. Most servers have enabled the SFTP mode if they have a SSH server. OpenSSH implementation have the SFTP module and can be enabled if desired. In order to connect to a SFTP server and use it as provider, the host and user must be defined, and use one of the following authentication methods. The backupPath must exist in the server, and the user must have write permissions on it. The authentication is attempted using the following order: If privateKey or privateKeyFile is defined, then this method will be used. If the private key is cyphered, use password as the passphrase of the key. If allowAgent is true, then will use the SSH Agent to connect to the server. Any key found in ~/.ssh . If the password is defined, then the classic username/password login will be used (discouraged). The known-hosts list is loaded by default from the default location ~/.ssh/known_hosts . If hostKeysFilePath is defined, then this file will be used instead. If disableHostKeys is set to false, then no known-hosts will be loaded. The knownHostsPolicy will set the policy that will be used when the SSH connection is set, but the host is being checked as a known or not-known host. The following policies are allowed: reject will close the connection if the host is not-known (default behaviour). auto-add will add to the list of known-hosts if the host is not-known. ignore will print a warning if the host is not-known. Note : if the knownHostsPolicy is auto-add and hostKeysFilePath is defined, then the new host will be saved into the file.","title":"SFTP"},{"location":"storage/sftp/#dependencies","text":"In order to use SFTP, you must install the following python packages: paramiko","title":"Dependencies"},{"location":"storage/sftp/#configuration-schema","text":"{ \"type\" : \"sftp\" , \"backupsPath\" : \"Path in the SFTP where the backups will be located\" , \"maxBackupsKept\" : \"Indicates how many backups to keep in this storage, or set to null to keep them all\" , \"host\" : \"Host where the SFTP server is located\" , \"port\" : \"(optional) Port of the SFTP server (by default 22)\" , \"user\" : \"User to connect to the SFTP server\" , \"password\" : \"(optional) Password for the user\" , \"privateKey\" : \"(optional) Private Key in base64\" , \"privateKeyPath\" : \"(optional) Private Key file path\" , \"allowAgent\" : \"(optional) if true, then the connection will interact with the SSH Agent, false if this behaviour is not desired (false by default)\" , \"compress\" : \"(optional) if true, then the connection is compressed (false by default)\" , \"knownHostsPolicy\" : \"(optional) Changes the Known Hosts Policy. 'reject' will reject any connection to a server that is not known (default behaviour), 'auto-add' will add to the known-hosts list this server, 'ignore' will print a warning but it will let you connect.\" , \"hostKeysFilePath\" : \"(optional) Path to the known-hosts file\" , \"disableHostKeys\" : \"(optional) If set to false, it won't load any known-hosts file (by default is true)\" }","title":"Configuration schema"}]}